{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import scipy.misc\n",
    "import scipy.ndimage\n",
    "import imageio\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epoch = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OHE(feature):\n",
    "    return pd.get_dummies(feature)\n",
    "def standarization(feature):\n",
    "    return (feature-np.mean(feature))/np.std(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "1. label 的資料要合併單位至天數才可以用，這邊先不讀，不然列數不同會爆開"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Csv_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, label = False, train_valid_test = None):\n",
    "        \n",
    "        # 要先把 __init__() 括弧裡面的東西給入 self.\n",
    "        print(\"path: \",path)\n",
    "\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "        self.train_valid_test = train_valid_test\n",
    "        \n",
    "        \n",
    "        oheList = ['hotel', 'meal', 'market_segment', 'distribution_channel', 'reserved_room_type', 'assigned_room_type', 'deposit_type', 'customer_type']\n",
    "        standardizeList = ['lead_time', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children', 'babies', 'previous_cancellations', 'previous_bookings_not_canceled', 'booking_changes', 'days_in_waiting_list', 'required_car_parking_spaces', 'total_of_special_requests']\n",
    "        donothingList = ['is_repeated_guest']\n",
    "        dropList = ['ID', 'arrival_date_year', 'arrival_date_month', 'arrival_date_week_number', 'arrival_date_day_of_month', 'country', 'agent', 'company', 'reservation_status', 'reservation_status_date']\n",
    "        \n",
    "        # label_csv_path = True 就是有 label\n",
    "        if self.train_valid_test == \"train\" or self.train_valid_test == \"valid\":\n",
    "            \n",
    "            csv_data_name = \"train.csv\"\n",
    "            csv_label_name = \"train_label.csv\"\n",
    "            \n",
    "            self.df_data = pd.read_csv(os.path.join(self.path, csv_data_name))  \n",
    "            self.df_data = self.df_data.fillna(0)\n",
    "            \n",
    "        elif self.train_valid_test == \"test\" :\n",
    "        \n",
    "            csv_data_name = \"test.csv\"\n",
    "            csv_label_name = \"test_nolabel.csv\"\n",
    "            \n",
    "            self.df_data = pd.read_csv(os.path.join(self.path, csv_data_name))  \n",
    "            \n",
    "            print(\"There's NO adr and is_canceled in testing data.\")\n",
    "            \n",
    "            self.df_data = self.df_data.fillna(0)\n",
    "        else:\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!  error train_valid_test is something wrong !!!!!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "###########################################################################\n",
    "####  讀取 adr 和 is_canceled\n",
    "#########################################################################################################################\n",
    "\n",
    "        if self.train_valid_test == \"train\" :\n",
    "\n",
    "            self.df_adr = self.df_data[\"adr\"].iloc[:72000].to_numpy()\n",
    "            print(\"----- df_adr.shape = {}\".format(self.df_adr.shape))\n",
    "\n",
    "            self.df_cancel = self.df_data[\"is_canceled\"].iloc[:72000].to_numpy()\n",
    "            print(\"----- df_cancel.shape = {}\".format(self.df_cancel.shape))\n",
    "\n",
    "        elif self.train_valid_test == \"valid\" :\n",
    "            self.df_adr = self.df_data[\"adr\"].iloc[72000:].to_numpy()\n",
    "            print(\"----- df_adr.shape = {}\".format(self.df_adr.shape))\n",
    "\n",
    "            self.df_cancel = self.df_data[\"is_canceled\"].iloc[72000:].to_numpy()\n",
    "            print(\"----- df_cancel.shape = {}\".format(self.df_cancel.shape))\n",
    "            \n",
    "#########################################################################################################################\n",
    "    \n",
    "        # 照著討論出的 baseline 去處理資料\n",
    "        key_df_list = list(self.df_data.keys())\n",
    "        data_feature = np.array([])\n",
    "        for k in key_df_list:\n",
    "\n",
    "            if k == \"hotel\" :\n",
    "                print(\"----------------------------------------\\noheList: \",k)\n",
    "                feature = self.df_data[k]\n",
    "                feature = OHE(feature).iloc[:].to_numpy()\n",
    "                print(\"oheList_feature: \",feature.shape)\n",
    "                data_feature = np.array(feature)\n",
    "                print(\"original data_feature: \",data_feature.shape)\n",
    "\n",
    "            # 做 onehot encoding\n",
    "            elif k in oheList :\n",
    "                print(\"----------------------------------------\\noheList: \",k)\n",
    "                feature = self.df_data[k]\n",
    "                feature = OHE(feature).iloc[:].to_numpy()\n",
    "                print(\"oheList_feature: \",feature.shape)\n",
    "                data_feature = np.column_stack((data_feature, feature))\n",
    "\n",
    "            # 做標準化\n",
    "            elif k in standardizeList :\n",
    "                print(\"----------------------------------------\\nstandarizeList: \",k)\n",
    "                feature = self.df_data[k].iloc[:].to_numpy()\n",
    "                feature = standarization(feature)\n",
    "                print(\"standardizeList_feature: \",feature.shape)\n",
    "                data_feature = np.column_stack((data_feature, feature))\n",
    "\n",
    "            # 原封不動給到我們的結果\n",
    "            elif k in donothingList :\n",
    "                print(\"----------------------------------------\\ndonothingList: \",k)\n",
    "                feature = self.df_data[k].iloc[:].to_numpy()\n",
    "                print(\"donothingList_feature: \",feature.shape)\n",
    "                data_feature = np.column_stack((data_feature, feature))\n",
    "\n",
    "            # 直接跳過\n",
    "            elif k in dropList :\n",
    "                print(\"----------------------------------------\\ndropList: \",k)\n",
    "\n",
    "            else:\n",
    "                print(\"----------------------------------------\\nbugggggggggg: \",k)\n",
    "\n",
    "            print(\"total data_feature: \",type(data_feature))\n",
    "            print(\"total data_feature: \",data_feature.shape)\n",
    "\n",
    "###########################################################################\n",
    "####  分割\n",
    "#########################################################################################################################\n",
    "\n",
    "        if self.train_valid_test == \"train\" :\n",
    "            self.df_data = data_feature[:72000,:] # 轉換結果就是我們要用的資料了\n",
    "            print(\"train df_data: \",self.df_data.shape) #這邊是 numpy 形式，而非 pd.DataFrame\n",
    "#             self.df_label = pd.read_csv(os.path.join(self.path, csv_label_name))  \n",
    "#             self.df_label = self.df_label.fillna(0)\n",
    "#             self.df_label = self.df_label.iloc[:,1].to_numpy()\n",
    "#             print(\"----- df_label.shape = {}\".format(self.df_label.shape))\n",
    "            \n",
    "        elif self.train_valid_test == \"valid\" :\n",
    "            self.df_data = data_feature[72000:,:] # 轉換結果就是我們要用的資料了\n",
    "            print(\"valid df_data: \",self.df_data.shape) #這邊是 numpy 形式，而非 pd.DataFrame\n",
    "\n",
    "#             self.df_label = pd.read_csv(os.path.join(self.path, csv_label_name))  \n",
    "#             self.df_label = self.df_label.fillna(0)\n",
    "#             self.df_label = self.df_label.iloc[:,1].to_numpy()\n",
    "#             print(\"----- df_label.shape = {}\".format(self.df_label.shape))\n",
    "            \n",
    "        elif self.train_valid_test == \"test\" :\n",
    "            self.df_data = data_feature[:,:] # 轉換結果就是我們要用的資料了\n",
    "            print(\"test df_data: \",self.df_data.shape) #這邊是 numpy 形式，而非 pd.DataFrame\n",
    "            print(\"\\nThere's no label in testing set, maybe read arrival_date only.\")\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "    # 在 from torch.utils.data import DataLoader, Dataset 中的 DataLoader, Dataset\n",
    "    # 需要 __len__ 及 __getitem__ 兩個函式\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df_data.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "#         print(\"self.path: \", self.path)\n",
    "        if self.label:\n",
    "            data = self.df_data[index]\n",
    "#             label = self.df_label[index]\n",
    "            adr = self.df_adr[index]\n",
    "            cancel = self.df_cancel[index]\n",
    "            \n",
    "#             print(\"data: \",data)\n",
    "            data = torch.tensor(data)\n",
    "#             label = torch.tensor(label)\n",
    "            adr = torch.tensor(adr)\n",
    "            cancel = torch.tensor(cancel)\n",
    "\n",
    "            return data, adr, cancel\n",
    "        # 其實就是 testing\n",
    "        else:\n",
    "            data = self.df_data[index]\n",
    "            data = torch.tensor(data)\n",
    "            \n",
    "            return data\n",
    "        \n",
    "    def get_feature_sum(self):\n",
    "        return self.df_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training Dataset=========================\n",
      "path:  ./data/\n",
      "----- df_adr.shape = (72000,)\n",
      "----- df_cancel.shape = (72000,)\n",
      "----------------------------------------\n",
      "dropList:  ID\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (0,)\n",
      "----------------------------------------\n",
      "oheList:  hotel\n",
      "oheList_feature:  (91531, 2)\n",
      "original data_feature:  (91531, 2)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 2)\n",
      "----------------------------------------\n",
      "bugggggggggg:  is_canceled\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 2)\n",
      "----------------------------------------\n",
      "standarizeList:  lead_time\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_year\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_month\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_week_number\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_day_of_month\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "standarizeList:  stays_in_weekend_nights\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 4)\n",
      "----------------------------------------\n",
      "standarizeList:  stays_in_week_nights\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 5)\n",
      "----------------------------------------\n",
      "standarizeList:  adults\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 6)\n",
      "----------------------------------------\n",
      "standarizeList:  children\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 7)\n",
      "----------------------------------------\n",
      "standarizeList:  babies\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 8)\n",
      "----------------------------------------\n",
      "oheList:  meal\n",
      "oheList_feature:  (91531, 5)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 13)\n",
      "----------------------------------------\n",
      "dropList:  country\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 13)\n",
      "----------------------------------------\n",
      "oheList:  market_segment\n",
      "oheList_feature:  (91531, 8)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 21)\n",
      "----------------------------------------\n",
      "oheList:  distribution_channel\n",
      "oheList_feature:  (91531, 5)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 26)\n",
      "----------------------------------------\n",
      "donothingList:  is_repeated_guest\n",
      "donothingList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 27)\n",
      "----------------------------------------\n",
      "standarizeList:  previous_cancellations\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 28)\n",
      "----------------------------------------\n",
      "standarizeList:  previous_bookings_not_canceled\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 29)\n",
      "----------------------------------------\n",
      "oheList:  reserved_room_type\n",
      "oheList_feature:  (91531, 10)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 39)\n",
      "----------------------------------------\n",
      "oheList:  assigned_room_type\n",
      "oheList_feature:  (91531, 12)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 51)\n",
      "----------------------------------------\n",
      "standarizeList:  booking_changes\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 52)\n",
      "----------------------------------------\n",
      "oheList:  deposit_type\n",
      "oheList_feature:  (91531, 3)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 55)\n",
      "----------------------------------------\n",
      "dropList:  agent\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 55)\n",
      "----------------------------------------\n",
      "dropList:  company\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 55)\n",
      "----------------------------------------\n",
      "standarizeList:  days_in_waiting_list\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 56)\n",
      "----------------------------------------\n",
      "oheList:  customer_type\n",
      "oheList_feature:  (91531, 4)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 60)\n",
      "----------------------------------------\n",
      "bugggggggggg:  adr\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 60)\n",
      "----------------------------------------\n",
      "standarizeList:  required_car_parking_spaces\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 61)\n",
      "----------------------------------------\n",
      "standarizeList:  total_of_special_requests\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 62)\n",
      "----------------------------------------\n",
      "dropList:  reservation_status\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 62)\n",
      "----------------------------------------\n",
      "dropList:  reservation_status_date\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 62)\n",
      "train df_data:  (72000, 62)\n",
      "\n",
      "==================== Validation Dataset=========================\n",
      "path:  ./data/\n",
      "----- df_adr.shape = (19531,)\n",
      "----- df_cancel.shape = (19531,)\n",
      "----------------------------------------\n",
      "dropList:  ID\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (0,)\n",
      "----------------------------------------\n",
      "oheList:  hotel\n",
      "oheList_feature:  (91531, 2)\n",
      "original data_feature:  (91531, 2)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 2)\n",
      "----------------------------------------\n",
      "bugggggggggg:  is_canceled\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 2)\n",
      "----------------------------------------\n",
      "standarizeList:  lead_time\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_year\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_month\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_week_number\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_day_of_month\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "standarizeList:  stays_in_weekend_nights\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 4)\n",
      "----------------------------------------\n",
      "standarizeList:  stays_in_week_nights\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 5)\n",
      "----------------------------------------\n",
      "standarizeList:  adults\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 6)\n",
      "----------------------------------------\n",
      "standarizeList:  children\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 7)\n",
      "----------------------------------------\n",
      "standarizeList:  babies\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 8)\n",
      "----------------------------------------\n",
      "oheList:  meal\n",
      "oheList_feature:  (91531, 5)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 13)\n",
      "----------------------------------------\n",
      "dropList:  country\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 13)\n",
      "----------------------------------------\n",
      "oheList:  market_segment\n",
      "oheList_feature:  (91531, 8)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 21)\n",
      "----------------------------------------\n",
      "oheList:  distribution_channel\n",
      "oheList_feature:  (91531, 5)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 26)\n",
      "----------------------------------------\n",
      "donothingList:  is_repeated_guest\n",
      "donothingList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 27)\n",
      "----------------------------------------\n",
      "standarizeList:  previous_cancellations\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 28)\n",
      "----------------------------------------\n",
      "standarizeList:  previous_bookings_not_canceled\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 29)\n",
      "----------------------------------------\n",
      "oheList:  reserved_room_type\n",
      "oheList_feature:  (91531, 10)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 39)\n",
      "----------------------------------------\n",
      "oheList:  assigned_room_type\n",
      "oheList_feature:  (91531, 12)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 51)\n",
      "----------------------------------------\n",
      "standarizeList:  booking_changes\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 52)\n",
      "----------------------------------------\n",
      "oheList:  deposit_type\n",
      "oheList_feature:  (91531, 3)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 55)\n",
      "----------------------------------------\n",
      "dropList:  agent\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 55)\n",
      "----------------------------------------\n",
      "dropList:  company\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 55)\n",
      "----------------------------------------\n",
      "standarizeList:  days_in_waiting_list\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 56)\n",
      "----------------------------------------\n",
      "oheList:  customer_type\n",
      "oheList_feature:  (91531, 4)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 60)\n",
      "----------------------------------------\n",
      "bugggggggggg:  adr\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 60)\n",
      "----------------------------------------\n",
      "standarizeList:  required_car_parking_spaces\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 61)\n",
      "----------------------------------------\n",
      "standarizeList:  total_of_special_requests\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 62)\n",
      "----------------------------------------\n",
      "dropList:  reservation_status\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 62)\n",
      "----------------------------------------\n",
      "dropList:  reservation_status_date\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (91531, 62)\n",
      "valid df_data:  (19531, 62)\n",
      "\n",
      "==================== Testing Dataset=========================\n",
      "path:  ./data/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's NO adr and is_canceled in testing data.\n",
      "----------------------------------------\n",
      "dropList:  ID\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (0,)\n",
      "----------------------------------------\n",
      "oheList:  hotel\n",
      "oheList_feature:  (27860, 2)\n",
      "original data_feature:  (27860, 2)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 2)\n",
      "----------------------------------------\n",
      "standarizeList:  lead_time\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_year\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_month\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_week_number\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_day_of_month\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 3)\n",
      "----------------------------------------\n",
      "standarizeList:  stays_in_weekend_nights\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 4)\n",
      "----------------------------------------\n",
      "standarizeList:  stays_in_week_nights\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 5)\n",
      "----------------------------------------\n",
      "standarizeList:  adults\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 6)\n",
      "----------------------------------------\n",
      "standarizeList:  children\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 7)\n",
      "----------------------------------------\n",
      "standarizeList:  babies\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 8)\n",
      "----------------------------------------\n",
      "oheList:  meal\n",
      "oheList_feature:  (27860, 5)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 13)\n",
      "----------------------------------------\n",
      "dropList:  country\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 13)\n",
      "----------------------------------------\n",
      "oheList:  market_segment\n",
      "oheList_feature:  (27860, 8)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 21)\n",
      "----------------------------------------\n",
      "oheList:  distribution_channel\n",
      "oheList_feature:  (27860, 5)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 26)\n",
      "----------------------------------------\n",
      "donothingList:  is_repeated_guest\n",
      "donothingList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 27)\n",
      "----------------------------------------\n",
      "standarizeList:  previous_cancellations\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 28)\n",
      "----------------------------------------\n",
      "standarizeList:  previous_bookings_not_canceled\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 29)\n",
      "----------------------------------------\n",
      "oheList:  reserved_room_type\n",
      "oheList_feature:  (27860, 10)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 39)\n",
      "----------------------------------------\n",
      "oheList:  assigned_room_type\n",
      "oheList_feature:  (27860, 12)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 51)\n",
      "----------------------------------------\n",
      "standarizeList:  booking_changes\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 52)\n",
      "----------------------------------------\n",
      "oheList:  deposit_type\n",
      "oheList_feature:  (27860, 3)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 55)\n",
      "----------------------------------------\n",
      "dropList:  agent\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 55)\n",
      "----------------------------------------\n",
      "dropList:  company\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 55)\n",
      "----------------------------------------\n",
      "standarizeList:  days_in_waiting_list\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 56)\n",
      "----------------------------------------\n",
      "oheList:  customer_type\n",
      "oheList_feature:  (27860, 4)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 60)\n",
      "----------------------------------------\n",
      "standarizeList:  required_car_parking_spaces\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 61)\n",
      "----------------------------------------\n",
      "standarizeList:  total_of_special_requests\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  <class 'numpy.ndarray'>\n",
      "total data_feature:  (27860, 62)\n",
      "test df_data:  (27860, 62)\n",
      "\n",
      "There's no label in testing set, maybe read arrival_date only.\n",
      "\n",
      "==================== Dataloader =========================\n",
      "\n",
      "==================== Done =========================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==================== Training Dataset=========================\")\n",
    "train_dataset = Csv_Dataset(path = \"./data/\", label = True, train_valid_test = \"train\")\n",
    "\n",
    "print(\"\\n==================== Validation Dataset=========================\")\n",
    "valid_dataset = Csv_Dataset(path = \"./data/\", label = True, train_valid_test = \"valid\")\n",
    "\n",
    "print(\"\\n==================== Testing Dataset=========================\")\n",
    "test_dataset = Csv_Dataset(path = \"./data/\", label = False, train_valid_test = \"test\")\n",
    "\n",
    "print(\"\\n==================== Dataloader =========================\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"\\n==================== Done =========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train feature_num: 62\n",
      "valid feature_num: 62\n",
      "test feature_num: 62\n",
      "========== feature_num:  62\n",
      "72000\n",
      "19531\n",
      "27860\n"
     ]
    }
   ],
   "source": [
    "print(\"train feature_num:\",train_dataset.get_feature_sum())\n",
    "print(\"valid feature_num:\",valid_dataset.get_feature_sum())\n",
    "print(\"test feature_num:\",test_dataset.get_feature_sum())\n",
    "\n",
    "if train_dataset.get_feature_sum() == valid_dataset.get_feature_sum() == test_dataset.get_feature_sum() :\n",
    "    feature_num = train_dataset.get_feature_sum()\n",
    "    print(\"========== feature_num: \", feature_num)\n",
    "else :\n",
    "    print(\"Error!! Train, valid, test with different feature !!!\")\n",
    "\n",
    "print(train_dataset.__len__())\n",
    "print(valid_dataset.__len__())\n",
    "print(test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADR_Model(nn.Module):\n",
    "    def __init__(self, in_feature, hidden_layer, out_feature=1):\n",
    "        super(ADR_Model, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_feature, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(hidden_layer, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(hidden_layer, out_feature),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layer(x)\n",
    "#         print(\"output: \",output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cancel_Model(nn.Module):\n",
    "    def __init__(self, in_feature, hidden_layer, out_feature=2):\n",
    "        super(Cancel_Model, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_feature, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(hidden_layer, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer, out_feature),\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layer(x)\n",
    "#         print(\"output: \",output.shape)\n",
    "#         output = F.softmax(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Final_Model(nn.Module):\n",
    "#     def __init__(self, in_feature, hidden_layer, out_feature=2):\n",
    "#         super(Final_Model, self).__init__()\n",
    "\n",
    "#         self.layer = nn.Sequential(\n",
    "#             nn.Linear(in_feature, hidden_layer),\n",
    "#             nn.BatchNorm1d(hidden_layer),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.3),\n",
    "\n",
    "#             nn.Linear(hidden_layer, hidden_layer),\n",
    "#             nn.BatchNorm1d(hidden_layer),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.3),\n",
    "\n",
    "#             nn.Linear(hidden_layer, out_feature),\n",
    "# #             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         output = self.layer(x)\n",
    "# #         print(\"output: \",output.shape)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_model = ADR_Model(in_feature = feature_num, hidden_layer = 256 ).cuda()\n",
    "cancel_model = Cancel_Model(in_feature = feature_num, hidden_layer = 128).cuda()\n",
    "# final_model = Final_Model(in_feature = 1, hidden_layer = 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_criterion = nn.MSELoss()\n",
    "cancel_criterion = nn.CrossEntropyLoss()\n",
    "# final_criterion = nn.CrossEntropyLoss()\n",
    "# l1_criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_adr = optim.Adam(adr_model.parameters())\n",
    "\n",
    "# optimizer_cancel = optim.Adam(cancel_model.parameters())\n",
    "optimizer_cancel = optim.SGD(cancel_model.parameters(),lr = 0.01, momentum=0.9 ,weight_decay=0.0001)\n",
    "\n",
    "# optimizer_final = optim.Adam(final_model.parameters())\n",
    "# optimizer_final = optim.SGD(final_model.parameters(),lr = 0.01, momentum=0.9 ,weight_decay=0.0001)\n",
    "optimizer_adr = optim.Adam(adr_model.parameters(),lr = 0.01, weight_decay=0.0001)\n",
    "# optimizer_adr = optim.Adam(adr_model.parameters(),lr = 0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "1. 暫定這個 adr_model，最低的 validation mean MSE 為 693\n",
    "2. 如果 cancel model 要小數點的regression結果，直接把adr train的那段複製到cancel再做以下操作就好了  \n",
    "   1. cancel_criterion改成 BCELOSS\n",
    "   2. cancel_model 最後一層用 nn.Sigmoid，BCELOSS 用 Sigmoid，多分類用的 CE 才使用 softmax\n",
    "   3. 承上， nn.CrossEntropyLoss() 已經包入 softmax 操作了\n",
    "   4. cancel_model 輸出的 feature 改為 1，輸出的這 1 個小數就算是regression結果\n",
    "   5. 備註：其實在 pytorch 中 BCEWithLogitsLoss 就是 BCELOSS 加上 Sigmoid\n",
    "3. 目前因為 valid classification 的結果不錯，所以就用 classification 了\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mse_min = np.inf\n",
    "train_mse_min = np.inf\n",
    "\n",
    "valid_bce_min = np.inf\n",
    "train_bce_min = np.inf\n",
    "\n",
    "train_acc_max = 0.0\n",
    "valid_acc_max = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [001/200],train_MSE_mean = 1786.8513, 4.62 sec(s)\n",
      "Valid MSE: 1140.8998, Valid MSE minimum: inf \n",
      "==================================================\n",
      "epoch [002/200],train_MSE_mean = 1554.9870, 4.60 sec(s)\n",
      "Valid MSE: 1027.0612, Valid MSE minimum: inf \n",
      "==================================================\n",
      "epoch [003/200],MSE = 1578.3223, 4.45 sec(s)s)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2235317598c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0madr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madr_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#         print(\"data: \",data.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/salsanext/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c54e955cb6a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;31m#         print(\"output: \",output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/salsanext/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/salsanext/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/salsanext/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/salsanext/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/salsanext/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# adr\n",
    "for epoch in range(0, max_epoch):    \n",
    "    epoch_start_time = time.time()\n",
    "    loss = 0.0\n",
    "    mse_mean = 0.0\n",
    "    mse = []\n",
    "    adr_model.train()\n",
    "    \n",
    "    for i, (data, adr, cancel) in enumerate(train_loader):\n",
    "        optimizer_adr.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "        \n",
    "#         print(\"type(data): \",type(data))\n",
    "#         print(\"type(adr): \",type(adr))\n",
    "\n",
    "        data = data.cuda().float()\n",
    "        adr = adr.cuda().float()\n",
    "        \n",
    "        prediction = adr_model(data).squeeze()\n",
    "        \n",
    "#         print(\"data: \",data.shape)\n",
    "#         print(\"prediction: \",prediction.shape)\n",
    "#         print(\"adr: \",adr.shape)\n",
    "\n",
    "        loss = adr_criterion(prediction, adr)\n",
    "\n",
    "#         print(\"prediction: \",prediction)\n",
    "#         print(\"adr: \",adr)\n",
    "#         print(\"loss: \",loss)\n",
    "        \n",
    "        mse.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_adr.step()\n",
    "        \n",
    "        print('epoch [%03d/%03d],MSE = %2.4f, %2.2f sec(s)' % (epoch + 1, max_epoch, loss, time.time()-epoch_start_time), end = '\\r')\n",
    "    \n",
    "    mse_mean = np.mean(mse)\n",
    "    print('epoch [%03d/%03d],train_MSE_mean = %2.4f, %2.2f sec(s)' % (epoch + 1, max_epoch, mse_mean, time.time()-epoch_start_time))\n",
    "    \n",
    "    if mse_mean < train_mse_min :\n",
    "        train_mse_min = mse_mean\n",
    "        torch.save(adr_model.state_dict(), f'./checkpoints/adr_model_train_mse_min.bin')\n",
    "        \n",
    "    \n",
    "    loss = 0.0\n",
    "    mse_mean = 0.0\n",
    "    mse = []\n",
    "    adr_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, adr, cancel) in enumerate(valid_loader):\n",
    "            \n",
    "            data = data.cuda().float()\n",
    "            adr = adr.cuda().float()\n",
    "            \n",
    "            val_pred = adr_model(data).squeeze()\n",
    "            \n",
    "            loss = adr_criterion(val_pred, adr)\n",
    "            mse.append(loss.item())\n",
    "            \n",
    "        mse_mean = np.mean(mse)\n",
    "        \n",
    "        if mse_mean < valid_mse_min and epoch > 10 :\n",
    "            valid_mse_min = mse_mean\n",
    "            torch.save(adr_model.state_dict(), f'./checkpoints/adr_model_valid_mse_min.bin')\n",
    "        print(\"Valid MSE: %.4f, Valid MSE minimum: %.4f \"%(mse_mean, valid_mse_min))\n",
    "        print(\"==================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cancel\n",
    "for epoch in range(0, max_epoch):    \n",
    "    epoch_start_time = time.time()\n",
    "    loss_cancel = 0.0\n",
    "    bce_mean = 0.0\n",
    "    bce = []\n",
    "    train_acc = 0.0\n",
    "    valid_acc = 0.0\n",
    "    \n",
    "    cancel_model.train()\n",
    "    \n",
    "    for i, (data, adr, cancel) in enumerate(train_loader):\n",
    "        optimizer_cancel.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "        \n",
    "#         print(\"type(data): \",type(data))\n",
    "#         print(\"type(cancel): \",type(cancel))\n",
    "\n",
    "        data = data.cuda().float()\n",
    "        cancel = cancel.cuda().long()\n",
    "        \n",
    "        prediction_cancel = cancel_model(data).squeeze()\n",
    "        \n",
    "#         print(\"data: \",data.shape)\n",
    "#         print(\"prediction_cancel: \",prediction_cancel.shape)\n",
    "#         print(\"cancel: \",cancel.shape)\n",
    "\n",
    "        loss_cancel = cancel_criterion(prediction_cancel, cancel)\n",
    "\n",
    "#         print(\"prediction_cancel: \",prediction_cancel)\n",
    "#         print(\"cancel: \",cancel)\n",
    "#         print(\"loss_cancel: \",loss_cancel)\n",
    "        \n",
    "        bce.append(loss_cancel.item())\n",
    "        \n",
    "        loss_cancel.backward()\n",
    "        optimizer_cancel.step()\n",
    "        \n",
    "        train_acc += np.sum(np.argmax(prediction_cancel.cpu().data.numpy(), axis=1) == cancel.cpu().numpy())\n",
    "        print('epoch [%03d/%03d],CE_loss = %2.4f, train_acc = %.3f, %2.1f sec' % (epoch + 1, max_epoch, loss_cancel, train_acc/(batch_size*(i+1)), time.time()-epoch_start_time), end = '\\r')\n",
    "    \n",
    "    bce_mean = np.mean(bce)\n",
    "    print('epoch [%03d/%03d],train_loss = %2.4f, train_acc = %.3f, %2.1f sec' % (epoch + 1, max_epoch, bce_mean, train_acc/train_dataset.__len__(), time.time()-epoch_start_time))\n",
    "    \n",
    "    if train_acc > train_acc_max :\n",
    "        train_acc_max = train_acc\n",
    "        torch.save(cancel_model.state_dict(), f'./checkpoints/cancel_model_train_acc_max.bin')\n",
    "        \n",
    "    \n",
    "    loss_cancel = 0.0\n",
    "    bce_mean = 0.0\n",
    "    bce = []\n",
    "    cancel_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, adr, cancel) in enumerate(valid_loader):\n",
    "            \n",
    "            data = data.cuda().float()\n",
    "            cancel = cancel.cuda().long()\n",
    "            \n",
    "            val_pred = cancel_model(data).squeeze()\n",
    "            \n",
    "            loss_cancel = cancel_criterion(val_pred, cancel)\n",
    "            bce.append(loss_cancel.item())\n",
    "            \n",
    "            valid_acc  += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == cancel.cpu().numpy())\n",
    "            \n",
    "        bce_mean = np.mean(bce)\n",
    "        \n",
    "        if valid_acc > valid_acc_max:\n",
    "            valid_acc_max = valid_acc\n",
    "            torch.save(cancel_model.state_dict(), f'./checkpoints/cancel_model_valid_acc_max.bin')\n",
    "        print(\"Valid loss: %.3f, valid_acc = %.3f, Valid acc max: %.3f\"%(bce_mean,  valid_acc/valid_dataset.__len__(), valid_acc_max/valid_dataset.__len__()))\n",
    "        print(\"epoch time: %.2f sec\"%(time.time()-epoch_start_time))\n",
    "        print(\"============================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load adr, cancel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_model.load_state_dict(torch.load('./checkpoints/adr_model_valid_mse_min.bin'))\n",
    "cancel_model.load_state_dict(torch.load('./checkpoints/cancel_model_valid_acc_max.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cancel_Model(\n",
       "  (layer): Sequential(\n",
       "    (0): Linear(in_features=62, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.02)\n",
       "    (3): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.02)\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): LeakyReLU(negative_slope=0.02)\n",
       "    (10): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): LeakyReLU(negative_slope=0.02)\n",
       "    (13): Dropout(p=0.5, inplace=False)\n",
       "    (14): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (15): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): LeakyReLU(negative_slope=0.02)\n",
       "    (17): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adr_model.eval()\n",
    "cancel_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面的是連原始資料都讀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # npy\n",
    "# epoch_start_time = time.time()\n",
    "# with torch.no_grad():\n",
    "#     for i, (data, adr, cancel) in enumerate(train_loader):\n",
    "#         print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "#         data = data.cuda().float()\n",
    "#         adr = adr.cuda().float()\n",
    "#         cancel = cancel.cuda().long()\n",
    "        \n",
    "#         predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "#         predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "#         concat_data = data.cpu().numpy()\n",
    "\n",
    "# #         print(\"data: \",data.shape)\n",
    "# #         print(\"adr: \",adr.shape)\n",
    "# #         print(\"cancel: \",cancel.shape)\n",
    "# #         print(\"predict_adr: \",predict_adr.shape)\n",
    "# #         print(\"predict_cancel: \",predict_cancel.shape)\n",
    "# #         print(\"concat_data: \",concat_data.shape)\n",
    "        \n",
    "#         npy_data = np.zeros(shape = (train_dataset.__len__(), (concat_data[1].shape[0]+2)))\n",
    "# #         print(\"npy_data: \",npy_data.shape)\n",
    "        \n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),:concat_data.shape[1]] = concat_data[:,:]\n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),concat_data.shape[1]] = predict_adr[:]\n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),(concat_data.shape[1]+1)] = predict_cancel[:]\n",
    "\n",
    "# #         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "# np.save(\"./npy_file/train_prediction.npy\", npy_data)\n",
    "# print(\"\\n====================== train_prediction done ======================\")\n",
    "\n",
    "# epoch_start_time = time.time()\n",
    "# with torch.no_grad():\n",
    "#     for i, (data, adr, cancel) in enumerate(valid_loader):\n",
    "#         print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "#         data = data.cuda().float()\n",
    "#         adr = adr.cuda().float()\n",
    "#         cancel = cancel.cuda().long()\n",
    "        \n",
    "#         predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "#         predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "#         concat_data = data.cpu().numpy()\n",
    "\n",
    "# #         print(\"data: \",data.shape)\n",
    "# #         print(\"adr: \",adr.shape)\n",
    "# #         print(\"cancel: \",cancel.shape)\n",
    "# #         print(\"predict_adr: \",predict_adr.shape)\n",
    "# #         print(\"predict_cancel: \",predict_cancel.shape)\n",
    "# #         print(\"concat_data: \",concat_data.shape)\n",
    "        \n",
    "#         npy_data = np.zeros(shape = (valid_dataset.__len__(), (concat_data[1].shape[0]+2)))\n",
    "# #         print(\"npy_data: \",npy_data.shape)\n",
    "        \n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),:concat_data.shape[1]] = concat_data[:,:]\n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),concat_data.shape[1]] = predict_adr[:]\n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),(concat_data.shape[1]+1)] = predict_cancel[:]\n",
    "\n",
    "# #         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "# np.save(\"./npy_file/valid_prediction.npy\", npy_data)\n",
    "# print(\"\\n====================== valid_prediction done ======================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_num = 72000, 2.0 sec\n",
      "npy_data.shape:  (72000, 2)\n",
      "====================== train_prediction done ======================\n",
      "file_num = 19584, 0.5 sec\n",
      "npy_data.shape:  (19531, 2)\n",
      "====================== valid_prediction done ======================\n",
      "file_num = 27904, 0.6 sec\n",
      "npy_data.shape:  (27860, 2)\n",
      "====================== test_prediction done ======================\n"
     ]
    }
   ],
   "source": [
    "# npy\n",
    "epoch_start_time = time.time()\n",
    "npy_data = np.zeros(shape = (train_dataset.__len__(), 2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, adr, cancel) in enumerate(train_loader):\n",
    "        print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "        data = data.cuda().float()\n",
    "        \n",
    "        predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "        predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "        concat_data = data.cpu().numpy()\n",
    "\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 0] = predict_adr[:]\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 1] = predict_cancel[:]\n",
    "\n",
    "#         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "np.save(\"./npy_file/train_prediction.npy\", npy_data)\n",
    "print(\"\\nnpy_data.shape: \",npy_data.shape)\n",
    "print(\"====================== train_prediction done ======================\")\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "npy_data = np.zeros(shape = (valid_dataset.__len__(), 2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, adr, cancel) in enumerate(valid_loader):\n",
    "        print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "        data = data.cuda().float()\n",
    "        \n",
    "        predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "        predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "        concat_data = data.cpu().numpy()\n",
    "        \n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 0] = predict_adr[:]\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 1] = predict_cancel[:]\n",
    "\n",
    "#         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "np.save(\"./npy_file/valid_prediction.npy\", npy_data)\n",
    "print(\"\\nnpy_data.shape: \",npy_data.shape)\n",
    "print(\"====================== valid_prediction done ======================\")\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "npy_data = np.zeros(shape = (test_dataset.__len__(), 2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "        data = data.cuda().float()\n",
    "        \n",
    "        predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "        predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "        concat_data = data.cpu().numpy()\n",
    "        \n",
    "#         print(\"data: \",data.shape)\n",
    "#         print(\"adr: \",adr.shape)\n",
    "#         print(\"cancel: \",cancel.shape)\n",
    "#         print(\"predict_adr: \",predict_adr.shape)\n",
    "#         print(\"predict_cancel: \",predict_cancel.shape)\n",
    "#         print(\"concat_data: \",concat_data.shape)\n",
    "#         print(\"npy_data: \",npy_data.shape)\n",
    "        \n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),:concat_data.shape[1]] = concat_data[:,:]\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 0] = predict_adr[:]\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 1] = predict_cancel[:]\n",
    "\n",
    "#         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "np.save(\"./npy_file/test_prediction.npy\", npy_data)\n",
    "print(\"\\nnpy_data.shape: \",npy_data.shape)\n",
    "print(\"====================== test_prediction done ======================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用蔡夯哥算法做predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training + validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- df_date.shape = (91531,)\n",
      "----- npy_train_data.shape = (72000, 2)\n",
      "----- npy_valid_data.shape = (19531, 2)\n",
      "========================:  91531\n",
      "----- npy_data.shape = (91531, 2)\n",
      "----- npy_data.shape = (91531, 2)\n"
     ]
    }
   ],
   "source": [
    "train_valid_test = \"train\"\n",
    "\n",
    "csv_data_name = \"train.csv\"\n",
    "csv_label_name = \"train_label.csv\"\n",
    "\n",
    "df_data = pd.read_csv(os.path.join(\"./data/\", csv_data_name))  \n",
    "\n",
    "#             if train_valid_test == \"train\" :\n",
    "\n",
    "df_year = df_data[\"arrival_date_year\"].iloc[:].to_numpy()\n",
    "df_month = df_data[\"arrival_date_month\"].iloc[:].to_numpy()\n",
    "df_week = df_data[\"arrival_date_week_number\"].iloc[:].to_numpy()\n",
    "df_date = df_data[\"arrival_date_day_of_month\"].iloc[:].to_numpy()\n",
    "print(\"----- df_date.shape = {}\".format(df_date.shape))\n",
    "\n",
    "#                 print(\"df_week: \",df_wee)\n",
    "\n",
    "df_weekend = df_data[\"stays_in_weekend_nights\"].iloc[:].to_numpy()\n",
    "df_weekdays = df_data[\"stays_in_week_nights\"].iloc[:].to_numpy()\n",
    "\n",
    "\n",
    "npy_train_data = np.load(\"./npy_file/train_prediction.npy\")\n",
    "npy_valid_data = np.load(\"./npy_file/valid_prediction.npy\")\n",
    "print(\"----- npy_train_data.shape = {}\".format(npy_train_data.shape))\n",
    "print(\"----- npy_valid_data.shape = {}\".format(npy_valid_data.shape))\n",
    "print(\"========================: \",(npy_train_data.shape[0] + npy_valid_data.shape[0]))\n",
    "\n",
    "npy_data = np.zeros(shape = ((npy_train_data.shape[0] + npy_valid_data.shape[0]), 2))\n",
    "print(\"----- npy_data.shape = {}\".format(npy_data.shape))\n",
    "\n",
    "npy_data[:npy_train_data.shape[0],:] = npy_train_data [:,:]\n",
    "npy_data[npy_train_data.shape[0]:,:] = npy_valid_data [:,:]\n",
    "\n",
    "print(\"----- npy_data.shape = {}\".format(npy_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- df_date.shape = (27860,)\n",
      "----- npy_data.shape = (27860, 2)\n"
     ]
    }
   ],
   "source": [
    "train_valid_test = \"test\"\n",
    "\n",
    "csv_data_name = \"test.csv\"\n",
    "csv_label_name = \"test_nolabel.csv\"\n",
    "\n",
    "df_data = pd.read_csv(os.path.join(\"./data/\", csv_data_name))  \n",
    "\n",
    "\n",
    "df_year = df_data[\"arrival_date_year\"].iloc[:].to_numpy()\n",
    "df_month = df_data[\"arrival_date_month\"].iloc[:].to_numpy()\n",
    "df_week = df_data[\"arrival_date_week_number\"].iloc[:].to_numpy()\n",
    "df_date = df_data[\"arrival_date_day_of_month\"].iloc[:].to_numpy()\n",
    "print(\"----- df_date.shape = {}\".format(df_date.shape))\n",
    "\n",
    "\n",
    "df_weekend = df_data[\"stays_in_weekend_nights\"].iloc[:].to_numpy()\n",
    "df_weekdays = df_data[\"stays_in_week_nights\"].iloc[:].to_numpy()\n",
    "\n",
    "\n",
    "npy_data = np.load(\"./npy_file/test_prediction.npy\")\n",
    "print(\"----- npy_data.shape = {}\".format(npy_data.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_day_revenue:  37464.2960767746\n",
      "=============== Change day 1 ===============\n",
      "data[0]:  79.22137451171875\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  30944.042373657227\n",
      "=============== Change day 2 ===============\n",
      "data[0]:  59.44499969482422\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  38605.67083930969\n",
      "=============== Change day 3 ===============\n",
      "data[0]:  182.72279357910156\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  20484.624027252197\n",
      "=============== Change day 4 ===============\n",
      "data[0]:  66.92256927490234\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  34648.69349217415\n",
      "=============== Change day 5 ===============\n",
      "data[0]:  84.34040832519531\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  27607.371473312378\n",
      "=============== Change day 6 ===============\n",
      "data[0]:  149.0601806640625\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  28670.72389602661\n",
      "=============== Change day 7 ===============\n",
      "data[0]:  45.53435134887695\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  58202.60573267937\n",
      "=============== Change day 8 ===============\n",
      "data[0]:  38.087242126464844\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  57230.7419128418\n",
      "=============== Change day 9 ===============\n",
      "data[0]:  116.44564056396484\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  48184.374740600586\n",
      "=============== Change day 10 ===============\n",
      "data[0]:  94.01368713378906\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  38657.96638584137\n",
      "=============== Change day 11 ===============\n",
      "data[0]:  115.9900894165039\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  33433.672119140625\n",
      "=============== Change day 12 ===============\n",
      "data[0]:  159.22817993164062\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  57208.48591530323\n",
      "=============== Change day 13 ===============\n",
      "data[0]:  39.393089294433594\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  52281.098096847534\n",
      "=============== Change day 14 ===============\n",
      "data[0]:  41.00564193725586\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  39429.064489364624\n",
      "=============== Change day 15 ===============\n",
      "data[0]:  51.43828582763672\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  48575.61618804932\n",
      "=============== Change day 16 ===============\n",
      "data[0]:  69.0599594116211\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  46060.4502620697\n",
      "=============== Change day 17 ===============\n",
      "data[0]:  128.01895141601562\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  24642.47101712227\n",
      "=============== Change day 18 ===============\n",
      "data[0]:  67.45806121826172\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  33889.03021621704\n",
      "=============== Change day 19 ===============\n",
      "data[0]:  102.00160217285156\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  30746.643602371216\n",
      "=============== Change day 20 ===============\n",
      "data[0]:  80.63938903808594\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  40573.57655405998\n",
      "=============== Change day 21 ===============\n",
      "data[0]:  79.42699432373047\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  49394.23929619789\n",
      "=============== Change day 22 ===============\n",
      "data[0]:  39.89424133300781\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  39021.0989010334\n",
      "=============== Change day 23 ===============\n",
      "data[0]:  117.54548645019531\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  33164.731674194336\n",
      "=============== Change day 24 ===============\n",
      "data[0]:  34.04558563232422\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  17806.881298065186\n",
      "=============== Change day 25 ===============\n",
      "data[0]:  165.24288940429688\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  16290.045942306519\n",
      "=============== Change day 26 ===============\n",
      "data[0]:  66.09622192382812\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  31693.63843536377\n",
      "=============== Change day 27 ===============\n",
      "data[0]:  100.48263549804688\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  61227.029267311096\n",
      "=============== Change day 28 ===============\n",
      "data[0]:  79.61968994140625\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  61354.30905473232\n",
      "=============== Change day 29 ===============\n",
      "data[0]:  18.322845458984375\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  38171.033321380615\n",
      "=============== Change day 30 ===============\n",
      "data[0]:  93.93850708007812\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  49204.92212104797\n",
      "=============== Change day 31 ===============\n",
      "data[0]:  44.39338302612305\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  40457.75966358185\n",
      "=============== Change day 32 ===============\n",
      "data[0]:  96.62115478515625\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  26947.520785808563\n",
      "=============== Change day 33 ===============\n",
      "data[0]:  30.939346313476562\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  37917.06119585037\n",
      "=============== Change day 34 ===============\n",
      "data[0]:  61.727230072021484\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  47095.76873588562\n",
      "=============== Change day 35 ===============\n",
      "data[0]:  134.84286499023438\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  43943.74489593506\n",
      "=============== Change day 36 ===============\n",
      "data[0]:  83.56013488769531\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  40459.88207626343\n",
      "=============== Change day 37 ===============\n",
      "data[0]:  107.96157836914062\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  48020.422055244446\n",
      "=============== Change day 38 ===============\n",
      "data[0]:  111.67618560791016\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  33396.78927898407\n",
      "=============== Change day 39 ===============\n",
      "data[0]:  77.71855163574219\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  31097.170295476913\n",
      "=============== Change day 40 ===============\n",
      "data[0]:  57.699588775634766\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  42216.949966430664\n",
      "=============== Change day 41 ===============\n",
      "data[0]:  148.95101928710938\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  35926.012104034424\n",
      "=============== Change day 42 ===============\n",
      "data[0]:  64.02307891845703\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  39746.94789123535\n",
      "=============== Change day 43 ===============\n",
      "data[0]:  79.07769012451172\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  49690.72254943848\n",
      "=============== Change day 44 ===============\n",
      "data[0]:  77.27249908447266\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  43507.4938993454\n",
      "=============== Change day 45 ===============\n",
      "data[0]:  169.25949096679688\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  32382.352569580078\n",
      "=============== Change day 46 ===============\n",
      "data[0]:  50.096595764160156\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  30034.885444641113\n",
      "=============== Change day 47 ===============\n",
      "data[0]:  102.32846069335938\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  37504.44676589966\n",
      "=============== Change day 48 ===============\n",
      "data[0]:  126.86647033691406\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  35147.20171546936\n",
      "=============== Change day 49 ===============\n",
      "data[0]:  -4.205435752868652\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  41944.07175731659\n",
      "=============== Change day 50 ===============\n",
      "data[0]:  145.9254150390625\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  43663.1743144989\n",
      "=============== Change day 51 ===============\n",
      "data[0]:  102.40154266357422\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  43841.43840789795\n",
      "=============== Change day 52 ===============\n",
      "data[0]:  152.23912048339844\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  27326.507389068604\n",
      "=============== Change day 53 ===============\n",
      "data[0]:  75.9930648803711\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  35653.175978899\n",
      "=============== Change day 54 ===============\n",
      "data[0]:  7.921476364135742\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  50902.29272162914\n",
      "=============== Change day 55 ===============\n",
      "data[0]:  86.88179779052734\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  31492.645572662354\n",
      "=============== Change day 56 ===============\n",
      "data[0]:  117.33952331542969\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  28630.973068237305\n",
      "=============== Change day 57 ===============\n",
      "data[0]:  132.0475616455078\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  49392.23085594177\n",
      "=============== Change day 58 ===============\n",
      "data[0]:  148.74966430664062\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  35572.11360359192\n",
      "=============== Change day 59 ===============\n",
      "data[0]:  69.44750213623047\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  25800.723138809204\n",
      "=============== Change day 60 ===============\n",
      "data[0]:  36.6217041015625\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  26079.071984052658\n",
      "=============== Change day 61 ===============\n",
      "data[0]:  54.95736312866211\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  38413.7630443573\n",
      "=============== Change day 62 ===============\n",
      "data[0]:  88.99311065673828\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  45938.585339307785\n",
      "=============== Change day 63 ===============\n",
      "data[0]:  95.12633514404297\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  42477.24834346771\n",
      "=============== Change day 64 ===============\n",
      "data[0]:  97.10648345947266\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  36789.980182647705\n",
      "=============== Change day 65 ===============\n",
      "data[0]:  129.78628540039062\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  50944.001735687256\n",
      "=============== Change day 66 ===============\n",
      "data[0]:  92.74156188964844\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  32077.788236618042\n",
      "=============== Change day 67 ===============\n",
      "data[0]:  28.115285873413086\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  24250.1238117218\n",
      "=============== Change day 68 ===============\n",
      "data[0]:  53.763275146484375\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  44135.06969642639\n",
      "=============== Change day 69 ===============\n",
      "data[0]:  114.82623291015625\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  41583.4385099411\n",
      "=============== Change day 70 ===============\n",
      "data[0]:  91.77882385253906\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  41428.35395240784\n",
      "=============== Change day 71 ===============\n",
      "data[0]:  42.370994567871094\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  59441.83895111084\n",
      "=============== Change day 72 ===============\n",
      "data[0]:  124.08091735839844\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  42312.7827796936\n",
      "=============== Change day 73 ===============\n",
      "data[0]:  107.52812957763672\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  28224.916858673096\n",
      "=============== Change day 74 ===============\n",
      "data[0]:  87.93712615966797\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  29025.224644184113\n",
      "=============== Change day 75 ===============\n",
      "data[0]:  74.01739501953125\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  45629.39751625061\n",
      "=============== Change day 76 ===============\n",
      "data[0]:  90.21026611328125\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  34900.58019542694\n",
      "=============== Change day 77 ===============\n",
      "data[0]:  38.436927795410156\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  39943.13624382019\n",
      "=============== Change day 78 ===============\n",
      "data[0]:  95.0247802734375\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  40235.197582006454\n",
      "=============== Change day 79 ===============\n",
      "data[0]:  103.42498016357422\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  3\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  47512.063378334045\n",
      "=============== Change day 80 ===============\n",
      "data[0]:  148.37033081054688\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  34793.67204093933\n",
      "=============== Change day 81 ===============\n",
      "data[0]:  92.15673065185547\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  32034.49321079254\n",
      "=============== Change day 82 ===============\n",
      "data[0]:  32.85133361816406\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  8\n",
      "pred_day_revenue:  34206.87025642395\n",
      "=============== Change day 83 ===============\n",
      "data[0]:  133.7110137939453\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  32310.80112171173\n",
      "=============== Change day 84 ===============\n",
      "data[0]:  90.58489990234375\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  34296.00825357437\n",
      "=============== Change day 85 ===============\n",
      "data[0]:  108.63831329345703\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  38130.29679942131\n",
      "=============== Change day 86 ===============\n",
      "data[0]:  136.16903686523438\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  39766.62462043762\n",
      "=============== Change day 87 ===============\n",
      "data[0]:  104.41881561279297\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  32340.07781600952\n",
      "=============== Change day 88 ===============\n",
      "data[0]:  64.45718383789062\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  29688.209612846375\n",
      "=============== Change day 89 ===============\n",
      "data[0]:  64.48871612548828\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  26622.340808153152\n",
      "=============== Change day 90 ===============\n",
      "data[0]:  100.79769897460938\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  31532.946571350098\n",
      "=============== Change day 91 ===============\n",
      "data[0]:  99.65918731689453\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  50160.43268203735\n",
      "=============== Change day 92 ===============\n",
      "data[0]:  140.15945434570312\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  37584.656297683716\n",
      "=============== Change day 93 ===============\n",
      "data[0]:  64.76478576660156\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  44557.50339126587\n",
      "=============== Change day 94 ===============\n",
      "data[0]:  81.81709289550781\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  38306.27400970459\n",
      "=============== Change day 95 ===============\n",
      "data[0]:  121.23915100097656\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  25836.880827903748\n",
      "=============== Change day 96 ===============\n",
      "data[0]:  142.92955017089844\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  36473.05119514465\n",
      "=============== Change day 97 ===============\n",
      "data[0]:  44.58390426635742\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  30610.18796157837\n",
      "=============== Change day 98 ===============\n",
      "data[0]:  97.96752166748047\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  42205.0313065052\n",
      "=============== Change day 99 ===============\n",
      "data[0]:  109.26665496826172\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  53222.18293762207\n",
      "=============== Change day 100 ===============\n",
      "data[0]:  87.32434844970703\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  51840.02507400513\n",
      "=============== Change day 101 ===============\n",
      "data[0]:  75.96353149414062\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  44874.82472229004\n",
      "=============== Change day 102 ===============\n",
      "data[0]:  73.19605255126953\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  33093.664890766144\n",
      "=============== Change day 103 ===============\n",
      "data[0]:  88.551513671875\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  36012.38338470459\n",
      "=============== Change day 104 ===============\n",
      "data[0]:  104.65511322021484\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  38750.58494949341\n",
      "=============== Change day 105 ===============\n",
      "data[0]:  107.55524444580078\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  71839.40678668022\n",
      "=============== Change day 106 ===============\n",
      "data[0]:  68.76969909667969\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  5\n",
      "df_weekdays[i]:  10\n",
      "pred_day_revenue:  43822.50555229187\n",
      "=============== Change day 107 ===============\n",
      "data[0]:  49.15168762207031\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  54239.12899494171\n",
      "=============== Change day 108 ===============\n",
      "data[0]:  101.11385345458984\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  34666.673771858215\n",
      "=============== Change day 109 ===============\n",
      "data[0]:  132.44334411621094\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  24052.766941070557\n",
      "=============== Change day 110 ===============\n",
      "data[0]:  53.585025787353516\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  4\n",
      "df_weekdays[i]:  10\n",
      "pred_day_revenue:  32175.317766189575\n",
      "=============== Change day 111 ===============\n",
      "data[0]:  76.00700378417969\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  3\n",
      "df_weekdays[i]:  7\n",
      "pred_day_revenue:  35194.0739364624\n",
      "=============== Change day 112 ===============\n",
      "data[0]:  135.45530700683594\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  52756.00581359863\n",
      "=============== Change day 113 ===============\n",
      "data[0]:  94.63326263427734\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  49372.56237030029\n",
      "=============== Change day 114 ===============\n",
      "data[0]:  86.28644561767578\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  62575.12938308716\n",
      "=============== Change day 115 ===============\n",
      "data[0]:  68.81651306152344\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  30742.678408145905\n",
      "=============== Change day 116 ===============\n",
      "data[0]:  35.302608489990234\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  29611.417211532593\n",
      "=============== Change day 117 ===============\n",
      "data[0]:  168.89974975585938\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  39646.85844993591\n",
      "=============== Change day 118 ===============\n",
      "data[0]:  150.24038696289062\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  38710.77771234512\n",
      "=============== Change day 119 ===============\n",
      "data[0]:  97.61854553222656\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  61143.31106567383\n",
      "=============== Change day 120 ===============\n",
      "data[0]:  112.62059783935547\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  43211.07524871826\n",
      "=============== Change day 121 ===============\n",
      "data[0]:  79.79920959472656\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  52687.76283454895\n",
      "=============== Change day 122 ===============\n",
      "data[0]:  77.47550964355469\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  49120.33220529556\n",
      "=============== Change day 123 ===============\n",
      "data[0]:  117.18254852294922\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  43980.30155944824\n",
      "=============== Change day 124 ===============\n",
      "data[0]:  105.00038146972656\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  48636.76008605957\n",
      "=============== Change day 125 ===============\n",
      "data[0]:  100.73869323730469\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  39810.94821166992\n",
      "=============== Change day 126 ===============\n",
      "data[0]:  85.3904037475586\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  46630.51039123535\n",
      "=============== Change day 127 ===============\n",
      "data[0]:  92.4968490600586\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  52332.5954284668\n",
      "=============== Change day 128 ===============\n",
      "data[0]:  137.13314819335938\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  63283.00956916809\n",
      "=============== Change day 129 ===============\n",
      "data[0]:  83.61993408203125\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  8\n",
      "pred_day_revenue:  32365.98843383789\n",
      "=============== Change day 130 ===============\n",
      "data[0]:  71.62185668945312\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  38315.896883010864\n",
      "=============== Change day 131 ===============\n",
      "data[0]:  6.014317512512207\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  8\n",
      "pred_day_revenue:  35768.95016479492\n",
      "=============== Change day 132 ===============\n",
      "data[0]:  99.18285369873047\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  32499.40665435791\n",
      "=============== Change day 133 ===============\n",
      "data[0]:  49.889892578125\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  59050.25671958923\n",
      "=============== Change day 134 ===============\n",
      "data[0]:  135.34088134765625\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  50842.60799026489\n",
      "=============== Change day 135 ===============\n",
      "data[0]:  127.65435791015625\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  44387.31913852692\n",
      "=============== Change day 136 ===============\n",
      "data[0]:  101.56800079345703\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  38015.21434259415\n",
      "=============== Change day 137 ===============\n",
      "data[0]:  82.3192138671875\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  48708.275482177734\n",
      "=============== Change day 138 ===============\n",
      "data[0]:  111.3953857421875\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  46394.65440297127\n",
      "=============== Change day 139 ===============\n",
      "data[0]:  88.38177490234375\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  45480.15026855469\n",
      "=============== Change day 140 ===============\n",
      "data[0]:  61.1408805847168\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  46512.056564331055\n",
      "=============== Change day 141 ===============\n",
      "data[0]:  122.00000762939453\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  51059.00644183159\n",
      "=============== Change day 142 ===============\n",
      "data[0]:  149.76214599609375\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  46371.27030944824\n",
      "=============== Change day 143 ===============\n",
      "data[0]:  94.83707427978516\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  26923.61674809456\n",
      "=============== Change day 144 ===============\n",
      "data[0]:  85.9112548828125\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  37169.15304374695\n",
      "=============== Change day 145 ===============\n",
      "data[0]:  28.70844841003418\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  33724.5853703022\n",
      "=============== Change day 146 ===============\n",
      "data[0]:  70.73361206054688\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  34578.16123962402\n",
      "=============== Change day 147 ===============\n",
      "data[0]:  32.55968475341797\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  53162.85934638977\n",
      "=============== Change day 148 ===============\n",
      "data[0]:  119.92899322509766\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  42808.77742958069\n",
      "=============== Change day 149 ===============\n",
      "data[0]:  39.36963653564453\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  59695.43259143829\n",
      "=============== Change day 150 ===============\n",
      "data[0]:  76.66449737548828\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  29707.3631772995\n",
      "=============== Change day 151 ===============\n",
      "data[0]:  4.1583380699157715\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  24521.83710861206\n",
      "=============== Change day 152 ===============\n",
      "data[0]:  141.62112426757812\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  37281.14380645752\n",
      "=============== Change day 153 ===============\n",
      "data[0]:  19.29012107849121\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "total_days:  153\n",
      "----- revenue_norm = 37464.2960767746\n",
      "----- revenue = 37464.2960767746\n",
      "----- revenue_norm.shape = (153,)\n",
      "----- revenue.shape = (153,)\n",
      "----- final_data.shape = (153, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\" 上述在讀回 np 資訊\"\n",
    "\n",
    "total_days = 0\n",
    "pred_day_revenue = 0\n",
    "total_weekdays = 0\n",
    "total_weekends = 0\n",
    "total_cancel = 0\n",
    "total_adr = 0\n",
    "\n",
    "\n",
    "bef_year = df_year[0]\n",
    "bef_month = df_month[0]\n",
    "bef_week = df_week[0]\n",
    "\n",
    "bef_date = df_date[0]\n",
    "pred_revenue_daylist = []\n",
    "#             adr_daylist = []\n",
    "#             cancel_daylist = []\n",
    "#             weekends_daylist = []\n",
    "#             weekdays_daylist = []\n",
    "\n",
    "for i, data in enumerate(npy_data):\n",
    "\n",
    "    if df_year[i] != bef_year or df_month[i] != bef_month or df_week[i] != bef_week or df_date[i] != bef_date or (i+1) ==len(npy_data):\n",
    "        bef_year = df_year[i]\n",
    "        bef_month = df_month[i]\n",
    "        bef_week = df_week[i]\n",
    "        bef_date = df_date[i]\n",
    "\n",
    "        pred_revenue_daylist.append(pred_day_revenue)\n",
    "        print(\"pred_day_revenue: \",pred_day_revenue)\n",
    "\n",
    "        total_days += 1\n",
    "        print(\"=============== Change day %d ===============\"%total_days)\n",
    "\n",
    "\n",
    "#                     cancel_daylist.append(total_cancel)\n",
    "#                     weekends_daylist.append(total_weekends)\n",
    "#                     weekdays_daylist.append(total_weekdays)\n",
    "#                     adr_daylist.append(total_adr)\n",
    "\n",
    "#                     total_weekdays = 0\n",
    "#                     total_weekends = 0\n",
    "#                     total_cancel = 0\n",
    "#                     total_adr = 0\n",
    "\n",
    "        pred_day_revenue = data[0] * (1-data[1]) * (df_weekend[i] + df_weekdays[i])\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"data[0]: \",data[0])\n",
    "        print(\"(1-data[1]): \",(1-data[1]))\n",
    "        print(\"df_weekend[i]: \",df_weekend[i])\n",
    "        print(\"df_weekdays[i]: \",df_weekdays[i])\n",
    "\n",
    "\n",
    "        \n",
    "    else :\n",
    "        pred_revenue = data[0] * (1-data[1]) * (df_weekend[i] + df_weekdays[i])\n",
    "\n",
    "#         print(\"data[0]: \",data[0])\n",
    "#         print(\"(1-data[1]): \",(1-data[1]))\n",
    "#         print(\"df_weekend[i]: \",df_weekend[i])\n",
    "#         print(\"df_weekdays[i]: \",df_weekdays[i])\n",
    "\n",
    "        pred_day_revenue += pred_revenue\n",
    "#         print(\"pred_day_revenue: \",pred_day_revenue)\n",
    "\n",
    "print(\"total_days: \",total_days)\n",
    "\n",
    "revenue = np.array(pred_revenue_daylist)\n",
    "revenue_norm = revenue\n",
    "# for i in range(len(revenue)):\n",
    "    \n",
    "#     revenue_norm[i] = (revenue[i]-min(revenue))/(max(revenue)-min(revenue))\n",
    "\n",
    "#             weekdays = np.array(weekdays_daylist)\n",
    "#             weekends = np.array(weekends_daylist)\n",
    "#             cancel = np.array(cancel_daylist)\n",
    "#             adr = np.array(adr_daylist)\n",
    "print(\"----- revenue_norm = {}\".format(revenue_norm[0]))\n",
    "print(\"----- revenue = {}\".format(revenue[0]))\n",
    "final_data = np.zeros(shape = (revenue.shape[0],1) )\n",
    "final_data[:,0] = revenue_norm[:]\n",
    "#             final_data[:,1] = adr[:]\n",
    "#             final_data[:,2] = cancel[:]\n",
    "#             final_data[:,3] = weekdays[:]\n",
    "#             final_data[:,4] = weekends[:]\n",
    "print(\"----- revenue_norm.shape = {}\".format(revenue_norm.shape))\n",
    "print(\"----- revenue.shape = {}\".format(revenue.shape))\n",
    "\n",
    "#             print(\"----- adr.shape = {}\".format(adr.shape))\n",
    "#             print(\"----- cancel.shape = {}\".format(cancel.shape))\n",
    "#             print(\"----- weekdays.shape = {}\".format(weekdays.shape))\n",
    "#             print(\"----- weekends.shape = {}\".format(weekends.shape))\n",
    "\n",
    "print(\"----- final_data.shape = {}\".format(final_data.shape))\n",
    "\n",
    "if train_valid_test == \"train\" :\n",
    "    df_label = pd.read_csv(os.path.join(\"./data/\", csv_label_name))  \n",
    "    df_label = df_label.fillna(0)\n",
    "    df_label = df_label.iloc[:,1].to_numpy()\n",
    "    print(\"----- df_label.shape = {}\".format(df_label.shape))\n",
    "    print(\"final_data: \",final_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train + valid metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "metric_list = []\n",
    "metric_data = []\n",
    "\n",
    "\n",
    "print(\"final_data.shape\",final_data.shape)\n",
    "print(\"df_label.shape\",df_label.shape)\n",
    "\n",
    "for i, data in enumerate (final_data) :\n",
    "#     if i == 500 :\n",
    "#         break\n",
    "    metric_data.append(np.abs(math.floor(data/10000)-df_label[i]))\n",
    "    print(\"i: \"+str(i)+\", data: \"+str(math.floor(data/10000))+\", label: \"+str(df_label[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(metric_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_data.shape (153, 1)\n",
      "i: 0, data: 3\n",
      "i: 1, data: 3\n",
      "i: 2, data: 3\n",
      "i: 3, data: 2\n",
      "i: 4, data: 3\n",
      "i: 5, data: 2\n",
      "i: 6, data: 2\n",
      "i: 7, data: 5\n",
      "i: 8, data: 5\n",
      "i: 9, data: 4\n",
      "i: 10, data: 3\n",
      "i: 11, data: 3\n",
      "i: 12, data: 5\n",
      "i: 13, data: 5\n",
      "i: 14, data: 3\n",
      "i: 15, data: 4\n",
      "i: 16, data: 4\n",
      "i: 17, data: 2\n",
      "i: 18, data: 3\n",
      "i: 19, data: 3\n",
      "i: 20, data: 4\n",
      "i: 21, data: 4\n",
      "i: 22, data: 3\n",
      "i: 23, data: 3\n",
      "i: 24, data: 1\n",
      "i: 25, data: 1\n",
      "i: 26, data: 3\n",
      "i: 27, data: 6\n",
      "i: 28, data: 6\n",
      "i: 29, data: 3\n",
      "i: 30, data: 4\n",
      "i: 31, data: 4\n",
      "i: 32, data: 2\n",
      "i: 33, data: 3\n",
      "i: 34, data: 4\n",
      "i: 35, data: 4\n",
      "i: 36, data: 4\n",
      "i: 37, data: 4\n",
      "i: 38, data: 3\n",
      "i: 39, data: 3\n",
      "i: 40, data: 4\n",
      "i: 41, data: 3\n",
      "i: 42, data: 3\n",
      "i: 43, data: 4\n",
      "i: 44, data: 4\n",
      "i: 45, data: 3\n",
      "i: 46, data: 3\n",
      "i: 47, data: 3\n",
      "i: 48, data: 3\n",
      "i: 49, data: 4\n",
      "i: 50, data: 4\n",
      "i: 51, data: 4\n",
      "i: 52, data: 2\n",
      "i: 53, data: 3\n",
      "i: 54, data: 5\n",
      "i: 55, data: 3\n",
      "i: 56, data: 2\n",
      "i: 57, data: 4\n",
      "i: 58, data: 3\n",
      "i: 59, data: 2\n",
      "i: 60, data: 2\n",
      "i: 61, data: 3\n",
      "i: 62, data: 4\n",
      "i: 63, data: 4\n",
      "i: 64, data: 3\n",
      "i: 65, data: 5\n",
      "i: 66, data: 3\n",
      "i: 67, data: 2\n",
      "i: 68, data: 4\n",
      "i: 69, data: 4\n",
      "i: 70, data: 4\n",
      "i: 71, data: 5\n",
      "i: 72, data: 4\n",
      "i: 73, data: 2\n",
      "i: 74, data: 2\n",
      "i: 75, data: 4\n",
      "i: 76, data: 3\n",
      "i: 77, data: 3\n",
      "i: 78, data: 4\n",
      "i: 79, data: 4\n",
      "i: 80, data: 3\n",
      "i: 81, data: 3\n",
      "i: 82, data: 3\n",
      "i: 83, data: 3\n",
      "i: 84, data: 3\n",
      "i: 85, data: 3\n",
      "i: 86, data: 3\n",
      "i: 87, data: 3\n",
      "i: 88, data: 2\n",
      "i: 89, data: 2\n",
      "i: 90, data: 3\n",
      "i: 91, data: 5\n",
      "i: 92, data: 3\n",
      "i: 93, data: 4\n",
      "i: 94, data: 3\n",
      "i: 95, data: 2\n",
      "i: 96, data: 3\n",
      "i: 97, data: 3\n",
      "i: 98, data: 4\n",
      "i: 99, data: 5\n",
      "i: 100, data: 5\n",
      "i: 101, data: 4\n",
      "i: 102, data: 3\n",
      "i: 103, data: 3\n",
      "i: 104, data: 3\n",
      "i: 105, data: 7\n",
      "i: 106, data: 4\n",
      "i: 107, data: 5\n",
      "i: 108, data: 3\n",
      "i: 109, data: 2\n",
      "i: 110, data: 3\n",
      "i: 111, data: 3\n",
      "i: 112, data: 5\n",
      "i: 113, data: 4\n",
      "i: 114, data: 6\n",
      "i: 115, data: 3\n",
      "i: 116, data: 2\n",
      "i: 117, data: 3\n",
      "i: 118, data: 3\n",
      "i: 119, data: 6\n",
      "i: 120, data: 4\n",
      "i: 121, data: 5\n",
      "i: 122, data: 4\n",
      "i: 123, data: 4\n",
      "i: 124, data: 4\n",
      "i: 125, data: 3\n",
      "i: 126, data: 4\n",
      "i: 127, data: 5\n",
      "i: 128, data: 6\n",
      "i: 129, data: 3\n",
      "i: 130, data: 3\n",
      "i: 131, data: 3\n",
      "i: 132, data: 3\n",
      "i: 133, data: 5\n",
      "i: 134, data: 5\n",
      "i: 135, data: 4\n",
      "i: 136, data: 3\n",
      "i: 137, data: 4\n",
      "i: 138, data: 4\n",
      "i: 139, data: 4\n",
      "i: 140, data: 4\n",
      "i: 141, data: 5\n",
      "i: 142, data: 4\n",
      "i: 143, data: 2\n",
      "i: 144, data: 3\n",
      "i: 145, data: 3\n",
      "i: 146, data: 3\n",
      "i: 147, data: 5\n",
      "i: 148, data: 4\n",
      "i: 149, data: 5\n",
      "i: 150, data: 2\n",
      "i: 151, data: 2\n",
      "i: 152, data: 3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "result = []\n",
    "print(\"final_data.shape\",final_data.shape)\n",
    "# print(\"df_label.shape\",df_label.shape)\n",
    "\n",
    "for i, data in enumerate (final_data) :\n",
    "    print(\"i: \"+str(i)+\", data: \"+str(math.floor(data/10000)))\n",
    "    result.append(math.floor(data/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of prediction\n"
     ]
    }
   ],
   "source": [
    "df_test_nolabel = pd.read_csv(\"./data/test_nolabel.csv\")  \n",
    "df_test_nolabel = df_test_nolabel.iloc[:,:].to_numpy()\n",
    "\n",
    "with open(os.path.join(\"./test_pred_metric_tsai_ver1.csv\"), 'w') as f:\n",
    "    f.write('arrival_date,label\\n')\n",
    "    for i, y in  enumerate(result):\n",
    "        f.write('{},{}\\n'.format(df_test_nolabel[i][0], y))\n",
    "print(\"End of prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 底下是想在最後再用NN做一次訓練，這樣好像會怪怪，先不考慮"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Final_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, label = False, train_valid_test = None):\n",
    "        # 要先把 __init__() 括弧裡面的東西給入 self.\n",
    "        print(\"path: \",path)\n",
    "\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "        self.train_valid_test = train_valid_test\n",
    "        \n",
    "        if self.label:\n",
    "            csv_data_name = \"train.csv\"\n",
    "            csv_label_name = \"train_label.csv\"\n",
    "            \n",
    "            self.df_data = pd.read_csv(os.path.join(self.path, csv_data_name))  \n",
    "            \n",
    "            \n",
    "            self.df_year = self.df_data[\"arrival_date_year\"].iloc[:].to_numpy()\n",
    "            self.df_month = self.df_data[\"arrival_date_month\"].iloc[:].to_numpy()\n",
    "            self.df_week = self.df_data[\"arrival_date_week_number\"].iloc[:].to_numpy()\n",
    "            self.df_date = self.df_data[\"arrival_date_day_of_month\"].iloc[:].to_numpy()\n",
    "            print(\"----- df_date.shape = {}\".format(self.df_date.shape))\n",
    "            \n",
    "            \n",
    "            self.df_weekend = self.df_data[\"stays_in_weekend_nights\"].iloc[:].to_numpy()\n",
    "            self.df_weekdays = self.df_data[\"stays_in_week_nights\"].iloc[:].to_numpy()\n",
    "\n",
    "\n",
    "            npy_train_data = np.load(\"./npy_file/train_prediction.npy\")\n",
    "            npy_valid_data = np.load(\"./npy_file/valid_prediction.npy\")\n",
    "            print(\"----- npy_train_data.shape = {}\".format(npy_train_data.shape))\n",
    "            print(\"----- npy_valid_data.shape = {}\".format(npy_valid_data.shape))\n",
    "            print(\"========================: \",(npy_train_data.shape[0] + npy_valid_data.shape[0]))\n",
    "            \n",
    "            npy_data = np.zeros(shape = ((npy_train_data.shape[0] + npy_valid_data.shape[0]), 2))\n",
    "            print(\"----- npy_data.shape = {}\".format(npy_data.shape))\n",
    "\n",
    "            npy_data[:npy_train_data.shape[0],:] = npy_train_data [:,:]\n",
    "            npy_data[npy_train_data.shape[0]:,:] = npy_valid_data [:,:]\n",
    "\n",
    "            print(\"----- npy_data.shape = {}\".format(npy_data.shape))\n",
    "        else :\n",
    "\n",
    "            csv_data_name = \"test.csv\"\n",
    "            csv_label_name = \"test_nolabel.csv\"\n",
    "\n",
    "            self.df_data = pd.read_csv(os.path.join(\"./data/\", csv_data_name))  \n",
    "\n",
    "\n",
    "            self.df_year = self.df_data[\"arrival_date_year\"].iloc[:].to_numpy()\n",
    "            self.df_month = self.df_data[\"arrival_date_month\"].iloc[:].to_numpy()\n",
    "            self.df_week = self.df_data[\"arrival_date_week_number\"].iloc[:].to_numpy()\n",
    "            self.df_date = self.df_data[\"arrival_date_day_of_month\"].iloc[:].to_numpy()\n",
    "            print(\"----- df_date.shape = {}\".format(self.df_date.shape))\n",
    "\n",
    "\n",
    "            self.df_weekend = self.df_data[\"stays_in_weekend_nights\"].iloc[:].to_numpy()\n",
    "            self.df_weekdays = self.df_data[\"stays_in_week_nights\"].iloc[:].to_numpy()\n",
    "\n",
    "\n",
    "            npy_data = np.load(\"./npy_file/test_prediction.npy\")\n",
    "            print(\"----- npy_data.shape = {}\".format(npy_data.shape))\n",
    "\n",
    "\n",
    "\n",
    "        self.total_days = 0\n",
    "        pred_day_revenue = 0\n",
    "        total_weekdays = 0\n",
    "        total_weekends = 0\n",
    "        total_cancel = 0\n",
    "        total_adr = 0\n",
    "\n",
    "\n",
    "        bef_year = self.df_year[0]\n",
    "        bef_month = self.df_month[0]\n",
    "        bef_week = self.df_week[0]\n",
    "\n",
    "        bef_date = self.df_date[0]\n",
    "        pred_revenue_daylist = []\n",
    "#         adr_daylist = []\n",
    "#             cancel_daylist = []\n",
    "#             weekends_daylist = []\n",
    "#             weekdays_daylist = []\n",
    "\n",
    "        for i, data in enumerate(npy_data):\n",
    "\n",
    "            if self.df_year[i] != bef_year or self.df_month[i] != bef_month or self.df_week[i] != bef_week or self.df_date[i] != bef_date or (i+1) ==len(npy_data):\n",
    "                bef_year = self.df_year[i]\n",
    "                bef_month = self.df_month[i]\n",
    "                bef_week = self.df_week[i]\n",
    "                bef_date = self.df_date[i]\n",
    "\n",
    "                pred_revenue_daylist.append(pred_day_revenue)\n",
    "#                     cancel_daylist.append(total_cancel)\n",
    "#                     weekends_daylist.append(total_weekends)\n",
    "#                     weekdays_daylist.append(total_weekdays)\n",
    "#                 adr_daylist.append(total_adr)\n",
    "                self.total_days += 1\n",
    "    \n",
    "                print(\"=============== Change day %d ===============\"%i)\n",
    "\n",
    "\n",
    "#                     total_weekdays = 0\n",
    "#                     total_weekends = 0\n",
    "#                     total_cancel = 0\n",
    "                total_adr = 0\n",
    "                pred_day_revenue  = data[0] * (1-data[1]) * (self.df_weekend[i] + self.df_weekdays[i])\n",
    "\n",
    "                \n",
    "            else:\n",
    "                pred_revenue = data[0] * (1-data[1]) * (self.df_weekend[i] + self.df_weekdays[i])\n",
    "#                     total_weekdays += self.df_weekdays[i]\n",
    "#                     total_weekends += self.df_weekend[i]\n",
    "#                     total_cancel += (1-data[1])\n",
    "                total_adr += data[0]\n",
    "                pred_day_revenue += pred_revenue\n",
    "\n",
    "        print(\"total_days: \",self.total_days)\n",
    "\n",
    "        self.revenue = np.array(pred_revenue_daylist)\n",
    "        \n",
    "#         self.revenue_norm = self.revenue\n",
    "#         for i in range(len(self.revenue)):\n",
    "#             self.revenue_norm[i] = (self.revenue[i]-min(self.revenue))/(max(self.revenue)-min(self.revenue))\n",
    "\n",
    "#             self.weekdays = np.array(weekdays_daylist)\n",
    "#             self.weekends = np.array(weekends_daylist)\n",
    "#             self.cancel = np.array(cancel_daylist)\n",
    "#         self.adr = np.array(adr_daylist)\n",
    "\n",
    "        self.final_data = np.zeros(shape = (self.revenue.shape[0],1) )\n",
    "        self.final_data[:,0] = self.revenue[:]\n",
    "#         self.final_data[:,1] = self.adr[:]\n",
    "#             self.final_data[:,2] = self.cancel[:]\n",
    "#             self.final_data[:,3] = self.weekdays[:]\n",
    "#             self.final_data[:,4] = self.weekends[:]\n",
    "#         print(\"----- revenue_norm.shape = {}\".format(self.revenue_norm.shape))\n",
    "        print(\"----- revenue.shape = {}\".format(self.revenue.shape))\n",
    "\n",
    "#         print(\"----- adr.shape = {}\".format(self.adr.shape))\n",
    "#             print(\"----- cancel.shape = {}\".format(self.cancel.shape))\n",
    "#             print(\"----- weekdays.shape = {}\".format(self.weekdays.shape))\n",
    "#             print(\"----- weekends.shape = {}\".format(self.weekends.shape))\n",
    "\n",
    "        print(\"----- final_data.shape = {}\".format(self.final_data.shape))\n",
    "        \n",
    "        if self.label:\n",
    "        \n",
    "            self.df_label = pd.read_csv(os.path.join(self.path, csv_label_name))  \n",
    "            self.df_label = self.df_label.fillna(0)\n",
    "            self.df_label = self.df_label.iloc[:,1].to_numpy()\n",
    "            print(\"----- df_label.shape = {}\".format(self.df_label.shape))\n",
    "            print(\"final_data: \",self.final_data.shape)\n",
    "\n",
    "\n",
    "            spl = int(0.2*self.final_data.shape[0])\n",
    "            print(\"spl: \",spl)\n",
    "\n",
    "            print(\"----- final_data.shape = {}\".format(self.final_data.shape))\n",
    "            print(\"----- df_label.shape = {}\".format(self.df_label.shape))\n",
    "\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "            if self.train_valid_test == \"train\" :\n",
    "                self.data_doggy = np.zeros( shape = (spl, self.final_data.shape[1]) )\n",
    "                self.label_doggy = np.zeros( shape = (spl) )\n",
    "\n",
    "                self.data_doggy = self.final_data[spl:,:]\n",
    "                self.label_doggy = self.df_label[spl:]\n",
    "                print(\"train_data_shape: \",self.data_doggy.shape)\n",
    "                print(\"train_label_shape: \",self.label_doggy.shape)\n",
    "\n",
    "\n",
    "            elif self.train_valid_test == \"valid\" :\n",
    "                self.data_doggy = np.zeros( shape = ( int(self.final_data.shape[0]-spl), self.final_data.shape[1]) )\n",
    "                self.label_doggy = np.zeros( shape = ( int(self.df_label.shape[0]-spl)) )\n",
    "\n",
    "                self.data_doggy = self.final_data[:spl,:]\n",
    "                self.label_doggy = self.df_label[:spl]\n",
    "                print(\"valid_data_shape: \",self.data_doggy.shape)\n",
    "                print(\"valid_label_shape: \",self.label_doggy.shape)\n",
    "            \n",
    "        else :\n",
    "            self.data_doggy = np.zeros( shape = (self.final_data.shape[0], self.final_data.shape[1]) )\n",
    "            print(\"data_shape: \",self.data_doggy.shape)\n",
    "            self.data_doggy = self.final_data[:,:]\n",
    "#########################################################################################################################\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.data_doggy.shape[0]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "#         print(\"self.path: \", self.path)\n",
    "        if self.label:\n",
    "#             print(\"revenue: \",self.revenue.shape)\n",
    "#             print(\"label: \",self.df_label.shape)\n",
    "            \n",
    "            final_data = self.data_doggy[index]\n",
    "#             label = self.df_label[index]\n",
    "            label = self.label_doggy[index]\n",
    "            \n",
    "#             print(\"revenue: \",revenue.shape)\n",
    "#             print(\"label: \",revenue.shape)\n",
    "            final_data = math.floor(final_data/10000)\n",
    "#             print(\"final_data: \",final_data.shape)\n",
    "\n",
    "            final_data = torch.tensor(final_data)\n",
    "            label = torch.tensor(label)\n",
    "\n",
    "            return final_data, label\n",
    "        \n",
    "        # 其實就是 testing\n",
    "        else:\n",
    "            final_data = self.data_doggy[index]\n",
    "            final_data = math.floor(final_data/10000)\n",
    "            final_data = torch.tensor(final_data)\n",
    "\n",
    "\n",
    "            return final_data\n",
    "\n",
    "    def get_comparison(self):\n",
    "        return self.revenue, self.df_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size\n",
    "metric_criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==================== Training Dataset =========================\")\n",
    "train_dataset_final = Final_Dataset(path = \"./data/\", label = True, train_valid_test = \"train\")\n",
    "\n",
    "print(\"\\n==================== Valid Dataset =========================\")\n",
    "valid_dataset_final = Final_Dataset(path = \"./data/\", label = True, train_valid_test = \"valid\")\n",
    "\n",
    "print(\"\\n==================== Testing Dataset =========================\")\n",
    "test_dataset_final = Final_Dataset(path = \"./data/\", label = False, train_valid_test = \"test\")\n",
    "\n",
    "print(\"\\n==================== Dataloader =========================\")\n",
    "train_loader_final = DataLoader(train_dataset_final, batch_size=batch_size, shuffle=True)\n",
    "valid_loader_final = DataLoader(valid_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "test_loader_final = DataLoader(test_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"\\n==================== Done =========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================== training data ====================\")\n",
    "metric_list = []\n",
    "for i, (data, label) in enumerate(train_loader_final):\n",
    "    data = data.cuda().float()\n",
    "    label = label.cuda().float()\n",
    "    metric_list.append(metric_criterion(data, label).cpu().numpy())\n",
    "    for j in range(batch_size):\n",
    "        print(\"file_num: \"+str(i*batch_size+j)+\", data: \"+str(data[j])+\", label: \"+str(label[j].item()))\n",
    "# #         print(\"\\ndata: \",data[j])\n",
    "# #         print(\"label: \",label[j])\n",
    "print(\"training L1 loss: \",np.mean(metric_list))\n",
    "\n",
    "print(\"==================== validation data ====================\")\n",
    "metric_list = []\n",
    "for i, (data, label) in enumerate(valid_loader_final):\n",
    "    data = data.cuda().float()\n",
    "    label = label.cuda().float()\n",
    "    metric_list.append(metric_criterion(data, label).cpu().numpy())\n",
    "    for j in range(batch_size):\n",
    "        print(\"file_num: \"+str(i*batch_size+j)+\", data: \"+str(data[j])+\", label: \"+str(label[j].item()))\n",
    "# #         print(\"\\ndata: \",data[j])\n",
    "# #         print(\"label: \",label[j])\n",
    "print(\"validation L1 loss: \",np.mean(metric_list))\n",
    "\n",
    "\n",
    "print(\"==================== testing data ====================\")\n",
    "for i, data_batch in enumerate(test_loader_final):\n",
    "    \n",
    "#     print(\"data: \",data.shape)\n",
    "    for j, data in enumerate(data_batch):\n",
    "        print(\"file_num: \"+str(i*batch_size+j)+\", data: \"+str(data))\n",
    "#         print(\"\\ndata: \",data[j])\n",
    "#         print(\"label: \",label[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(test_loader_final):\n",
    "#     print(\"i: \",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Final_Model(nn.Module):\n",
    "    def __init__(self, in_feature, hidden_layer, out_feature=1):\n",
    "        super(Final_Model, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_feature, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(hidden_layer, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(hidden_layer, out_feature),\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"x: \",x.shape)\n",
    "        output = self.layer(x.unsqueeze(1))\n",
    "#         print(\"output: \",output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Final_Model(in_feature = 1, hidden_layer = 16).cuda()\n",
    "# final_criterion = nn.CrossEntropyLoss()\n",
    "optimizer_final = optim.Adam(final_model.parameters())\n",
    "# optimizer_final = optim.SGD(final_model.parameters(),lr = 0.1 ,weight_decay=0.0001)\n",
    "# optimizer_final = optim.SGD(final_model.parameters(),lr = 0.01, momentum=0.9 ,weight_decay=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final\n",
    "\n",
    "train_acc_max = 0.0\n",
    "valid_acc_max = 0.0\n",
    "\n",
    "train_metric_min = np.inf\n",
    "valid_metric_min = np.inf\n",
    "\n",
    "max_epoch = 500000\n",
    "metric = []\n",
    "\n",
    "for epoch in range(0, max_epoch):    \n",
    "    epoch_start_time = time.time()\n",
    "    loss_final = 0.0\n",
    "    bce_mean = 0.0\n",
    "    bce = []\n",
    "    train_acc = 0.0\n",
    "    valid_acc = 0.0\n",
    "    \n",
    "    metric_mean_train = 0.0\n",
    "    \n",
    "    final_model.train()\n",
    "    \n",
    "    for i, (data, final) in enumerate(train_loader_final):\n",
    "        optimizer_final.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "\n",
    "        data = data.cuda().float()\n",
    "        final = final.cuda().float()\n",
    "        \n",
    "        prediction_final = final_model(data).squeeze()\n",
    "        \n",
    "#         print(\"data: \",data.shape)\n",
    "#         print(\"prediction_final: \",prediction_final.shape)\n",
    "#         print(\"final: \",final.shape)\n",
    "\n",
    "        loss_final = metric_criterion(prediction_final, final)\n",
    "\n",
    "#         print(\"prediction_final: \",prediction_final)\n",
    "#         print(\"final: \",final)\n",
    "\n",
    "        \n",
    "        loss_final.backward()\n",
    "        optimizer_final.step()\n",
    "        \n",
    "        metric.append(loss_final.item())\n",
    "        \n",
    "        prediction_final = torch.round(prediction_final)\n",
    "        \n",
    "        train_acc += torch.sum(prediction_final == final).cpu().numpy()\n",
    "        print('epoch [%03d/%03d],CE_loss = %2.4f, train_acc = %.3f, %2.1f sec' % (epoch + 1, max_epoch, loss_final, train_acc/(batch_size*(i+1)), time.time()-epoch_start_time), end = '\\r')\n",
    "    \n",
    "    metric_mean_train = np.mean(metric)\n",
    "    if metric_mean_train < train_metric_min :\n",
    "        train_metric_min = metric_mean_train\n",
    "        torch.save(final_model.state_dict(), f'./checkpoints/final_model_train_metric_min.bin')\n",
    "# #         print(\"============================================================\")\n",
    "    \n",
    "    metric = []\n",
    "    metric_mean_valid = 0.0\n",
    "\n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, cancel) in enumerate(valid_loader_final):\n",
    "            \n",
    "            data = data.cuda().float()\n",
    "            cancel = cancel.cuda().float()\n",
    "            \n",
    "            val_pred = final_model(data).squeeze()\n",
    "            \n",
    "            loss_cancel = metric_criterion(val_pred, cancel)\n",
    "            metric.append(loss_cancel.item())\n",
    "            \n",
    "            val_pred = torch.round(val_pred)\n",
    "\n",
    "            valid_acc  += torch.sum(val_pred == cancel).cpu().numpy()\n",
    "            \n",
    "        metric_mean_valid = np.mean(metric)\n",
    "        \n",
    "        if valid_acc > valid_acc_max:\n",
    "            valid_acc_max = valid_acc\n",
    "        \n",
    "        if metric_mean_valid < valid_metric_min :\n",
    "            valid_metric_min = metric_mean_valid\n",
    "            torch.save(final_model.state_dict(), f'./checkpoints/final_model_valid_metric_min.bin')\n",
    "#         print(\"Valid loss: %.3f, valid_acc = %.3f, Valid acc max: %.3f\"%(bce_mean,  valid_acc/int(640*0.2), valid_acc_max/int(640*0.2)))\n",
    "#         print(\"epoch time: %.2f sec\"%(time.time()-epoch_start_time))\n",
    "#         print(\"============================================================\")\n",
    "    print('epoch [%03d/%03d], train_acc = %.3f, valid_acc = %.3f, valid_acc_max = %.3f, metric_train = %.3f, metric_valid = %.3f, metric_valid_min = %.3f,' % (epoch + 1, max_epoch, train_acc/int(640*0.8), valid_acc/int(640*0.2), valid_acc_max/int(640*0.2), metric_mean_train, metric_mean_valid, valid_metric_min), end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.load_state_dict(torch.load('./checkpoints/final_model_valid_metric_min.bin'))\n",
    "final_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_nolabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader_final):\n",
    "        print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "        data = data.cuda().float()\n",
    "        \n",
    "        predict = torch.round(final_model(data).squeeze()).cpu().numpy()\n",
    "        \n",
    "        result.append(predict)\n",
    "\n",
    "    print(\"====================== test_prediction done ======================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.concatenate(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_nolabel[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_nolabel = pd.read_csv(\"./data/test_nolabel.csv\")  \n",
    "df_test_nolabel = df_test_nolabel.iloc[:,:].to_numpy()\n",
    "\n",
    "with open(os.path.join(\"./test_pred_metric_tsai_ver2.csv\"), 'w') as f:\n",
    "    f.write('arrival_date,label\\n')\n",
    "    for i, y in  enumerate(result):\n",
    "        f.write('{},{}\\n'.format(df_test_nolabel[i][0], y))\n",
    "print(\"End of prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

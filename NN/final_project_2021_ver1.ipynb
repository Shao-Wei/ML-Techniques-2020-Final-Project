{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import scipy.misc\n",
    "import scipy.ndimage\n",
    "import imageio\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epoch = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OHE(feature):\n",
    "    return pd.get_dummies(feature)\n",
    "def standarization(feature):\n",
    "    return (feature-np.mean(feature))/np.std(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "1. label 的資料要合併單位至天數才可以用，這邊先不讀，不然列數不同會爆開"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Csv_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, label = False, train_valid_test = None):\n",
    "        \n",
    "        # 要先把 __init__() 括弧裡面的東西給入 self.\n",
    "        print(\"path: \",path)\n",
    "\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "        self.train_valid_test = train_valid_test\n",
    "        \n",
    "        \n",
    "        oheList = ['hotel', 'meal', 'market_segment', 'distribution_channel', 'reserved_room_type', 'assigned_room_type', 'deposit_type', 'customer_type']\n",
    "        standardizeList = ['lead_time', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children', 'babies', 'previous_cancellations', 'previous_bookings_not_canceled', 'booking_changes', 'days_in_waiting_list', 'required_car_parking_spaces', 'total_of_special_requests']\n",
    "        donothingList = ['is_repeated_guest']\n",
    "        dropList = ['ID', 'arrival_date_year', 'arrival_date_month', 'arrival_date_week_number', 'arrival_date_day_of_month', 'country', 'agent', 'company', 'reservation_status', 'reservation_status_date']\n",
    "        \n",
    "        # label_csv_path = True 就是有 label\n",
    "        if self.train_valid_test == \"train\" or self.train_valid_test == \"valid\":\n",
    "            \n",
    "            csv_data_name = \"train.csv\"\n",
    "            csv_label_name = \"train_label.csv\"\n",
    "            \n",
    "            self.df_data = pd.read_csv(os.path.join(self.path, csv_data_name))  \n",
    "            self.df_data = self.df_data.fillna(0)\n",
    "            \n",
    "        elif self.train_valid_test == \"test\" :\n",
    "        \n",
    "            csv_data_name = \"test.csv\"\n",
    "            csv_label_name = \"test_nolabel.csv\"\n",
    "            \n",
    "            self.df_data = pd.read_csv(os.path.join(self.path, csv_data_name))  \n",
    "            \n",
    "            print(\"There's NO adr and is_canceled in testing data.\")\n",
    "            \n",
    "            self.df_data = self.df_data.fillna(0)\n",
    "        else:\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!  error train_valid_test is something wrong !!!!!!!!!!!!!!!!!!!!\")\n",
    "\n",
    "###########################################################################\n",
    "####  讀取 adr 和 is_canceled\n",
    "#########################################################################################################################\n",
    "\n",
    "        if self.train_valid_test == \"train\" :\n",
    "\n",
    "            self.df_adr = self.df_data[\"adr\"].iloc[:72000].to_numpy()\n",
    "            print(\"----- df_adr.shape = {}\".format(self.df_adr.shape))\n",
    "\n",
    "            self.df_cancel = self.df_data[\"is_canceled\"].iloc[:72000].to_numpy()\n",
    "            print(\"----- df_cancel.shape = {}\".format(self.df_cancel.shape))\n",
    "\n",
    "        elif self.train_valid_test == \"valid\" :\n",
    "            self.df_adr = self.df_data[\"adr\"].iloc[72000:].to_numpy()\n",
    "            print(\"----- df_adr.shape = {}\".format(self.df_adr.shape))\n",
    "\n",
    "            self.df_cancel = self.df_data[\"is_canceled\"].iloc[72000:].to_numpy()\n",
    "            print(\"----- df_cancel.shape = {}\".format(self.df_cancel.shape))\n",
    "            \n",
    "#########################################################################################################################\n",
    "    \n",
    "        # 照著討論出的 baseline 去處理資料\n",
    "        key_df_list = list(self.df_data.keys())\n",
    "        data_feature = np.array([])\n",
    "        for k in key_df_list:\n",
    "\n",
    "            if k == \"hotel\" :\n",
    "                print(\"----------------------------------------\\noheList: \",k)\n",
    "                feature = self.df_data[k]\n",
    "                feature = OHE(feature).iloc[:].to_numpy()\n",
    "                print(\"oheList_feature: \",feature.shape)\n",
    "                data_feature = np.array(feature)\n",
    "                print(\"original data_feature: \",data_feature.shape)\n",
    "\n",
    "            # 做 onehot encoding\n",
    "            elif k in oheList :\n",
    "                print(\"----------------------------------------\\noheList: \",k)\n",
    "                feature = self.df_data[k]\n",
    "                feature = OHE(feature).iloc[:].to_numpy()\n",
    "                print(\"oheList_feature: \",feature.shape)\n",
    "                data_feature = np.column_stack((data_feature, feature))\n",
    "\n",
    "            # 做標準化\n",
    "            elif k in standardizeList :\n",
    "                print(\"----------------------------------------\\nstandarizeList: \",k)\n",
    "                feature = self.df_data[k].iloc[:].to_numpy()\n",
    "                feature = standarization(feature)\n",
    "                print(\"standardizeList_feature: \",feature.shape)\n",
    "                data_feature = np.column_stack((data_feature, feature))\n",
    "\n",
    "            # 原封不動給到我們的結果\n",
    "            elif k in donothingList :\n",
    "                print(\"----------------------------------------\\ndonothingList: \",k)\n",
    "                feature = self.df_data[k].iloc[:].to_numpy()\n",
    "                print(\"donothingList_feature: \",feature.shape)\n",
    "                data_feature = np.column_stack((data_feature, feature))\n",
    "\n",
    "            # 直接跳過\n",
    "            elif k in dropList :\n",
    "                print(\"----------------------------------------\\ndropList: \",k)\n",
    "\n",
    "            else:\n",
    "                print(\"----------------------------------------\\nbugggggggggg: \",k)\n",
    "\n",
    "            print(\"total data_feature: \",type(data_feature))\n",
    "            print(\"total data_feature: \",data_feature.shape)\n",
    "\n",
    "###########################################################################\n",
    "####  分割\n",
    "#########################################################################################################################\n",
    "\n",
    "        if self.train_valid_test == \"train\" :\n",
    "            self.df_data = data_feature[:72000,:] # 轉換結果就是我們要用的資料了\n",
    "            print(\"train df_data: \",self.df_data.shape) #這邊是 numpy 形式，而非 pd.DataFrame\n",
    "#             self.df_label = pd.read_csv(os.path.join(self.path, csv_label_name))  \n",
    "#             self.df_label = self.df_label.fillna(0)\n",
    "#             self.df_label = self.df_label.iloc[:,1].to_numpy()\n",
    "#             print(\"----- df_label.shape = {}\".format(self.df_label.shape))\n",
    "            \n",
    "        elif self.train_valid_test == \"valid\" :\n",
    "            self.df_data = data_feature[72000:,:] # 轉換結果就是我們要用的資料了\n",
    "            print(\"valid df_data: \",self.df_data.shape) #這邊是 numpy 形式，而非 pd.DataFrame\n",
    "\n",
    "#             self.df_label = pd.read_csv(os.path.join(self.path, csv_label_name))  \n",
    "#             self.df_label = self.df_label.fillna(0)\n",
    "#             self.df_label = self.df_label.iloc[:,1].to_numpy()\n",
    "#             print(\"----- df_label.shape = {}\".format(self.df_label.shape))\n",
    "            \n",
    "        elif self.train_valid_test == \"test\" :\n",
    "            self.df_data = data_feature[:,:] # 轉換結果就是我們要用的資料了\n",
    "            print(\"test df_data: \",self.df_data.shape) #這邊是 numpy 形式，而非 pd.DataFrame\n",
    "            print(\"\\nThere's no label in testing set, maybe read arrival_date only.\")\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "    # 在 from torch.utils.data import DataLoader, Dataset 中的 DataLoader, Dataset\n",
    "    # 需要 __len__ 及 __getitem__ 兩個函式\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df_data.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "#         print(\"self.path: \", self.path)\n",
    "        if self.label:\n",
    "            data = self.df_data[index]\n",
    "#             label = self.df_label[index]\n",
    "            adr = self.df_adr[index]\n",
    "            cancel = self.df_cancel[index]\n",
    "            \n",
    "#             print(\"data: \",data)\n",
    "            data = torch.tensor(data)\n",
    "#             label = torch.tensor(label)\n",
    "            adr = torch.tensor(adr)\n",
    "            cancel = torch.tensor(cancel)\n",
    "\n",
    "            return data, adr, cancel\n",
    "        # 其實就是 testing\n",
    "        else:\n",
    "            data = self.df_data[index]\n",
    "            data = torch.tensor(data)\n",
    "            \n",
    "            return data\n",
    "        \n",
    "    def get_feature_sum(self):\n",
    "        return self.df_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n==================== Training Dataset=========================\")\n",
    "train_dataset = Csv_Dataset(path = \"./data/\", label = True, train_valid_test = \"train\")\n",
    "\n",
    "print(\"\\n==================== Validation Dataset=========================\")\n",
    "valid_dataset = Csv_Dataset(path = \"./data/\", label = True, train_valid_test = \"valid\")\n",
    "\n",
    "print(\"\\n==================== Testing Dataset=========================\")\n",
    "test_dataset = Csv_Dataset(path = \"./data/\", label = False, train_valid_test = \"test\")\n",
    "\n",
    "print(\"\\n==================== Dataloader =========================\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"\\n==================== Done =========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train feature_num:\",train_dataset.get_feature_sum())\n",
    "print(\"valid feature_num:\",valid_dataset.get_feature_sum())\n",
    "print(\"test feature_num:\",test_dataset.get_feature_sum())\n",
    "\n",
    "if train_dataset.get_feature_sum() == valid_dataset.get_feature_sum() == test_dataset.get_feature_sum() :\n",
    "    feature_num = train_dataset.get_feature_sum()\n",
    "    print(\"========== feature_num: \", feature_num)\n",
    "else :\n",
    "    print(\"Error!! Train, valid, test with different feature !!!\")\n",
    "\n",
    "print(train_dataset.__len__())\n",
    "print(valid_dataset.__len__())\n",
    "print(test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADR_Model(nn.Module):\n",
    "    def __init__(self, in_feature, hidden_layer, out_feature=1):\n",
    "        super(ADR_Model, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_feature, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(hidden_layer, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(hidden_layer, out_feature),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layer(x)\n",
    "#         print(\"output: \",output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cancel_Model(nn.Module):\n",
    "    def __init__(self, in_feature, hidden_layer, out_feature=2):\n",
    "        super(Cancel_Model, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_feature, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(hidden_layer, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer, out_feature),\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layer(x)\n",
    "#         print(\"output: \",output.shape)\n",
    "#         output = F.softmax(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Final_Model(nn.Module):\n",
    "#     def __init__(self, in_feature, hidden_layer, out_feature=2):\n",
    "#         super(Final_Model, self).__init__()\n",
    "\n",
    "#         self.layer = nn.Sequential(\n",
    "#             nn.Linear(in_feature, hidden_layer),\n",
    "#             nn.BatchNorm1d(hidden_layer),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.3),\n",
    "\n",
    "#             nn.Linear(hidden_layer, hidden_layer),\n",
    "#             nn.BatchNorm1d(hidden_layer),\n",
    "#             nn.ReLU(),\n",
    "# #             nn.Dropout(0.3),\n",
    "\n",
    "#             nn.Linear(hidden_layer, out_feature),\n",
    "# #             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         output = self.layer(x)\n",
    "# #         print(\"output: \",output.shape)\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_model = ADR_Model(in_feature = feature_num, hidden_layer = 256 ).cuda()\n",
    "cancel_model = Cancel_Model(in_feature = feature_num, hidden_layer = 128).cuda()\n",
    "# final_model = Final_Model(in_feature = 1, hidden_layer = 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_criterion = nn.MSELoss()\n",
    "cancel_criterion = nn.CrossEntropyLoss()\n",
    "# final_criterion = nn.CrossEntropyLoss()\n",
    "# l1_criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_adr = optim.Adam(adr_model.parameters())\n",
    "\n",
    "# optimizer_cancel = optim.Adam(cancel_model.parameters())\n",
    "optimizer_cancel = optim.SGD(cancel_model.parameters(),lr = 0.01, momentum=0.9 ,weight_decay=0.0001)\n",
    "\n",
    "# optimizer_final = optim.Adam(final_model.parameters())\n",
    "# optimizer_final = optim.SGD(final_model.parameters(),lr = 0.01, momentum=0.9 ,weight_decay=0.0001)\n",
    "optimizer_adr = optim.Adam(adr_model.parameters(),lr = 0.01, weight_decay=0.0001)\n",
    "# optimizer_adr = optim.Adam(adr_model.parameters(),lr = 0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "1. 暫定這個 adr_model，最低的 validation mean MSE 為 693\n",
    "2. 如果 cancel model 要小數點的regression結果，直接把adr train的那段複製到cancel再做以下操作就好了  \n",
    "   1. cancel_criterion改成 BCELOSS\n",
    "   2. cancel_model 最後一層用 nn.Sigmoid，BCELOSS 用 Sigmoid，多分類用的 CE 才使用 softmax\n",
    "   3. 承上， nn.CrossEntropyLoss() 已經包入 softmax 操作了\n",
    "   4. cancel_model 輸出的 feature 改為 1，輸出的這 1 個小數就算是regression結果\n",
    "   5. 備註：其實在 pytorch 中 BCEWithLogitsLoss 就是 BCELOSS 加上 Sigmoid\n",
    "3. 目前因為 valid classification 的結果不錯，所以就用 classification 了\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mse_min = np.inf\n",
    "train_mse_min = np.inf\n",
    "\n",
    "valid_bce_min = np.inf\n",
    "train_bce_min = np.inf\n",
    "\n",
    "train_acc_max = 0.0\n",
    "valid_acc_max = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adr\n",
    "for epoch in range(0, max_epoch):    \n",
    "    epoch_start_time = time.time()\n",
    "    loss = 0.0\n",
    "    mse_mean = 0.0\n",
    "    mse = []\n",
    "    adr_model.train()\n",
    "    \n",
    "    for i, (data, adr, cancel) in enumerate(train_loader):\n",
    "        optimizer_adr.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "        \n",
    "#         print(\"type(data): \",type(data))\n",
    "#         print(\"type(adr): \",type(adr))\n",
    "\n",
    "        data = data.cuda().float()\n",
    "        adr = adr.cuda().float()\n",
    "        \n",
    "        prediction = adr_model(data).squeeze()\n",
    "        \n",
    "#         print(\"data: \",data.shape)\n",
    "#         print(\"prediction: \",prediction.shape)\n",
    "#         print(\"adr: \",adr.shape)\n",
    "\n",
    "        loss = adr_criterion(prediction, adr)\n",
    "\n",
    "#         print(\"prediction: \",prediction)\n",
    "#         print(\"adr: \",adr)\n",
    "#         print(\"loss: \",loss)\n",
    "        \n",
    "        mse.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_adr.step()\n",
    "        \n",
    "        print('epoch [%03d/%03d],MSE = %2.4f, %2.2f sec(s)' % (epoch + 1, max_epoch, loss, time.time()-epoch_start_time), end = '\\r')\n",
    "    \n",
    "    mse_mean = np.mean(mse)\n",
    "    print('epoch [%03d/%03d],train_MSE_mean = %2.4f, %2.2f sec(s)' % (epoch + 1, max_epoch, mse_mean, time.time()-epoch_start_time))\n",
    "    \n",
    "    if mse_mean < train_mse_min :\n",
    "        train_mse_min = mse_mean\n",
    "        torch.save(adr_model.state_dict(), f'./checkpoints/adr_model_train_mse_min.bin')\n",
    "        \n",
    "    \n",
    "    loss = 0.0\n",
    "    mse_mean = 0.0\n",
    "    mse = []\n",
    "    adr_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, adr, cancel) in enumerate(valid_loader):\n",
    "            \n",
    "            data = data.cuda().float()\n",
    "            adr = adr.cuda().float()\n",
    "            \n",
    "            val_pred = adr_model(data).squeeze()\n",
    "            \n",
    "            loss = adr_criterion(val_pred, adr)\n",
    "            mse.append(loss.item())\n",
    "            \n",
    "        mse_mean = np.mean(mse)\n",
    "        \n",
    "        if mse_mean < valid_mse_min and epoch > 10 :\n",
    "            valid_mse_min = mse_mean\n",
    "            torch.save(adr_model.state_dict(), f'./checkpoints/adr_model_valid_mse_min.bin')\n",
    "        print(\"Valid MSE: %.4f, Valid MSE minimum: %.4f \"%(mse_mean, valid_mse_min))\n",
    "        print(\"==================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cancel\n",
    "for epoch in range(0, max_epoch):    \n",
    "    epoch_start_time = time.time()\n",
    "    loss_cancel = 0.0\n",
    "    bce_mean = 0.0\n",
    "    bce = []\n",
    "    train_acc = 0.0\n",
    "    valid_acc = 0.0\n",
    "    \n",
    "    cancel_model.train()\n",
    "    \n",
    "    for i, (data, adr, cancel) in enumerate(train_loader):\n",
    "        optimizer_cancel.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "        \n",
    "#         print(\"type(data): \",type(data))\n",
    "#         print(\"type(cancel): \",type(cancel))\n",
    "\n",
    "        data = data.cuda().float()\n",
    "        cancel = cancel.cuda().long()\n",
    "        \n",
    "        prediction_cancel = cancel_model(data).squeeze()\n",
    "        \n",
    "#         print(\"data: \",data.shape)\n",
    "#         print(\"prediction_cancel: \",prediction_cancel.shape)\n",
    "#         print(\"cancel: \",cancel.shape)\n",
    "\n",
    "        loss_cancel = cancel_criterion(prediction_cancel, cancel)\n",
    "\n",
    "#         print(\"prediction_cancel: \",prediction_cancel)\n",
    "#         print(\"cancel: \",cancel)\n",
    "#         print(\"loss_cancel: \",loss_cancel)\n",
    "        \n",
    "        bce.append(loss_cancel.item())\n",
    "        \n",
    "        loss_cancel.backward()\n",
    "        optimizer_cancel.step()\n",
    "        \n",
    "        train_acc += np.sum(np.argmax(prediction_cancel.cpu().data.numpy(), axis=1) == cancel.cpu().numpy())\n",
    "        print('epoch [%03d/%03d],CE_loss = %2.4f, train_acc = %.3f, %2.1f sec' % (epoch + 1, max_epoch, loss_cancel, train_acc/(batch_size*(i+1)), time.time()-epoch_start_time), end = '\\r')\n",
    "    \n",
    "    bce_mean = np.mean(bce)\n",
    "    print('epoch [%03d/%03d],train_loss = %2.4f, train_acc = %.3f, %2.1f sec' % (epoch + 1, max_epoch, bce_mean, train_acc/train_dataset.__len__(), time.time()-epoch_start_time))\n",
    "    \n",
    "    if train_acc > train_acc_max :\n",
    "        train_acc_max = train_acc\n",
    "        torch.save(cancel_model.state_dict(), f'./checkpoints/cancel_model_train_acc_max.bin')\n",
    "        \n",
    "    \n",
    "    loss_cancel = 0.0\n",
    "    bce_mean = 0.0\n",
    "    bce = []\n",
    "    cancel_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, adr, cancel) in enumerate(valid_loader):\n",
    "            \n",
    "            data = data.cuda().float()\n",
    "            cancel = cancel.cuda().long()\n",
    "            \n",
    "            val_pred = cancel_model(data).squeeze()\n",
    "            \n",
    "            loss_cancel = cancel_criterion(val_pred, cancel)\n",
    "            bce.append(loss_cancel.item())\n",
    "            \n",
    "            valid_acc  += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == cancel.cpu().numpy())\n",
    "            \n",
    "        bce_mean = np.mean(bce)\n",
    "        \n",
    "        if valid_acc > valid_acc_max:\n",
    "            valid_acc_max = valid_acc\n",
    "            torch.save(cancel_model.state_dict(), f'./checkpoints/cancel_model_valid_acc_max.bin')\n",
    "        print(\"Valid loss: %.3f, valid_acc = %.3f, Valid acc max: %.3f\"%(bce_mean,  valid_acc/valid_dataset.__len__(), valid_acc_max/valid_dataset.__len__()))\n",
    "        print(\"epoch time: %.2f sec\"%(time.time()-epoch_start_time))\n",
    "        print(\"============================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load adr, cancel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_model.load_state_dict(torch.load('./checkpoints/adr_model_valid_mse_min.bin'))\n",
    "cancel_model.load_state_dict(torch.load('./checkpoints/cancel_model_valid_acc_max.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_model.eval()\n",
    "cancel_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面的是連原始資料都讀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # npy\n",
    "# epoch_start_time = time.time()\n",
    "# with torch.no_grad():\n",
    "#     for i, (data, adr, cancel) in enumerate(train_loader):\n",
    "#         print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "#         data = data.cuda().float()\n",
    "#         adr = adr.cuda().float()\n",
    "#         cancel = cancel.cuda().long()\n",
    "        \n",
    "#         predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "#         predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "#         concat_data = data.cpu().numpy()\n",
    "\n",
    "# #         print(\"data: \",data.shape)\n",
    "# #         print(\"adr: \",adr.shape)\n",
    "# #         print(\"cancel: \",cancel.shape)\n",
    "# #         print(\"predict_adr: \",predict_adr.shape)\n",
    "# #         print(\"predict_cancel: \",predict_cancel.shape)\n",
    "# #         print(\"concat_data: \",concat_data.shape)\n",
    "        \n",
    "#         npy_data = np.zeros(shape = (train_dataset.__len__(), (concat_data[1].shape[0]+2)))\n",
    "# #         print(\"npy_data: \",npy_data.shape)\n",
    "        \n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),:concat_data.shape[1]] = concat_data[:,:]\n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),concat_data.shape[1]] = predict_adr[:]\n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),(concat_data.shape[1]+1)] = predict_cancel[:]\n",
    "\n",
    "# #         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "# np.save(\"./npy_file/train_prediction.npy\", npy_data)\n",
    "# print(\"\\n====================== train_prediction done ======================\")\n",
    "\n",
    "# epoch_start_time = time.time()\n",
    "# with torch.no_grad():\n",
    "#     for i, (data, adr, cancel) in enumerate(valid_loader):\n",
    "#         print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "#         data = data.cuda().float()\n",
    "#         adr = adr.cuda().float()\n",
    "#         cancel = cancel.cuda().long()\n",
    "        \n",
    "#         predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "#         predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "#         concat_data = data.cpu().numpy()\n",
    "\n",
    "# #         print(\"data: \",data.shape)\n",
    "# #         print(\"adr: \",adr.shape)\n",
    "# #         print(\"cancel: \",cancel.shape)\n",
    "# #         print(\"predict_adr: \",predict_adr.shape)\n",
    "# #         print(\"predict_cancel: \",predict_cancel.shape)\n",
    "# #         print(\"concat_data: \",concat_data.shape)\n",
    "        \n",
    "#         npy_data = np.zeros(shape = (valid_dataset.__len__(), (concat_data[1].shape[0]+2)))\n",
    "# #         print(\"npy_data: \",npy_data.shape)\n",
    "        \n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),:concat_data.shape[1]] = concat_data[:,:]\n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),concat_data.shape[1]] = predict_adr[:]\n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),(concat_data.shape[1]+1)] = predict_cancel[:]\n",
    "\n",
    "# #         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "# np.save(\"./npy_file/valid_prediction.npy\", npy_data)\n",
    "# print(\"\\n====================== valid_prediction done ======================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# npy\n",
    "epoch_start_time = time.time()\n",
    "npy_data = np.zeros(shape = (train_dataset.__len__(), 2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, adr, cancel) in enumerate(train_loader):\n",
    "        print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "        data = data.cuda().float()\n",
    "        \n",
    "        predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "        predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "        concat_data = data.cpu().numpy()\n",
    "\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 0] = predict_adr[:]\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 1] = predict_cancel[:]\n",
    "\n",
    "#         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "np.save(\"./npy_file/train_prediction.npy\", npy_data)\n",
    "print(\"\\nnpy_data.shape: \",npy_data.shape)\n",
    "print(\"====================== train_prediction done ======================\")\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "npy_data = np.zeros(shape = (valid_dataset.__len__(), 2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, adr, cancel) in enumerate(valid_loader):\n",
    "        print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "        data = data.cuda().float()\n",
    "        \n",
    "        predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "        predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "        concat_data = data.cpu().numpy()\n",
    "        \n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 0] = predict_adr[:]\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 1] = predict_cancel[:]\n",
    "\n",
    "#         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "np.save(\"./npy_file/valid_prediction.npy\", npy_data)\n",
    "print(\"\\nnpy_data.shape: \",npy_data.shape)\n",
    "print(\"====================== valid_prediction done ======================\")\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "npy_data = np.zeros(shape = (test_dataset.__len__(), 2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "        data = data.cuda().float()\n",
    "        \n",
    "        predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "        predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "        concat_data = data.cpu().numpy()\n",
    "        \n",
    "#         print(\"data: \",data.shape)\n",
    "#         print(\"adr: \",adr.shape)\n",
    "#         print(\"cancel: \",cancel.shape)\n",
    "#         print(\"predict_adr: \",predict_adr.shape)\n",
    "#         print(\"predict_cancel: \",predict_cancel.shape)\n",
    "#         print(\"concat_data: \",concat_data.shape)\n",
    "#         print(\"npy_data: \",npy_data.shape)\n",
    "        \n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),:concat_data.shape[1]] = concat_data[:,:]\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 0] = predict_adr[:]\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 1] = predict_cancel[:]\n",
    "\n",
    "#         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "np.save(\"./npy_file/test_prediction.npy\", npy_data)\n",
    "print(\"\\nnpy_data.shape: \",npy_data.shape)\n",
    "print(\"====================== test_prediction done ======================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用蔡夯哥算法做predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training + validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_test = \"train\"\n",
    "\n",
    "csv_data_name = \"train.csv\"\n",
    "csv_label_name = \"train_label.csv\"\n",
    "\n",
    "df_data = pd.read_csv(os.path.join(\"./data/\", csv_data_name))  \n",
    "\n",
    "#             if train_valid_test == \"train\" :\n",
    "\n",
    "df_year = df_data[\"arrival_date_year\"].iloc[:].to_numpy()\n",
    "df_month = df_data[\"arrival_date_month\"].iloc[:].to_numpy()\n",
    "df_week = df_data[\"arrival_date_week_number\"].iloc[:].to_numpy()\n",
    "df_date = df_data[\"arrival_date_day_of_month\"].iloc[:].to_numpy()\n",
    "print(\"----- df_date.shape = {}\".format(df_date.shape))\n",
    "\n",
    "#                 print(\"df_week: \",df_wee)\n",
    "\n",
    "df_weekend = df_data[\"stays_in_weekend_nights\"].iloc[:].to_numpy()\n",
    "df_weekdays = df_data[\"stays_in_week_nights\"].iloc[:].to_numpy()\n",
    "\n",
    "\n",
    "npy_train_data = np.load(\"./npy_file/train_prediction.npy\")\n",
    "npy_valid_data = np.load(\"./npy_file/valid_prediction.npy\")\n",
    "print(\"----- npy_train_data.shape = {}\".format(npy_train_data.shape))\n",
    "print(\"----- npy_valid_data.shape = {}\".format(npy_valid_data.shape))\n",
    "print(\"========================: \",(npy_train_data.shape[0] + npy_valid_data.shape[0]))\n",
    "\n",
    "npy_data = np.zeros(shape = ((npy_train_data.shape[0] + npy_valid_data.shape[0]), 2))\n",
    "print(\"----- npy_data.shape = {}\".format(npy_data.shape))\n",
    "\n",
    "npy_data[:npy_train_data.shape[0],:] = npy_train_data [:,:]\n",
    "npy_data[npy_train_data.shape[0]:,:] = npy_valid_data [:,:]\n",
    "\n",
    "print(\"----- npy_data.shape = {}\".format(npy_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_test = \"test\"\n",
    "\n",
    "csv_data_name = \"test.csv\"\n",
    "csv_label_name = \"test_nolabel.csv\"\n",
    "\n",
    "df_data = pd.read_csv(os.path.join(\"./data/\", csv_data_name))  \n",
    "\n",
    "\n",
    "df_year = df_data[\"arrival_date_year\"].iloc[:].to_numpy()\n",
    "df_month = df_data[\"arrival_date_month\"].iloc[:].to_numpy()\n",
    "df_week = df_data[\"arrival_date_week_number\"].iloc[:].to_numpy()\n",
    "df_date = df_data[\"arrival_date_day_of_month\"].iloc[:].to_numpy()\n",
    "print(\"----- df_date.shape = {}\".format(df_date.shape))\n",
    "\n",
    "\n",
    "df_weekend = df_data[\"stays_in_weekend_nights\"].iloc[:].to_numpy()\n",
    "df_weekdays = df_data[\"stays_in_week_nights\"].iloc[:].to_numpy()\n",
    "\n",
    "\n",
    "npy_data = np.load(\"./npy_file/test_prediction.npy\")\n",
    "print(\"----- npy_data.shape = {}\".format(npy_data.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\" 上述在讀回 np 資訊\"\n",
    "\n",
    "total_days = 0\n",
    "pred_day_revenue = 0\n",
    "total_weekdays = 0\n",
    "total_weekends = 0\n",
    "total_cancel = 0\n",
    "total_adr = 0\n",
    "\n",
    "\n",
    "bef_year = df_year[0]\n",
    "bef_month = df_month[0]\n",
    "bef_week = df_week[0]\n",
    "\n",
    "bef_date = df_date[0]\n",
    "pred_revenue_daylist = []\n",
    "#             adr_daylist = []\n",
    "#             cancel_daylist = []\n",
    "#             weekends_daylist = []\n",
    "#             weekdays_daylist = []\n",
    "\n",
    "for i, data in enumerate(npy_data):\n",
    "\n",
    "    if df_year[i] != bef_year or df_month[i] != bef_month or df_week[i] != bef_week or df_date[i] != bef_date or (i+1) ==len(npy_data):\n",
    "        bef_year = df_year[i]\n",
    "        bef_month = df_month[i]\n",
    "        bef_week = df_week[i]\n",
    "        bef_date = df_date[i]\n",
    "\n",
    "        pred_revenue_daylist.append(pred_day_revenue)\n",
    "        print(\"pred_day_revenue: \",pred_day_revenue)\n",
    "\n",
    "        total_days += 1\n",
    "        print(\"=============== Change day %d ===============\"%total_days)\n",
    "\n",
    "\n",
    "#                     cancel_daylist.append(total_cancel)\n",
    "#                     weekends_daylist.append(total_weekends)\n",
    "#                     weekdays_daylist.append(total_weekdays)\n",
    "#                     adr_daylist.append(total_adr)\n",
    "\n",
    "#                     total_weekdays = 0\n",
    "#                     total_weekends = 0\n",
    "#                     total_cancel = 0\n",
    "#                     total_adr = 0\n",
    "\n",
    "        pred_day_revenue = data[0] * (1-data[1]) * (df_weekend[i] + df_weekdays[i])\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"data[0]: \",data[0])\n",
    "        print(\"(1-data[1]): \",(1-data[1]))\n",
    "        print(\"df_weekend[i]: \",df_weekend[i])\n",
    "        print(\"df_weekdays[i]: \",df_weekdays[i])\n",
    "\n",
    "\n",
    "        \n",
    "    else :\n",
    "        pred_revenue = data[0] * (1-data[1]) * (df_weekend[i] + df_weekdays[i])\n",
    "\n",
    "#         print(\"data[0]: \",data[0])\n",
    "#         print(\"(1-data[1]): \",(1-data[1]))\n",
    "#         print(\"df_weekend[i]: \",df_weekend[i])\n",
    "#         print(\"df_weekdays[i]: \",df_weekdays[i])\n",
    "\n",
    "        pred_day_revenue += pred_revenue\n",
    "#         print(\"pred_day_revenue: \",pred_day_revenue)\n",
    "\n",
    "print(\"total_days: \",total_days)\n",
    "\n",
    "revenue = np.array(pred_revenue_daylist)\n",
    "revenue_norm = revenue\n",
    "# for i in range(len(revenue)):\n",
    "    \n",
    "#     revenue_norm[i] = (revenue[i]-min(revenue))/(max(revenue)-min(revenue))\n",
    "\n",
    "#             weekdays = np.array(weekdays_daylist)\n",
    "#             weekends = np.array(weekends_daylist)\n",
    "#             cancel = np.array(cancel_daylist)\n",
    "#             adr = np.array(adr_daylist)\n",
    "print(\"----- revenue_norm = {}\".format(revenue_norm[0]))\n",
    "print(\"----- revenue = {}\".format(revenue[0]))\n",
    "final_data = np.zeros(shape = (revenue.shape[0],1) )\n",
    "final_data[:,0] = revenue_norm[:]\n",
    "#             final_data[:,1] = adr[:]\n",
    "#             final_data[:,2] = cancel[:]\n",
    "#             final_data[:,3] = weekdays[:]\n",
    "#             final_data[:,4] = weekends[:]\n",
    "print(\"----- revenue_norm.shape = {}\".format(revenue_norm.shape))\n",
    "print(\"----- revenue.shape = {}\".format(revenue.shape))\n",
    "\n",
    "#             print(\"----- adr.shape = {}\".format(adr.shape))\n",
    "#             print(\"----- cancel.shape = {}\".format(cancel.shape))\n",
    "#             print(\"----- weekdays.shape = {}\".format(weekdays.shape))\n",
    "#             print(\"----- weekends.shape = {}\".format(weekends.shape))\n",
    "\n",
    "print(\"----- final_data.shape = {}\".format(final_data.shape))\n",
    "\n",
    "if train_valid_test == \"train\" :\n",
    "    df_label = pd.read_csv(os.path.join(\"./data/\", csv_label_name))  \n",
    "    df_label = df_label.fillna(0)\n",
    "    df_label = df_label.iloc[:,1].to_numpy()\n",
    "    print(\"----- df_label.shape = {}\".format(df_label.shape))\n",
    "    print(\"final_data: \",final_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train + valid metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "metric_list = []\n",
    "metric_data = []\n",
    "\n",
    "\n",
    "print(\"final_data.shape\",final_data.shape)\n",
    "print(\"df_label.shape\",df_label.shape)\n",
    "\n",
    "for i, data in enumerate (final_data) :\n",
    "#     if i == 500 :\n",
    "#         break\n",
    "    metric_data.append(np.abs(math.floor(data/10000)-df_label[i]))\n",
    "    print(\"i: \"+str(i)+\", data: \"+str(math.floor(data/10000))+\", label: \"+str(df_label[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(metric_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "result = []\n",
    "print(\"final_data.shape\",final_data.shape)\n",
    "# print(\"df_label.shape\",df_label.shape)\n",
    "\n",
    "for i, data in enumerate (final_data) :\n",
    "    print(\"i: \"+str(i)+\", data: \"+str(math.floor(data/10000)))\n",
    "    result.append(math.floor(data/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_nolabel = pd.read_csv(\"./data/test_nolabel.csv\")  \n",
    "df_test_nolabel = df_test_nolabel.iloc[:,:].to_numpy()\n",
    "\n",
    "with open(os.path.join(\"./test_pred_metric_tsai_ver1.csv\"), 'w') as f:\n",
    "    f.write('arrival_date,label\\n')\n",
    "    for i, y in  enumerate(result):\n",
    "        f.write('{},{}\\n'.format(df_test_nolabel[i][0], y))\n",
    "print(\"End of prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 底下是想在最後再用NN做一次訓練，這樣好像會怪怪，先不使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Final_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, label = False, train_valid_test = None):\n",
    "        # 要先把 __init__() 括弧裡面的東西給入 self.\n",
    "        print(\"path: \",path)\n",
    "\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "        self.train_valid_test = train_valid_test\n",
    "        \n",
    "        if self.label:\n",
    "            csv_data_name = \"train.csv\"\n",
    "            csv_label_name = \"train_label.csv\"\n",
    "            \n",
    "            self.df_data = pd.read_csv(os.path.join(self.path, csv_data_name))  \n",
    "            \n",
    "            \n",
    "            self.df_year = self.df_data[\"arrival_date_year\"].iloc[:].to_numpy()\n",
    "            self.df_month = self.df_data[\"arrival_date_month\"].iloc[:].to_numpy()\n",
    "            self.df_week = self.df_data[\"arrival_date_week_number\"].iloc[:].to_numpy()\n",
    "            self.df_date = self.df_data[\"arrival_date_day_of_month\"].iloc[:].to_numpy()\n",
    "            print(\"----- df_date.shape = {}\".format(self.df_date.shape))\n",
    "            \n",
    "            \n",
    "            self.df_weekend = self.df_data[\"stays_in_weekend_nights\"].iloc[:].to_numpy()\n",
    "            self.df_weekdays = self.df_data[\"stays_in_week_nights\"].iloc[:].to_numpy()\n",
    "\n",
    "\n",
    "            npy_train_data = np.load(\"./npy_file/train_prediction.npy\")\n",
    "            npy_valid_data = np.load(\"./npy_file/valid_prediction.npy\")\n",
    "            print(\"----- npy_train_data.shape = {}\".format(npy_train_data.shape))\n",
    "            print(\"----- npy_valid_data.shape = {}\".format(npy_valid_data.shape))\n",
    "            print(\"========================: \",(npy_train_data.shape[0] + npy_valid_data.shape[0]))\n",
    "            \n",
    "            npy_data = np.zeros(shape = ((npy_train_data.shape[0] + npy_valid_data.shape[0]), 2))\n",
    "            print(\"----- npy_data.shape = {}\".format(npy_data.shape))\n",
    "\n",
    "            npy_data[:npy_train_data.shape[0],:] = npy_train_data [:,:]\n",
    "            npy_data[npy_train_data.shape[0]:,:] = npy_valid_data [:,:]\n",
    "\n",
    "            print(\"----- npy_data.shape = {}\".format(npy_data.shape))\n",
    "        else :\n",
    "\n",
    "            csv_data_name = \"test.csv\"\n",
    "            csv_label_name = \"test_nolabel.csv\"\n",
    "\n",
    "            self.df_data = pd.read_csv(os.path.join(\"./data/\", csv_data_name))  \n",
    "\n",
    "\n",
    "            self.df_year = self.df_data[\"arrival_date_year\"].iloc[:].to_numpy()\n",
    "            self.df_month = self.df_data[\"arrival_date_month\"].iloc[:].to_numpy()\n",
    "            self.df_week = self.df_data[\"arrival_date_week_number\"].iloc[:].to_numpy()\n",
    "            self.df_date = self.df_data[\"arrival_date_day_of_month\"].iloc[:].to_numpy()\n",
    "            print(\"----- df_date.shape = {}\".format(self.df_date.shape))\n",
    "\n",
    "\n",
    "            self.df_weekend = self.df_data[\"stays_in_weekend_nights\"].iloc[:].to_numpy()\n",
    "            self.df_weekdays = self.df_data[\"stays_in_week_nights\"].iloc[:].to_numpy()\n",
    "\n",
    "\n",
    "            npy_data = np.load(\"./npy_file/test_prediction.npy\")\n",
    "            print(\"----- npy_data.shape = {}\".format(npy_data.shape))\n",
    "\n",
    "\n",
    "\n",
    "        self.total_days = 0\n",
    "        pred_day_revenue = 0\n",
    "        total_weekdays = 0\n",
    "        total_weekends = 0\n",
    "        total_cancel = 0\n",
    "        total_adr = 0\n",
    "\n",
    "\n",
    "        bef_year = self.df_year[0]\n",
    "        bef_month = self.df_month[0]\n",
    "        bef_week = self.df_week[0]\n",
    "\n",
    "        bef_date = self.df_date[0]\n",
    "        pred_revenue_daylist = []\n",
    "#         adr_daylist = []\n",
    "#             cancel_daylist = []\n",
    "#             weekends_daylist = []\n",
    "#             weekdays_daylist = []\n",
    "\n",
    "        for i, data in enumerate(npy_data):\n",
    "\n",
    "            if self.df_year[i] != bef_year or self.df_month[i] != bef_month or self.df_week[i] != bef_week or self.df_date[i] != bef_date or (i+1) ==len(npy_data):\n",
    "                bef_year = self.df_year[i]\n",
    "                bef_month = self.df_month[i]\n",
    "                bef_week = self.df_week[i]\n",
    "                bef_date = self.df_date[i]\n",
    "\n",
    "                pred_revenue_daylist.append(pred_day_revenue)\n",
    "#                     cancel_daylist.append(total_cancel)\n",
    "#                     weekends_daylist.append(total_weekends)\n",
    "#                     weekdays_daylist.append(total_weekdays)\n",
    "#                 adr_daylist.append(total_adr)\n",
    "                self.total_days += 1\n",
    "    \n",
    "                print(\"=============== Change day %d ===============\"%i)\n",
    "\n",
    "\n",
    "#                     total_weekdays = 0\n",
    "#                     total_weekends = 0\n",
    "#                     total_cancel = 0\n",
    "                total_adr = 0\n",
    "                pred_day_revenue  = data[0] * (1-data[1]) * (self.df_weekend[i] + self.df_weekdays[i])\n",
    "\n",
    "                \n",
    "            else:\n",
    "                pred_revenue = data[0] * (1-data[1]) * (self.df_weekend[i] + self.df_weekdays[i])\n",
    "#                     total_weekdays += self.df_weekdays[i]\n",
    "#                     total_weekends += self.df_weekend[i]\n",
    "#                     total_cancel += (1-data[1])\n",
    "                total_adr += data[0]\n",
    "                pred_day_revenue += pred_revenue\n",
    "\n",
    "        print(\"total_days: \",self.total_days)\n",
    "\n",
    "        self.revenue = np.array(pred_revenue_daylist)\n",
    "        \n",
    "#         self.revenue_norm = self.revenue\n",
    "#         for i in range(len(self.revenue)):\n",
    "#             self.revenue_norm[i] = (self.revenue[i]-min(self.revenue))/(max(self.revenue)-min(self.revenue))\n",
    "\n",
    "#             self.weekdays = np.array(weekdays_daylist)\n",
    "#             self.weekends = np.array(weekends_daylist)\n",
    "#             self.cancel = np.array(cancel_daylist)\n",
    "#         self.adr = np.array(adr_daylist)\n",
    "\n",
    "        self.final_data = np.zeros(shape = (self.revenue.shape[0],1) )\n",
    "        self.final_data[:,0] = self.revenue[:]\n",
    "#         self.final_data[:,1] = self.adr[:]\n",
    "#             self.final_data[:,2] = self.cancel[:]\n",
    "#             self.final_data[:,3] = self.weekdays[:]\n",
    "#             self.final_data[:,4] = self.weekends[:]\n",
    "#         print(\"----- revenue_norm.shape = {}\".format(self.revenue_norm.shape))\n",
    "        print(\"----- revenue.shape = {}\".format(self.revenue.shape))\n",
    "\n",
    "#         print(\"----- adr.shape = {}\".format(self.adr.shape))\n",
    "#             print(\"----- cancel.shape = {}\".format(self.cancel.shape))\n",
    "#             print(\"----- weekdays.shape = {}\".format(self.weekdays.shape))\n",
    "#             print(\"----- weekends.shape = {}\".format(self.weekends.shape))\n",
    "\n",
    "        print(\"----- final_data.shape = {}\".format(self.final_data.shape))\n",
    "        \n",
    "        if self.label:\n",
    "        \n",
    "            self.df_label = pd.read_csv(os.path.join(self.path, csv_label_name))  \n",
    "            self.df_label = self.df_label.fillna(0)\n",
    "            self.df_label = self.df_label.iloc[:,1].to_numpy()\n",
    "            print(\"----- df_label.shape = {}\".format(self.df_label.shape))\n",
    "            print(\"final_data: \",self.final_data.shape)\n",
    "\n",
    "\n",
    "            spl = int(0.2*self.final_data.shape[0])\n",
    "            print(\"spl: \",spl)\n",
    "\n",
    "            print(\"----- final_data.shape = {}\".format(self.final_data.shape))\n",
    "            print(\"----- df_label.shape = {}\".format(self.df_label.shape))\n",
    "\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "            if self.train_valid_test == \"train\" :\n",
    "                self.data_doggy = np.zeros( shape = (spl, self.final_data.shape[1]) )\n",
    "                self.label_doggy = np.zeros( shape = (spl) )\n",
    "\n",
    "                self.data_doggy = self.final_data[spl:,:]\n",
    "                self.label_doggy = self.df_label[spl:]\n",
    "                print(\"train_data_shape: \",self.data_doggy.shape)\n",
    "                print(\"train_label_shape: \",self.label_doggy.shape)\n",
    "\n",
    "\n",
    "            elif self.train_valid_test == \"valid\" :\n",
    "                self.data_doggy = np.zeros( shape = ( int(self.final_data.shape[0]-spl), self.final_data.shape[1]) )\n",
    "                self.label_doggy = np.zeros( shape = ( int(self.df_label.shape[0]-spl)) )\n",
    "\n",
    "                self.data_doggy = self.final_data[:spl,:]\n",
    "                self.label_doggy = self.df_label[:spl]\n",
    "                print(\"valid_data_shape: \",self.data_doggy.shape)\n",
    "                print(\"valid_label_shape: \",self.label_doggy.shape)\n",
    "            \n",
    "        else :\n",
    "            self.data_doggy = np.zeros( shape = (self.final_data.shape[0], self.final_data.shape[1]) )\n",
    "            print(\"data_shape: \",self.data_doggy.shape)\n",
    "            self.data_doggy = self.final_data[:,:]\n",
    "#########################################################################################################################\n",
    "            \n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.data_doggy.shape[0]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "#         print(\"self.path: \", self.path)\n",
    "        if self.label:\n",
    "#             print(\"revenue: \",self.revenue.shape)\n",
    "#             print(\"label: \",self.df_label.shape)\n",
    "            \n",
    "            final_data = self.data_doggy[index]\n",
    "#             label = self.df_label[index]\n",
    "            label = self.label_doggy[index]\n",
    "            \n",
    "#             print(\"revenue: \",revenue.shape)\n",
    "#             print(\"label: \",revenue.shape)\n",
    "            final_data = math.floor(final_data/10000)\n",
    "#             print(\"final_data: \",final_data.shape)\n",
    "\n",
    "            final_data = torch.tensor(final_data)\n",
    "            label = torch.tensor(label)\n",
    "\n",
    "            return final_data, label\n",
    "        \n",
    "        # 其實就是 testing\n",
    "        else:\n",
    "            final_data = self.data_doggy[index]\n",
    "            final_data = math.floor(final_data/10000)\n",
    "            final_data = torch.tensor(final_data)\n",
    "\n",
    "\n",
    "            return final_data\n",
    "\n",
    "    def get_comparison(self):\n",
    "        return self.revenue, self.df_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size\n",
    "metric_criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n==================== Training Dataset =========================\")\n",
    "train_dataset_final = Final_Dataset(path = \"./data/\", label = True, train_valid_test = \"train\")\n",
    "\n",
    "print(\"\\n==================== Valid Dataset =========================\")\n",
    "valid_dataset_final = Final_Dataset(path = \"./data/\", label = True, train_valid_test = \"valid\")\n",
    "\n",
    "print(\"\\n==================== Testing Dataset =========================\")\n",
    "test_dataset_final = Final_Dataset(path = \"./data/\", label = False, train_valid_test = \"test\")\n",
    "\n",
    "print(\"\\n==================== Dataloader =========================\")\n",
    "train_loader_final = DataLoader(train_dataset_final, batch_size=batch_size, shuffle=True)\n",
    "valid_loader_final = DataLoader(valid_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "test_loader_final = DataLoader(test_dataset_final, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"\\n==================== Done =========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"==================== training data ====================\")\n",
    "metric_list = []\n",
    "for i, (data, label) in enumerate(train_loader_final):\n",
    "    data = data.cuda().float()\n",
    "    label = label.cuda().float()\n",
    "    metric_list.append(metric_criterion(data, label).cpu().numpy())\n",
    "    for j in range(batch_size):\n",
    "        print(\"file_num: \"+str(i*batch_size+j)+\", data: \"+str(data[j])+\", label: \"+str(label[j].item()))\n",
    "# #         print(\"\\ndata: \",data[j])\n",
    "# #         print(\"label: \",label[j])\n",
    "print(\"training L1 loss: \",np.mean(metric_list))\n",
    "\n",
    "print(\"==================== validation data ====================\")\n",
    "metric_list = []\n",
    "for i, (data, label) in enumerate(valid_loader_final):\n",
    "    data = data.cuda().float()\n",
    "    label = label.cuda().float()\n",
    "    metric_list.append(metric_criterion(data, label).cpu().numpy())\n",
    "    for j in range(batch_size):\n",
    "        print(\"file_num: \"+str(i*batch_size+j)+\", data: \"+str(data[j])+\", label: \"+str(label[j].item()))\n",
    "# #         print(\"\\ndata: \",data[j])\n",
    "# #         print(\"label: \",label[j])\n",
    "print(\"validation L1 loss: \",np.mean(metric_list))\n",
    "\n",
    "\n",
    "print(\"==================== testing data ====================\")\n",
    "for i, data_batch in enumerate(test_loader_final):\n",
    "    \n",
    "#     print(\"data: \",data.shape)\n",
    "    for j, data in enumerate(data_batch):\n",
    "        print(\"file_num: \"+str(i*batch_size+j)+\", data: \"+str(data))\n",
    "#         print(\"\\ndata: \",data[j])\n",
    "#         print(\"label: \",label[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(test_loader_final):\n",
    "#     print(\"i: \",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Final_Model(nn.Module):\n",
    "    def __init__(self, in_feature, hidden_layer, out_feature=1):\n",
    "        super(Final_Model, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_feature, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(hidden_layer, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(hidden_layer, out_feature),\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(\"x: \",x.shape)\n",
    "        output = self.layer(x.unsqueeze(1))\n",
    "#         print(\"output: \",output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = Final_Model(in_feature = 1, hidden_layer = 16).cuda()\n",
    "# final_criterion = nn.CrossEntropyLoss()\n",
    "optimizer_final = optim.Adam(final_model.parameters())\n",
    "# optimizer_final = optim.SGD(final_model.parameters(),lr = 0.1 ,weight_decay=0.0001)\n",
    "# optimizer_final = optim.SGD(final_model.parameters(),lr = 0.01, momentum=0.9 ,weight_decay=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# final\n",
    "\n",
    "train_acc_max = 0.0\n",
    "valid_acc_max = 0.0\n",
    "\n",
    "train_metric_min = np.inf\n",
    "valid_metric_min = np.inf\n",
    "\n",
    "max_epoch = 500000\n",
    "metric = []\n",
    "\n",
    "for epoch in range(0, max_epoch):    \n",
    "    epoch_start_time = time.time()\n",
    "    loss_final = 0.0\n",
    "    bce_mean = 0.0\n",
    "    bce = []\n",
    "    train_acc = 0.0\n",
    "    valid_acc = 0.0\n",
    "    \n",
    "    metric_mean_train = 0.0\n",
    "    \n",
    "    final_model.train()\n",
    "    \n",
    "    for i, (data, final) in enumerate(train_loader_final):\n",
    "        optimizer_final.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "\n",
    "        data = data.cuda().float()\n",
    "        final = final.cuda().float()\n",
    "        \n",
    "        prediction_final = final_model(data).squeeze()\n",
    "        \n",
    "#         print(\"data: \",data.shape)\n",
    "#         print(\"prediction_final: \",prediction_final.shape)\n",
    "#         print(\"final: \",final.shape)\n",
    "\n",
    "        loss_final = metric_criterion(prediction_final, final)\n",
    "\n",
    "#         print(\"prediction_final: \",prediction_final)\n",
    "#         print(\"final: \",final)\n",
    "\n",
    "        \n",
    "        loss_final.backward()\n",
    "        optimizer_final.step()\n",
    "        \n",
    "        metric.append(loss_final.item())\n",
    "        \n",
    "        prediction_final = torch.round(prediction_final)\n",
    "        \n",
    "        train_acc += torch.sum(prediction_final == final).cpu().numpy()\n",
    "        print('epoch [%03d/%03d],CE_loss = %2.4f, train_acc = %.3f, %2.1f sec' % (epoch + 1, max_epoch, loss_final, train_acc/(batch_size*(i+1)), time.time()-epoch_start_time), end = '\\r')\n",
    "    \n",
    "    metric_mean_train = np.mean(metric)\n",
    "    if metric_mean_train < train_metric_min :\n",
    "        train_metric_min = metric_mean_train\n",
    "        torch.save(final_model.state_dict(), f'./checkpoints/final_model_train_metric_min.bin')\n",
    "# #         print(\"============================================================\")\n",
    "    \n",
    "    metric = []\n",
    "    metric_mean_valid = 0.0\n",
    "\n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, cancel) in enumerate(valid_loader_final):\n",
    "            \n",
    "            data = data.cuda().float()\n",
    "            cancel = cancel.cuda().float()\n",
    "            \n",
    "            val_pred = final_model(data).squeeze()\n",
    "            \n",
    "            loss_cancel = metric_criterion(val_pred, cancel)\n",
    "            metric.append(loss_cancel.item())\n",
    "            \n",
    "            val_pred = torch.round(val_pred)\n",
    "\n",
    "            valid_acc  += torch.sum(val_pred == cancel).cpu().numpy()\n",
    "            \n",
    "        metric_mean_valid = np.mean(metric)\n",
    "        \n",
    "        if valid_acc > valid_acc_max:\n",
    "            valid_acc_max = valid_acc\n",
    "        \n",
    "        if metric_mean_valid < valid_metric_min :\n",
    "            valid_metric_min = metric_mean_valid\n",
    "            torch.save(final_model.state_dict(), f'./checkpoints/final_model_valid_metric_min.bin')\n",
    "#         print(\"Valid loss: %.3f, valid_acc = %.3f, Valid acc max: %.3f\"%(bce_mean,  valid_acc/int(640*0.2), valid_acc_max/int(640*0.2)))\n",
    "#         print(\"epoch time: %.2f sec\"%(time.time()-epoch_start_time))\n",
    "#         print(\"============================================================\")\n",
    "    print('epoch [%03d/%03d], train_acc = %.3f, valid_acc = %.3f, valid_acc_max = %.3f, metric_train = %.3f, metric_valid = %.3f, metric_valid_min = %.3f,' % (epoch + 1, max_epoch, train_acc/int(640*0.8), valid_acc/int(640*0.2), valid_acc_max/int(640*0.2), metric_mean_train, metric_mean_valid, valid_metric_min), end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.load_state_dict(torch.load('./checkpoints/final_model_valid_metric_min.bin'))\n",
    "final_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_nolabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader_final):\n",
    "        print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "        data = data.cuda().float()\n",
    "        \n",
    "        predict = torch.round(final_model(data).squeeze()).cpu().numpy()\n",
    "        \n",
    "        result.append(predict)\n",
    "\n",
    "    print(\"====================== test_prediction done ======================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.concatenate(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_nolabel[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_nolabel = pd.read_csv(\"./data/test_nolabel.csv\")  \n",
    "df_test_nolabel = df_test_nolabel.iloc[:,:].to_numpy()\n",
    "\n",
    "with open(os.path.join(\"./test_pred_metric_tsai_ver2.csv\"), 'w') as f:\n",
    "    f.write('arrival_date,label\\n')\n",
    "    for i, y in  enumerate(result):\n",
    "        f.write('{},{}\\n'.format(df_test_nolabel[i][0], y))\n",
    "print(\"End of prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

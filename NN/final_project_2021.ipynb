{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import需要的套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import scipy.misc\n",
    "import scipy.ndimage\n",
    "import imageio\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_epoch = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OHE(feature):\n",
    "    return pd.get_dummies(feature)\n",
    "def standarization(feature):\n",
    "    return (feature-np.mean(feature))/np.std(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "1. label 的資料要合併單位至天數才可以用，這邊先不讀，不然列數不同會爆開"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Csv_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path, label = False, train_valid_test = None):\n",
    "        \n",
    "        print(\"path: \",path)\n",
    "        self.path = path\n",
    "        self.label = label\n",
    "        self.train_valid_test = train_valid_test\n",
    "        \n",
    "        \n",
    "        oheList = ['hotel', 'meal', 'market_segment', 'distribution_channel', 'reserved_room_type', 'assigned_room_type', 'deposit_type', 'customer_type']\n",
    "        standardizeList = ['lead_time', 'stays_in_weekend_nights', 'stays_in_week_nights', 'adults', 'children', 'babies', 'previous_cancellations', 'previous_bookings_not_canceled', 'booking_changes', 'days_in_waiting_list', 'required_car_parking_spaces', 'total_of_special_requests']\n",
    "        donothingList = ['is_repeated_guest']\n",
    "        dropList = ['ID', 'arrival_date_year', 'arrival_date_month', 'arrival_date_week_number', 'arrival_date_day_of_month', 'country', 'agent', 'company', 'reservation_status', 'reservation_status_date']\n",
    "        \n",
    "        # label_csv_path = True 就是有 label\n",
    "        if self.train_valid_test == \"train\" or self.train_valid_test == \"valid\":\n",
    "            \n",
    "            csv_data_name = \"train.csv\"\n",
    "            csv_label_name = \"train_label.csv\"\n",
    "            \n",
    "            self.df_data = pd.read_csv(os.path.join(self.path, csv_data_name))  \n",
    "            self.df_data = self.df_data.fillna(0)\n",
    "            \n",
    "            print(\"check 17008 arrival_date_day_of_month: \",self.df_data[\"arrival_date_day_of_month\"].to_numpy()[17008])\n",
    "            print(\"check 29640 arrival_date_day_of_month: \",self.df_data[\"arrival_date_day_of_month\"].to_numpy()[29640])\n",
    "            print(\"check 17010 arrival_date_day_of_month: \",self.df_data[\"arrival_date_day_of_month\"].to_numpy()[17010])\n",
    "\n",
    "            \n",
    "        elif self.train_valid_test == \"test\" :\n",
    "        \n",
    "            csv_data_name = \"test.csv\"\n",
    "            csv_label_name = \"test_nolabel.csv\"\n",
    "            \n",
    "            self.df_data = pd.read_csv(os.path.join(self.path, csv_data_name))  \n",
    "            \n",
    "            print(\"There's NO adr and is_canceled in testing data.\")\n",
    "            \n",
    "            self.df_data = self.df_data.fillna(0)\n",
    "        else:\n",
    "            print(\"!!!!!!!!!!!!!!!!!!!!  error train_valid_test is something wrong !!!!!!!!!!!!!!!!!!!!\")\n",
    "        \n",
    "###########################################################################\n",
    "####  讀取 adr 和 is_canceled\n",
    "#########################################################################################################################\n",
    "\n",
    "        if self.train_valid_test == \"train\" :\n",
    "            \n",
    "            train_len = self.df_data.to_numpy().shape[0] - (51605 - 29640)\n",
    "            train_adr = np.zeros(shape = train_len)\n",
    "            train_cancel = np.zeros(shape = train_len)\n",
    "            print(\"train_np.shape: \",train_adr.shape)\n",
    "            \n",
    "            \n",
    "            train_adr[:29640] = self.df_data[\"adr\"].iloc[:29640].to_numpy()\n",
    "            train_adr[29640:] = self.df_data[\"adr\"].iloc[51605:].to_numpy()\n",
    "            self.df_adr = train_adr\n",
    "            print(\"----- df_adr.shape = {}\".format(self.df_adr.shape))\n",
    "            \n",
    "            train_cancel[:29640] = self.df_data[\"is_canceled\"].iloc[:29640].to_numpy()\n",
    "            train_cancel[29640:] = self.df_data[\"is_canceled\"].iloc[51605:].to_numpy()\n",
    "            self.df_cancel = train_cancel\n",
    "            print(\"----- df_cancel.shape = {}\".format(self.df_cancel.shape))\n",
    "\n",
    "        elif self.train_valid_test == \"valid\" :\n",
    "            \n",
    "            self.df_adr = self.df_data[\"adr\"].iloc[29640:51605].to_numpy()\n",
    "            print(\"----- df_adr.shape = {}\".format(self.df_adr.shape))\n",
    "            \n",
    "            self.df_cancel = self.df_data[\"is_canceled\"].iloc[29640:51605].to_numpy()\n",
    "            print(\"----- df_cancel.shape = {}\".format(self.df_cancel.shape))\n",
    "            \n",
    "#########################################################################################################################\n",
    "    \n",
    "        # 照著討論出的 baseline 去處理資料\n",
    "        key_df_list = list(self.df_data.keys())\n",
    "        data_feature = np.array([])\n",
    "        for k in key_df_list:\n",
    "\n",
    "            if k == \"hotel\" :\n",
    "                print(\"----------------------------------------\\noheList: \",k)\n",
    "                feature = self.df_data[k]\n",
    "                feature = OHE(feature).iloc[:].to_numpy()\n",
    "                print(\"oheList_feature: \",feature.shape)\n",
    "                data_feature = np.array(feature)\n",
    "                print(\"original data_feature: \",data_feature.shape)\n",
    "\n",
    "            # 做 onehot encoding\n",
    "            elif k in oheList :\n",
    "                print(\"----------------------------------------\\noheList: \",k)\n",
    "                feature = self.df_data[k]\n",
    "                feature = OHE(feature).iloc[:].to_numpy()\n",
    "                print(\"oheList_feature: \",feature.shape)\n",
    "                data_feature = np.column_stack((data_feature, feature))\n",
    "\n",
    "            # 做標準化\n",
    "            elif k in standardizeList :\n",
    "                print(\"----------------------------------------\\nstandarizeList: \",k)\n",
    "                feature = self.df_data[k].iloc[:].to_numpy()\n",
    "                feature = standarization(feature)\n",
    "                print(\"standardizeList_feature: \",feature.shape)\n",
    "                data_feature = np.column_stack((data_feature, feature))\n",
    "\n",
    "            # 原封不動給到我們的結果\n",
    "            elif k in donothingList :\n",
    "                print(\"----------------------------------------\\ndonothingList: \",k)\n",
    "                feature = self.df_data[k].iloc[:].to_numpy()\n",
    "                print(\"donothingList_feature: \",feature.shape)\n",
    "                data_feature = np.column_stack((data_feature, feature))\n",
    "\n",
    "            # 直接跳過\n",
    "            elif k in dropList :\n",
    "                print(\"----------------------------------------\\ndropList: \",k)\n",
    "\n",
    "            else:\n",
    "                print(\"----------------------------------------\\nadr, is_cancelled or bugggggggggg: \",k)\n",
    "\n",
    "            print(\"total data_feature: \",data_feature.shape)\n",
    "\n",
    "###########################################################################\n",
    "####  分割\n",
    "#########################################################################################################################\n",
    "\n",
    "        if self.train_valid_test == \"train\" :\n",
    "        \n",
    "            train_len = self.df_data.to_numpy().shape[0] - (51605 - 29640)\n",
    "            train_np = np.zeros(shape = (train_len, data_feature.shape[1]))\n",
    "            print(\"train_np.shape: \",train_np.shape)\n",
    "            \n",
    "            train_np[:29640, :] = data_feature[:29640, :]\n",
    "            train_np[29640:, :] = data_feature[51605:, :]\n",
    "            \n",
    "            self.df_data = train_np # 轉換結果就是我們要用的資料了\n",
    "            print(\"train df_data: \",self.df_data.shape) #這邊是 numpy 形式，而非 pd.DataFrame\n",
    "            \n",
    "        elif self.train_valid_test == \"valid\" :\n",
    "\n",
    "            self.df_data = data_feature[29640:51605, :] # 轉換結果就是我們要用的資料了\n",
    "            print(\"valid df_data: \",self.df_data.shape) #這邊是 numpy 形式，而非 pd.DataFrame\n",
    "            \n",
    "        elif self.train_valid_test == \"test\" :\n",
    "            self.df_data = data_feature[:,:] # 轉換結果就是我們要用的資料了\n",
    "            print(\"test df_data: \",self.df_data.shape) #這邊是 numpy 形式，而非 pd.DataFrame\n",
    "            print(\"\\nThere's no label in testing set, maybe read arrival_date only.\")\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "    # 在 from torch.utils.data import DataLoader, Dataset 中的 DataLoader, Dataset\n",
    "    # 需要 __len__ 及 __getitem__ 兩個函式\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df_data.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.label:\n",
    "            data = self.df_data[index]\n",
    "            adr = self.df_adr[index]\n",
    "            cancel = self.df_cancel[index]\n",
    "            \n",
    "            data = torch.tensor(data)\n",
    "            adr = torch.tensor(adr)\n",
    "            cancel = torch.tensor(cancel)\n",
    "\n",
    "            return data, adr, cancel\n",
    "        \n",
    "        # 其實就是 testing\n",
    "        else:\n",
    "            data = self.df_data[index]\n",
    "            data = torch.tensor(data)\n",
    "            \n",
    "            return data\n",
    "        \n",
    "    def get_feature_sum(self):\n",
    "        return self.df_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Training Dataset=========================\n",
      "path:  ./data/\n",
      "check 17008 arrival_date_day_of_month:  4\n",
      "check 29640 arrival_date_day_of_month:  12\n",
      "check 17010 arrival_date_day_of_month:  5\n",
      "train_np.shape:  (69566,)\n",
      "----- df_adr.shape = (69566,)\n",
      "----- df_cancel.shape = (69566,)\n",
      "----------------------------------------\n",
      "dropList:  ID\n",
      "total data_feature:  (0,)\n",
      "----------------------------------------\n",
      "oheList:  hotel\n",
      "oheList_feature:  (91531, 2)\n",
      "original data_feature:  (91531, 2)\n",
      "total data_feature:  (91531, 2)\n",
      "----------------------------------------\n",
      "adr, is_cancelled or bugggggggggg:  is_canceled\n",
      "total data_feature:  (91531, 2)\n",
      "----------------------------------------\n",
      "standarizeList:  lead_time\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_year\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_month\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_week_number\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_day_of_month\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "standarizeList:  stays_in_weekend_nights\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 4)\n",
      "----------------------------------------\n",
      "standarizeList:  stays_in_week_nights\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 5)\n",
      "----------------------------------------\n",
      "standarizeList:  adults\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 6)\n",
      "----------------------------------------\n",
      "standarizeList:  children\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 7)\n",
      "----------------------------------------\n",
      "standarizeList:  babies\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 8)\n",
      "----------------------------------------\n",
      "oheList:  meal\n",
      "oheList_feature:  (91531, 5)\n",
      "total data_feature:  (91531, 13)\n",
      "----------------------------------------\n",
      "dropList:  country\n",
      "total data_feature:  (91531, 13)\n",
      "----------------------------------------\n",
      "oheList:  market_segment\n",
      "oheList_feature:  (91531, 8)\n",
      "total data_feature:  (91531, 21)\n",
      "----------------------------------------\n",
      "oheList:  distribution_channel\n",
      "oheList_feature:  (91531, 5)\n",
      "total data_feature:  (91531, 26)\n",
      "----------------------------------------\n",
      "donothingList:  is_repeated_guest\n",
      "donothingList_feature:  (91531,)\n",
      "total data_feature:  (91531, 27)\n",
      "----------------------------------------\n",
      "standarizeList:  previous_cancellations\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 28)\n",
      "----------------------------------------\n",
      "standarizeList:  previous_bookings_not_canceled\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 29)\n",
      "----------------------------------------\n",
      "oheList:  reserved_room_type\n",
      "oheList_feature:  (91531, 10)\n",
      "total data_feature:  (91531, 39)\n",
      "----------------------------------------\n",
      "oheList:  assigned_room_type\n",
      "oheList_feature:  (91531, 12)\n",
      "total data_feature:  (91531, 51)\n",
      "----------------------------------------\n",
      "standarizeList:  booking_changes\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 52)\n",
      "----------------------------------------\n",
      "oheList:  deposit_type\n",
      "oheList_feature:  (91531, 3)\n",
      "total data_feature:  (91531, 55)\n",
      "----------------------------------------\n",
      "dropList:  agent\n",
      "total data_feature:  (91531, 55)\n",
      "----------------------------------------\n",
      "dropList:  company\n",
      "total data_feature:  (91531, 55)\n",
      "----------------------------------------\n",
      "standarizeList:  days_in_waiting_list\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 56)\n",
      "----------------------------------------\n",
      "oheList:  customer_type\n",
      "oheList_feature:  (91531, 4)\n",
      "total data_feature:  (91531, 60)\n",
      "----------------------------------------\n",
      "adr, is_cancelled or bugggggggggg:  adr\n",
      "total data_feature:  (91531, 60)\n",
      "----------------------------------------\n",
      "standarizeList:  required_car_parking_spaces\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 61)\n",
      "----------------------------------------\n",
      "standarizeList:  total_of_special_requests\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 62)\n",
      "----------------------------------------\n",
      "dropList:  reservation_status\n",
      "total data_feature:  (91531, 62)\n",
      "----------------------------------------\n",
      "dropList:  reservation_status_date\n",
      "total data_feature:  (91531, 62)\n",
      "train_np.shape:  (69566, 62)\n",
      "train df_data:  (69566, 62)\n",
      "\n",
      "==================== Validation Dataset=========================\n",
      "path:  ./data/\n",
      "check 17008 arrival_date_day_of_month:  4\n",
      "check 29640 arrival_date_day_of_month:  12\n",
      "check 17010 arrival_date_day_of_month:  5\n",
      "----- df_adr.shape = (21965,)\n",
      "----- df_cancel.shape = (21965,)\n",
      "----------------------------------------\n",
      "dropList:  ID\n",
      "total data_feature:  (0,)\n",
      "----------------------------------------\n",
      "oheList:  hotel\n",
      "oheList_feature:  (91531, 2)\n",
      "original data_feature:  (91531, 2)\n",
      "total data_feature:  (91531, 2)\n",
      "----------------------------------------\n",
      "adr, is_cancelled or bugggggggggg:  is_canceled\n",
      "total data_feature:  (91531, 2)\n",
      "----------------------------------------\n",
      "standarizeList:  lead_time\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_year\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_month\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_week_number\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_day_of_month\n",
      "total data_feature:  (91531, 3)\n",
      "----------------------------------------\n",
      "standarizeList:  stays_in_weekend_nights\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 4)\n",
      "----------------------------------------\n",
      "standarizeList:  stays_in_week_nights\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 5)\n",
      "----------------------------------------\n",
      "standarizeList:  adults\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 6)\n",
      "----------------------------------------\n",
      "standarizeList:  children\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 7)\n",
      "----------------------------------------\n",
      "standarizeList:  babies\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 8)\n",
      "----------------------------------------\n",
      "oheList:  meal\n",
      "oheList_feature:  (91531, 5)\n",
      "total data_feature:  (91531, 13)\n",
      "----------------------------------------\n",
      "dropList:  country\n",
      "total data_feature:  (91531, 13)\n",
      "----------------------------------------\n",
      "oheList:  market_segment\n",
      "oheList_feature:  (91531, 8)\n",
      "total data_feature:  (91531, 21)\n",
      "----------------------------------------\n",
      "oheList:  distribution_channel\n",
      "oheList_feature:  (91531, 5)\n",
      "total data_feature:  (91531, 26)\n",
      "----------------------------------------\n",
      "donothingList:  is_repeated_guest\n",
      "donothingList_feature:  (91531,)\n",
      "total data_feature:  (91531, 27)\n",
      "----------------------------------------\n",
      "standarizeList:  previous_cancellations\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 28)\n",
      "----------------------------------------\n",
      "standarizeList:  previous_bookings_not_canceled\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 29)\n",
      "----------------------------------------\n",
      "oheList:  reserved_room_type\n",
      "oheList_feature:  (91531, 10)\n",
      "total data_feature:  (91531, 39)\n",
      "----------------------------------------\n",
      "oheList:  assigned_room_type\n",
      "oheList_feature:  (91531, 12)\n",
      "total data_feature:  (91531, 51)\n",
      "----------------------------------------\n",
      "standarizeList:  booking_changes\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 52)\n",
      "----------------------------------------\n",
      "oheList:  deposit_type\n",
      "oheList_feature:  (91531, 3)\n",
      "total data_feature:  (91531, 55)\n",
      "----------------------------------------\n",
      "dropList:  agent\n",
      "total data_feature:  (91531, 55)\n",
      "----------------------------------------\n",
      "dropList:  company\n",
      "total data_feature:  (91531, 55)\n",
      "----------------------------------------\n",
      "standarizeList:  days_in_waiting_list\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 56)\n",
      "----------------------------------------\n",
      "oheList:  customer_type\n",
      "oheList_feature:  (91531, 4)\n",
      "total data_feature:  (91531, 60)\n",
      "----------------------------------------\n",
      "adr, is_cancelled or bugggggggggg:  adr\n",
      "total data_feature:  (91531, 60)\n",
      "----------------------------------------\n",
      "standarizeList:  required_car_parking_spaces\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 61)\n",
      "----------------------------------------\n",
      "standarizeList:  total_of_special_requests\n",
      "standardizeList_feature:  (91531,)\n",
      "total data_feature:  (91531, 62)\n",
      "----------------------------------------\n",
      "dropList:  reservation_status\n",
      "total data_feature:  (91531, 62)\n",
      "----------------------------------------\n",
      "dropList:  reservation_status_date\n",
      "total data_feature:  (91531, 62)\n",
      "valid df_data:  (21965, 62)\n",
      "\n",
      "==================== Testing Dataset=========================\n",
      "path:  ./data/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's NO adr and is_canceled in testing data.\n",
      "----------------------------------------\n",
      "dropList:  ID\n",
      "total data_feature:  (0,)\n",
      "----------------------------------------\n",
      "oheList:  hotel\n",
      "oheList_feature:  (27860, 2)\n",
      "original data_feature:  (27860, 2)\n",
      "total data_feature:  (27860, 2)\n",
      "----------------------------------------\n",
      "standarizeList:  lead_time\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  (27860, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_year\n",
      "total data_feature:  (27860, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_month\n",
      "total data_feature:  (27860, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_week_number\n",
      "total data_feature:  (27860, 3)\n",
      "----------------------------------------\n",
      "dropList:  arrival_date_day_of_month\n",
      "total data_feature:  (27860, 3)\n",
      "----------------------------------------\n",
      "standarizeList:  stays_in_weekend_nights\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  (27860, 4)\n",
      "----------------------------------------\n",
      "standarizeList:  stays_in_week_nights\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  (27860, 5)\n",
      "----------------------------------------\n",
      "standarizeList:  adults\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  (27860, 6)\n",
      "----------------------------------------\n",
      "standarizeList:  children\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  (27860, 7)\n",
      "----------------------------------------\n",
      "standarizeList:  babies\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  (27860, 8)\n",
      "----------------------------------------\n",
      "oheList:  meal\n",
      "oheList_feature:  (27860, 5)\n",
      "total data_feature:  (27860, 13)\n",
      "----------------------------------------\n",
      "dropList:  country\n",
      "total data_feature:  (27860, 13)\n",
      "----------------------------------------\n",
      "oheList:  market_segment\n",
      "oheList_feature:  (27860, 8)\n",
      "total data_feature:  (27860, 21)\n",
      "----------------------------------------\n",
      "oheList:  distribution_channel\n",
      "oheList_feature:  (27860, 5)\n",
      "total data_feature:  (27860, 26)\n",
      "----------------------------------------\n",
      "donothingList:  is_repeated_guest\n",
      "donothingList_feature:  (27860,)\n",
      "total data_feature:  (27860, 27)\n",
      "----------------------------------------\n",
      "standarizeList:  previous_cancellations\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  (27860, 28)\n",
      "----------------------------------------\n",
      "standarizeList:  previous_bookings_not_canceled\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  (27860, 29)\n",
      "----------------------------------------\n",
      "oheList:  reserved_room_type\n",
      "oheList_feature:  (27860, 10)\n",
      "total data_feature:  (27860, 39)\n",
      "----------------------------------------\n",
      "oheList:  assigned_room_type\n",
      "oheList_feature:  (27860, 12)\n",
      "total data_feature:  (27860, 51)\n",
      "----------------------------------------\n",
      "standarizeList:  booking_changes\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  (27860, 52)\n",
      "----------------------------------------\n",
      "oheList:  deposit_type\n",
      "oheList_feature:  (27860, 3)\n",
      "total data_feature:  (27860, 55)\n",
      "----------------------------------------\n",
      "dropList:  agent\n",
      "total data_feature:  (27860, 55)\n",
      "----------------------------------------\n",
      "dropList:  company\n",
      "total data_feature:  (27860, 55)\n",
      "----------------------------------------\n",
      "standarizeList:  days_in_waiting_list\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  (27860, 56)\n",
      "----------------------------------------\n",
      "oheList:  customer_type\n",
      "oheList_feature:  (27860, 4)\n",
      "total data_feature:  (27860, 60)\n",
      "----------------------------------------\n",
      "standarizeList:  required_car_parking_spaces\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  (27860, 61)\n",
      "----------------------------------------\n",
      "standarizeList:  total_of_special_requests\n",
      "standardizeList_feature:  (27860,)\n",
      "total data_feature:  (27860, 62)\n",
      "test df_data:  (27860, 62)\n",
      "\n",
      "There's no label in testing set, maybe read arrival_date only.\n",
      "\n",
      "==================== Dataloader =========================\n",
      "\n",
      "==================== Done =========================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n==================== Training Dataset=========================\")\n",
    "train_dataset = Csv_Dataset(path = \"./data/\", label = True, train_valid_test = \"train\")\n",
    "\n",
    "print(\"\\n==================== Validation Dataset=========================\")\n",
    "valid_dataset = Csv_Dataset(path = \"./data/\", label = True, train_valid_test = \"valid\")\n",
    "\n",
    "print(\"\\n==================== Testing Dataset=========================\")\n",
    "test_dataset = Csv_Dataset(path = \"./data/\", label = False, train_valid_test = \"test\")\n",
    "\n",
    "print(\"\\n==================== Dataloader =========================\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"\\n==================== Done =========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train feature_num: 62\n",
      "valid feature_num: 62\n",
      "test feature_num: 62\n",
      "========== feature_num:  62\n",
      "69566\n",
      "21965\n",
      "27860\n"
     ]
    }
   ],
   "source": [
    "print(\"train feature_num:\",train_dataset.get_feature_sum())\n",
    "print(\"valid feature_num:\",valid_dataset.get_feature_sum())\n",
    "print(\"test feature_num:\",test_dataset.get_feature_sum())\n",
    "\n",
    "if train_dataset.get_feature_sum() == valid_dataset.get_feature_sum() == test_dataset.get_feature_sum() :\n",
    "    feature_num = train_dataset.get_feature_sum()\n",
    "    print(\"========== feature_num: \", feature_num)\n",
    "else :\n",
    "    print(\"Error!! Train, valid, test with different feature !!!\")\n",
    "\n",
    "print(train_dataset.__len__())\n",
    "print(valid_dataset.__len__())\n",
    "print(test_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADR_Model(nn.Module):\n",
    "    def __init__(self, in_feature, hidden_layer, out_feature=1):\n",
    "        super(ADR_Model, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_feature, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.4),\n",
    "            \n",
    "            nn.Linear(hidden_layer, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(hidden_layer, out_feature),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layer(x)\n",
    "#         print(\"output: \",output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cancel_Model(nn.Module):\n",
    "    def __init__(self, in_feature, hidden_layer, out_feature=2):\n",
    "        super(Cancel_Model, self).__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_feature, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer*2),\n",
    "            nn.BatchNorm1d(hidden_layer*2),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer*2, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(hidden_layer, hidden_layer),\n",
    "            nn.BatchNorm1d(hidden_layer),\n",
    "            nn.LeakyReLU(0.02),\n",
    "#             nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(hidden_layer, out_feature),\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.layer(x)\n",
    "#         print(\"output: \",output.shape)\n",
    "#         output = F.softmax(output, dim=0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_model = ADR_Model(in_feature = feature_num, hidden_layer = 128 ).cuda()\n",
    "# adr_model = ADR_Model().cuda()\n",
    "\n",
    "cancel_model = Cancel_Model(in_feature = feature_num, hidden_layer = 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_criterion = nn.MSELoss()\n",
    "cancel_criterion = nn.CrossEntropyLoss()\n",
    "# final_criterion = nn.CrossEntropyLoss()\n",
    "# l1_criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer_cancel = optim.SGD(cancel_model.parameters(),lr = 0.01, momentum=0.9 ,weight_decay=0.0001)\n",
    "\n",
    "# optimizer_final = optim.Adam(final_model.parameters())\n",
    "# optimizer_final = optim.SGD(final_model.parameters(),lr = 0.01, momen tum=0.9 ,weight_decay=0.0001)\n",
    "optimizer_adr = optim.Adam(adr_model.parameters(),lr = 0.01, weight_decay=0.0001)\n",
    "# optimizer_adr = optim.Adam(adr_model.parameters(),lr = 0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "1. 暫定這個 adr_model，最低的 validation mean MSE 為 693\n",
    "2. 如果 cancel model 要小數點的regression結果，直接把adr train的那段複製到cancel再做以下操作就好了  \n",
    "   1. cancel_criterion改成 BCELOSS\n",
    "   2. cancel_model 最後一層用 nn.Sigmoid，BCELOSS 用 Sigmoid，多分類用的 CE 才使用 softmax\n",
    "   3. 承上， nn.CrossEntropyLoss() 已經包入 softmax 操作了\n",
    "   4. cancel_model 輸出的 feature 改為 1，輸出的這 1 個小數就算是regression結果\n",
    "   5. 備註：其實在 pytorch 中 BCEWithLogitsLoss 就是 BCELOSS 加上 Sigmoid\n",
    "3. 目前因為 valid classification 的結果不錯，所以就用 classification 了\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mse_min = np.inf\n",
    "train_mse_min = np.inf\n",
    "\n",
    "valid_bce_min = np.inf\n",
    "train_bce_min = np.inf\n",
    "\n",
    "train_acc_max = 0.0\n",
    "valid_acc_max = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adr\n",
    "for epoch in range(0, 300):    \n",
    "    epoch_start_time = time.time()\n",
    "    loss = 0.0\n",
    "    mse_mean = 0.0\n",
    "    mse = []\n",
    "    adr_model.train()\n",
    "    \n",
    "    for i, (data, adr, cancel) in enumerate(train_loader):\n",
    "        optimizer_adr.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "\n",
    "        data = data.cuda().float()\n",
    "        adr = adr.cuda().float()\n",
    "\n",
    "        prediction = adr_model(data).squeeze()\n",
    "\n",
    "        loss = adr_criterion(prediction, adr)\n",
    "        \n",
    "        mse.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_adr.step()\n",
    "        \n",
    "        print('epoch [%03d/%03d],MSE = %2.4f, %2.2f sec(s)' % (epoch + 1, max_epoch, loss, time.time()-epoch_start_time), end = '\\r')\n",
    "    \n",
    "    mse_mean = np.mean(mse)\n",
    "    print('epoch [%03d/%03d],train_MSE_mean = %2.4f, %2.2f sec(s)' % (epoch + 1, max_epoch, mse_mean, time.time()-epoch_start_time))\n",
    "    \n",
    "    if mse_mean < train_mse_min :\n",
    "        train_mse_min = mse_mean\n",
    "        torch.save(adr_model, \"./checkpoints/adr_model_train_mse_min.bin\")\n",
    "        \n",
    "    \n",
    "    loss = 0.0\n",
    "    mse_mean = 0.0\n",
    "    mse = []\n",
    "    adr_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, adr, cancel) in enumerate(valid_loader):\n",
    "            \n",
    "            data = data.cuda().float()\n",
    "            adr = adr.cuda().float()\n",
    "\n",
    "            val_pred = adr_model(data).squeeze()\n",
    "            \n",
    "            loss = adr_criterion(val_pred, adr)\n",
    "            mse.append(loss.item())\n",
    "            \n",
    "        mse_mean = np.mean(mse)\n",
    "        \n",
    "        if mse_mean < valid_mse_min and epoch > 1 :\n",
    "            valid_mse_min = mse_mean\n",
    "            torch.save(adr_model, \"./checkpoints/adr_model_valid_mse_min.bin\")\n",
    "        print(\"Valid MSE: %.4f, Valid MSE minimum: %.4f \"%(mse_mean, valid_mse_min))\n",
    "        print(\"==================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cancel\n",
    "for epoch in range(0, 100):    \n",
    "    epoch_start_time = time.time()\n",
    "    loss_cancel = 0.0\n",
    "    bce_mean = 0.0\n",
    "    bce = []\n",
    "    train_acc = 0.0\n",
    "    valid_acc = 0.0\n",
    "    \n",
    "    cancel_model.train()\n",
    "    \n",
    "    for i, (data, adr, cancel) in enumerate(train_loader):\n",
    "        optimizer_cancel.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "\n",
    "        data = data.cuda().float()\n",
    "        cancel = cancel.cuda().long()\n",
    "        \n",
    "        prediction_cancel = cancel_model(data).squeeze()\n",
    "        loss_cancel = cancel_criterion(prediction_cancel, cancel)\n",
    "        \n",
    "        bce.append(loss_cancel.item())\n",
    "        loss_cancel.backward()\n",
    "        optimizer_cancel.step()\n",
    "        \n",
    "        train_acc += np.sum(np.argmax(prediction_cancel.cpu().data.numpy(), axis=1) == cancel.cpu().numpy())\n",
    "        print('epoch [%03d/%03d],CE_loss = %2.4f, train_acc = %.3f, %2.1f sec' % (epoch + 1, max_epoch, loss_cancel, train_acc/(batch_size*(i+1)), time.time()-epoch_start_time), end = '\\r')\n",
    "    \n",
    "    bce_mean = np.mean(bce)\n",
    "    print('epoch [%03d/%03d],train_loss = %2.4f, train_acc = %.3f, %2.1f sec' % (epoch + 1, max_epoch, bce_mean, train_acc/train_dataset.__len__(), time.time()-epoch_start_time))\n",
    "    \n",
    "    if train_acc > train_acc_max :\n",
    "        train_acc_max = train_acc\n",
    "        torch.save(cancel_model, \"./checkpoints/cancel_model_train_acc_max.bin\")\n",
    "        \n",
    "    \n",
    "    loss_cancel = 0.0\n",
    "    bce_mean = 0.0\n",
    "    bce = []\n",
    "    cancel_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (data, adr, cancel) in enumerate(valid_loader):\n",
    "            data = data.cuda().float()\n",
    "            cancel = cancel.cuda().long()\n",
    "            \n",
    "            val_pred = cancel_model(data).squeeze()\n",
    "            \n",
    "            loss_cancel = cancel_criterion(val_pred, cancel)\n",
    "            bce.append(loss_cancel.item())\n",
    "            \n",
    "            valid_acc  += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == cancel.cpu().numpy())\n",
    "            \n",
    "        bce_mean = np.mean(bce)\n",
    "        \n",
    "        if valid_acc > valid_acc_max:\n",
    "            valid_acc_max = valid_acc\n",
    "            torch.save(cancel_model, \"./checkpoints/cancel_model_valid_acc_max.bin\")\n",
    "        print(\"Valid loss: %.3f, valid_acc = %.3f, Valid acc max: %.3f\"%(bce_mean,  valid_acc/valid_dataset.__len__(), valid_acc_max/valid_dataset.__len__()))\n",
    "        print(\"epoch time: %.2f sec\"%(time.time()-epoch_start_time))\n",
    "        print(\"============================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load adr, cancel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_model = torch.load('./checkpoints/adr_model_valid_mse_min.bin')\n",
    "cancel_model = torch.load('./checkpoints/cancel_model_valid_acc_max.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cancel_Model(\n",
       "  (layer): Sequential(\n",
       "    (0): Linear(in_features=62, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.02)\n",
       "    (3): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.02)\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (8): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): LeakyReLU(negative_slope=0.02)\n",
       "    (10): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (11): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): LeakyReLU(negative_slope=0.02)\n",
       "    (13): Dropout(p=0.5, inplace=False)\n",
       "    (14): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (15): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): LeakyReLU(negative_slope=0.02)\n",
       "    (17): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adr_model.eval()\n",
    "cancel_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面的是連原始資料都讀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_num = 69568, 5.4 sec\n",
      "npy_data.shape:  (69566, 2)\n",
      "====================== train_prediction done ======================\n",
      "file_num = 21968, 1.7 sec\n",
      "npy_data.shape:  (21965, 2)\n",
      "====================== valid_prediction done ======================\n",
      "file_num = 27872, 1.9 sec\n",
      "npy_data.shape:  (27860, 2)\n",
      "====================== test_prediction done ======================\n"
     ]
    }
   ],
   "source": [
    "# npy\n",
    "epoch_start_time = time.time()\n",
    "npy_data = np.zeros(shape = (train_dataset.__len__(), 2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, adr, cancel) in enumerate(train_loader):\n",
    "        print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "        data = data.cuda().float()\n",
    "        \n",
    "        predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "        predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "        concat_data = data.cpu().numpy()\n",
    "\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 0] = predict_adr[:]\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 1] = predict_cancel[:]\n",
    "\n",
    "#         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "np.save(\"./npy_file/train_prediction.npy\", npy_data)\n",
    "print(\"\\nnpy_data.shape: \",npy_data.shape)\n",
    "print(\"====================== train_prediction done ======================\")\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "npy_data = np.zeros(shape = (valid_dataset.__len__(), 2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, adr, cancel) in enumerate(valid_loader):\n",
    "        print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "        data = data.cuda().float()\n",
    "        \n",
    "        predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "        predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "        concat_data = data.cpu().numpy()\n",
    "        \n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 0] = predict_adr[:]\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 1] = predict_cancel[:]\n",
    "\n",
    "#         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "np.save(\"./npy_file/valid_prediction.npy\", npy_data)\n",
    "print(\"\\nnpy_data.shape: \",npy_data.shape)\n",
    "print(\"====================== valid_prediction done ======================\")\n",
    "\n",
    "epoch_start_time = time.time()\n",
    "npy_data = np.zeros(shape = (test_dataset.__len__(), 2))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        print('file_num = %d, %2.1f sec' % ((i+1)*batch_size, time.time()-epoch_start_time), end='\\r')\n",
    "        data = data.cuda().float()\n",
    "        \n",
    "        predict_adr = adr_model(data).cpu().numpy().squeeze()\n",
    "        predict_cancel = np.argmax(cancel_model(data).cpu().data.numpy(), axis=1).squeeze()\n",
    "        concat_data = data.cpu().numpy()\n",
    "        \n",
    "#         print(\"data: \",data.shape)\n",
    "#         print(\"adr: \",adr.shape)\n",
    "#         print(\"cancel: \",cancel.shape)\n",
    "#         print(\"predict_adr: \",predict_adr.shape)\n",
    "#         print(\"predict_cancel: \",predict_cancel.shape)\n",
    "#         print(\"concat_data: \",concat_data.shape)\n",
    "#         print(\"npy_data: \",npy_data.shape)\n",
    "        \n",
    "#         npy_data[(i*batch_size):((i+1)*batch_size),:concat_data.shape[1]] = concat_data[:,:]\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 0] = predict_adr[:]\n",
    "        npy_data[(i*batch_size):((i+1)*batch_size), 1] = predict_cancel[:]\n",
    "\n",
    "#         print(\"npy_data: \",npy_data.shape)\n",
    "\n",
    "np.save(\"./npy_file/test_prediction.npy\", npy_data)\n",
    "print(\"\\nnpy_data.shape: \",npy_data.shape)\n",
    "print(\"====================== test_prediction done ======================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用蔡夯哥算法做predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training + validation data or testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_test = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testtttttttttttttttttttttt\n",
      "----- df_date.shape = (27860,)\n",
      "----- npy_data.shape = (27860, 2)\n"
     ]
    }
   ],
   "source": [
    "if train_valid_test == \"train\" or train_valid_test == \"valid\" :\n",
    "    print(\"now train_valid_test: \",train_valid_test)\n",
    "\n",
    "    csv_data_name = \"train.csv\"\n",
    "    csv_label_name = \"train_label.csv\"\n",
    "\n",
    "    df_data = pd.read_csv(os.path.join(\"./data/\", csv_data_name))  \n",
    "\n",
    "    #             if train_valid_test == \"train\" :\n",
    "\n",
    "    df_year = df_data[\"arrival_date_year\"].iloc[:].to_numpy()\n",
    "    df_month = df_data[\"arrival_date_month\"].iloc[:].to_numpy()\n",
    "    df_week = df_data[\"arrival_date_week_number\"].iloc[:].to_numpy()\n",
    "    df_date = df_data[\"arrival_date_day_of_month\"].iloc[:].to_numpy()\n",
    "    print(\"----- df_date.shape = {}\".format(df_date.shape))\n",
    "\n",
    "    #                 print(\"df_week: \",df_wee)\n",
    "\n",
    "    df_weekend = df_data[\"stays_in_weekend_nights\"].iloc[:].to_numpy()\n",
    "    df_weekdays = df_data[\"stays_in_week_nights\"].iloc[:].to_numpy()\n",
    "\n",
    "\n",
    "    npy_train_data = np.load(\"./npy_file/train_prediction.npy\")\n",
    "    npy_valid_data = np.load(\"./npy_file/valid_prediction.npy\")\n",
    "    print(\"----- npy_train_data.shape = {}\".format(npy_train_data.shape))\n",
    "    print(\"----- npy_valid_data.shape = {}\".format(npy_valid_data.shape))\n",
    "    print(\"========================: \",(npy_train_data.shape[0] + npy_valid_data.shape[0]))\n",
    "\n",
    "    npy_data = np.zeros(shape = ((npy_train_data.shape[0] + npy_valid_data.shape[0]), 2))\n",
    "    print(\"----- npy_data.shape = {}\".format(npy_data.shape))\n",
    "\n",
    "    npy_data[:npy_train_data.shape[0],:] = npy_train_data [:,:]\n",
    "    npy_data[npy_train_data.shape[0]:,:] = npy_valid_data [:,:]\n",
    "\n",
    "    print(\"----- npy_data.shape = {}\".format(npy_data.shape))\n",
    "    \n",
    "elif train_valid_test == \"test\" :\n",
    "    print(\"testtttttttttttttttttttttt\")\n",
    "    csv_data_name = \"test.csv\"\n",
    "    csv_label_name = \"test_nolabel.csv\"\n",
    "\n",
    "    df_data = pd.read_csv(os.path.join(\"./data/\", csv_data_name))  \n",
    "\n",
    "\n",
    "    df_year = df_data[\"arrival_date_year\"].iloc[:].to_numpy()\n",
    "    df_month = df_data[\"arrival_date_month\"].iloc[:].to_numpy()\n",
    "    df_week = df_data[\"arrival_date_week_number\"].iloc[:].to_numpy()\n",
    "    df_date = df_data[\"arrival_date_day_of_month\"].iloc[:].to_numpy()\n",
    "    print(\"----- df_date.shape = {}\".format(df_date.shape))\n",
    "\n",
    "\n",
    "    df_weekend = df_data[\"stays_in_weekend_nights\"].iloc[:].to_numpy()\n",
    "    df_weekdays = df_data[\"stays_in_week_nights\"].iloc[:].to_numpy()\n",
    "\n",
    "\n",
    "    npy_data = np.load(\"./npy_file/test_prediction.npy\")\n",
    "    print(\"----- npy_data.shape = {}\".format(npy_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_day_revenue:  27145.130876541138\n",
      "=============== Change day 1 ===============\n",
      "data[0]:  81.33728790283203\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  26709.864728927612\n",
      "=============== Change day 2 ===============\n",
      "data[0]:  60.72134780883789\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  39054.4862537384\n",
      "=============== Change day 3 ===============\n",
      "data[0]:  165.46859741210938\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  18203.164324760437\n",
      "=============== Change day 4 ===============\n",
      "data[0]:  64.41281127929688\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  32533.38987827301\n",
      "=============== Change day 5 ===============\n",
      "data[0]:  85.60386657714844\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  23926.240795135498\n",
      "=============== Change day 6 ===============\n",
      "data[0]:  143.59408569335938\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  24588.124668121338\n",
      "=============== Change day 7 ===============\n",
      "data[0]:  38.151939392089844\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  53702.74289035797\n",
      "=============== Change day 8 ===============\n",
      "data[0]:  40.82008361816406\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  50453.66997528076\n",
      "=============== Change day 9 ===============\n",
      "data[0]:  114.13241577148438\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  42663.01982498169\n",
      "=============== Change day 10 ===============\n",
      "data[0]:  97.12476348876953\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  35188.304144859314\n",
      "=============== Change day 11 ===============\n",
      "data[0]:  110.58544921875\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  29813.93719482422\n",
      "=============== Change day 12 ===============\n",
      "data[0]:  178.4743194580078\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  53812.90461540222\n",
      "=============== Change day 13 ===============\n",
      "data[0]:  47.868247985839844\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  46314.85076236725\n",
      "=============== Change day 14 ===============\n",
      "data[0]:  41.11708068847656\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  36894.589836120605\n",
      "=============== Change day 15 ===============\n",
      "data[0]:  49.20848846435547\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  39680.84953689575\n",
      "=============== Change day 16 ===============\n",
      "data[0]:  74.42399597167969\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  40598.69185447693\n",
      "=============== Change day 17 ===============\n",
      "data[0]:  121.60755920410156\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  22335.21785545349\n",
      "=============== Change day 18 ===============\n",
      "data[0]:  67.4870834350586\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  21418.097785949707\n",
      "=============== Change day 19 ===============\n",
      "data[0]:  92.16632843017578\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  27584.564304351807\n",
      "=============== Change day 20 ===============\n",
      "data[0]:  76.90506744384766\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  36605.79547595978\n",
      "=============== Change day 21 ===============\n",
      "data[0]:  76.75187683105469\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  41160.15928077698\n",
      "=============== Change day 22 ===============\n",
      "data[0]:  14.508469581604004\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  34468.509256362915\n",
      "=============== Change day 23 ===============\n",
      "data[0]:  114.54271697998047\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  31216.131492614746\n",
      "=============== Change day 24 ===============\n",
      "data[0]:  24.948932647705078\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  17004.006259918213\n",
      "=============== Change day 25 ===============\n",
      "data[0]:  170.38674926757812\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  15785.129955291748\n",
      "=============== Change day 26 ===============\n",
      "data[0]:  71.1830062866211\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  29278.0862159729\n",
      "=============== Change day 27 ===============\n",
      "data[0]:  109.93413543701172\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  56954.11141395569\n",
      "=============== Change day 28 ===============\n",
      "data[0]:  76.4676742553711\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  58413.19813346863\n",
      "=============== Change day 29 ===============\n",
      "data[0]:  19.33409309387207\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  40146.88530540466\n",
      "=============== Change day 30 ===============\n",
      "data[0]:  73.3598403930664\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  45749.179626464844\n",
      "=============== Change day 31 ===============\n",
      "data[0]:  49.85224151611328\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  33488.49118423462\n",
      "=============== Change day 32 ===============\n",
      "data[0]:  107.44102478027344\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  24741.033655166626\n",
      "=============== Change day 33 ===============\n",
      "data[0]:  26.378602981567383\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  35973.560874938965\n",
      "=============== Change day 34 ===============\n",
      "data[0]:  56.17323303222656\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  43256.779499053955\n",
      "=============== Change day 35 ===============\n",
      "data[0]:  80.87173461914062\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  38960.30725479126\n",
      "=============== Change day 36 ===============\n",
      "data[0]:  73.533203125\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  38433.3731470108\n",
      "=============== Change day 37 ===============\n",
      "data[0]:  102.82061004638672\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  44609.35396003723\n",
      "=============== Change day 38 ===============\n",
      "data[0]:  98.53140258789062\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  30515.850774765015\n",
      "=============== Change day 39 ===============\n",
      "data[0]:  58.19795608520508\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  27059.40772628784\n",
      "=============== Change day 40 ===============\n",
      "data[0]:  62.831748962402344\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  39432.87085533142\n",
      "=============== Change day 41 ===============\n",
      "data[0]:  145.6612548828125\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  33298.66213989258\n",
      "=============== Change day 42 ===============\n",
      "data[0]:  61.485992431640625\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  33431.70267677307\n",
      "=============== Change day 43 ===============\n",
      "data[0]:  79.26639556884766\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  45366.81998062134\n",
      "=============== Change day 44 ===============\n",
      "data[0]:  79.41709899902344\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  41198.049092292786\n",
      "=============== Change day 45 ===============\n",
      "data[0]:  157.05096435546875\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  30135.014753341675\n",
      "=============== Change day 46 ===============\n",
      "data[0]:  54.2245979309082\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  29879.68096923828\n",
      "=============== Change day 47 ===============\n",
      "data[0]:  104.54407501220703\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  36394.513845443726\n",
      "=============== Change day 48 ===============\n",
      "data[0]:  129.87550354003906\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  33811.85963821411\n",
      "=============== Change day 49 ===============\n",
      "data[0]:  13.325047492980957\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  37327.7067565918\n",
      "=============== Change day 50 ===============\n",
      "data[0]:  154.81886291503906\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  41713.451813697815\n",
      "=============== Change day 51 ===============\n",
      "data[0]:  102.56271362304688\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  39941.12713241577\n",
      "=============== Change day 52 ===============\n",
      "data[0]:  122.52842712402344\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  27579.74310874939\n",
      "=============== Change day 53 ===============\n",
      "data[0]:  56.90596008300781\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  33626.59827709198\n",
      "=============== Change day 54 ===============\n",
      "data[0]:  17.93301010131836\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  47842.88485908508\n",
      "=============== Change day 55 ===============\n",
      "data[0]:  81.69911193847656\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  28008.61290359497\n",
      "=============== Change day 56 ===============\n",
      "data[0]:  115.37544250488281\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  25345.18691253662\n",
      "=============== Change day 57 ===============\n",
      "data[0]:  134.91355895996094\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  47245.3932094574\n",
      "=============== Change day 58 ===============\n",
      "data[0]:  164.05044555664062\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  36964.64803123474\n",
      "=============== Change day 59 ===============\n",
      "data[0]:  69.12519073486328\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  23891.874977111816\n",
      "=============== Change day 60 ===============\n",
      "data[0]:  38.0770149230957\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  23376.70230102539\n",
      "=============== Change day 61 ===============\n",
      "data[0]:  58.25012969970703\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  34904.54293060303\n",
      "=============== Change day 62 ===============\n",
      "data[0]:  84.41535186767578\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  43304.47336387634\n",
      "=============== Change day 63 ===============\n",
      "data[0]:  88.73601531982422\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  40317.16380405426\n",
      "=============== Change day 64 ===============\n",
      "data[0]:  85.26958465576172\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  35813.25617790222\n",
      "=============== Change day 65 ===============\n",
      "data[0]:  142.4137725830078\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  49514.07493400574\n",
      "=============== Change day 66 ===============\n",
      "data[0]:  88.78540802001953\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  31381.57818222046\n",
      "=============== Change day 67 ===============\n",
      "data[0]:  34.27131271362305\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  23778.665992736816\n",
      "=============== Change day 68 ===============\n",
      "data[0]:  56.216487884521484\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  40484.75122451782\n",
      "=============== Change day 69 ===============\n",
      "data[0]:  119.43438720703125\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  38658.129813194275\n",
      "=============== Change day 70 ===============\n",
      "data[0]:  90.70263671875\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  37407.68098068237\n",
      "=============== Change day 71 ===============\n",
      "data[0]:  50.34938049316406\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  59020.19750785828\n",
      "=============== Change day 72 ===============\n",
      "data[0]:  117.29981994628906\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  41719.38351917267\n",
      "=============== Change day 73 ===============\n",
      "data[0]:  94.95977783203125\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  25255.303173065186\n",
      "=============== Change day 74 ===============\n",
      "data[0]:  84.00897216796875\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  28364.107298851013\n",
      "=============== Change day 75 ===============\n",
      "data[0]:  72.07750701904297\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  43588.32640647888\n",
      "=============== Change day 76 ===============\n",
      "data[0]:  87.3897705078125\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  30837.240154266357\n",
      "=============== Change day 77 ===============\n",
      "data[0]:  47.37643051147461\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  39125.592571258545\n",
      "=============== Change day 78 ===============\n",
      "data[0]:  91.64051818847656\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  38349.48215961456\n",
      "=============== Change day 79 ===============\n",
      "data[0]:  102.47266387939453\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  3\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  42829.922775268555\n",
      "=============== Change day 80 ===============\n",
      "data[0]:  148.1077880859375\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  35052.025859832764\n",
      "=============== Change day 81 ===============\n",
      "data[0]:  94.95552825927734\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  32709.065358161926\n",
      "=============== Change day 82 ===============\n",
      "data[0]:  35.579200744628906\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  8\n",
      "pred_day_revenue:  31433.179986953735\n",
      "=============== Change day 83 ===============\n",
      "data[0]:  109.16816711425781\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  30619.979177474976\n",
      "=============== Change day 84 ===============\n",
      "data[0]:  92.99861907958984\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  34978.74676322937\n",
      "=============== Change day 85 ===============\n",
      "data[0]:  104.32875061035156\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  38883.06421852112\n",
      "=============== Change day 86 ===============\n",
      "data[0]:  131.69822692871094\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  35342.27297592163\n",
      "=============== Change day 87 ===============\n",
      "data[0]:  98.16634368896484\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  29018.275619506836\n",
      "=============== Change day 88 ===============\n",
      "data[0]:  54.945316314697266\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  25961.409534454346\n",
      "=============== Change day 89 ===============\n",
      "data[0]:  72.46490478515625\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  24105.730818748474\n",
      "=============== Change day 90 ===============\n",
      "data[0]:  97.03959655761719\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  26844.24220752716\n",
      "=============== Change day 91 ===============\n",
      "data[0]:  90.8091812133789\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  44058.1690864563\n",
      "=============== Change day 92 ===============\n",
      "data[0]:  130.46054077148438\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  35657.951095581055\n",
      "=============== Change day 93 ===============\n",
      "data[0]:  77.81941986083984\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  43993.30991744995\n",
      "=============== Change day 94 ===============\n",
      "data[0]:  74.67656707763672\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  41308.687368392944\n",
      "=============== Change day 95 ===============\n",
      "data[0]:  95.10516357421875\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  24829.54248905182\n",
      "=============== Change day 96 ===============\n",
      "data[0]:  142.4884796142578\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  30610.68914413452\n",
      "=============== Change day 97 ===============\n",
      "data[0]:  50.09040069580078\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  27010.338401794434\n",
      "=============== Change day 98 ===============\n",
      "data[0]:  91.76739501953125\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  38503.87956905365\n",
      "=============== Change day 99 ===============\n",
      "data[0]:  114.29549407958984\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  50792.66107559204\n",
      "=============== Change day 100 ===============\n",
      "data[0]:  82.91644287109375\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  45318.066314697266\n",
      "=============== Change day 101 ===============\n",
      "data[0]:  71.61820983886719\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  38652.413928985596\n",
      "=============== Change day 102 ===============\n",
      "data[0]:  73.83688354492188\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  30875.53652381897\n",
      "=============== Change day 103 ===============\n",
      "data[0]:  79.77934265136719\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  32717.441848754883\n",
      "=============== Change day 104 ===============\n",
      "data[0]:  102.59365844726562\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  34848.030113220215\n",
      "=============== Change day 105 ===============\n",
      "data[0]:  108.01895141601562\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  66392.25791358948\n",
      "=============== Change day 106 ===============\n",
      "data[0]:  66.35859680175781\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  5\n",
      "df_weekdays[i]:  10\n",
      "pred_day_revenue:  43018.71521568298\n",
      "=============== Change day 107 ===============\n",
      "data[0]:  56.67277145385742\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  52293.048681259155\n",
      "=============== Change day 108 ===============\n",
      "data[0]:  89.2911148071289\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  32598.753902435303\n",
      "=============== Change day 109 ===============\n",
      "data[0]:  115.43340301513672\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  23923.58490753174\n",
      "=============== Change day 110 ===============\n",
      "data[0]:  59.518470764160156\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  4\n",
      "df_weekdays[i]:  10\n",
      "pred_day_revenue:  30949.963369369507\n",
      "=============== Change day 111 ===============\n",
      "data[0]:  73.10430145263672\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  3\n",
      "df_weekdays[i]:  7\n",
      "pred_day_revenue:  36285.59401702881\n",
      "=============== Change day 112 ===============\n",
      "data[0]:  130.63046264648438\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  47385.62908554077\n",
      "=============== Change day 113 ===============\n",
      "data[0]:  78.02791595458984\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  45890.15835952759\n",
      "=============== Change day 114 ===============\n",
      "data[0]:  70.59809112548828\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  60559.06025314331\n",
      "=============== Change day 115 ===============\n",
      "data[0]:  80.22748565673828\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  30684.63822555542\n",
      "=============== Change day 116 ===============\n",
      "data[0]:  34.08531188964844\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  29316.082153320312\n",
      "=============== Change day 117 ===============\n",
      "data[0]:  145.9364776611328\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  36890.08439254761\n",
      "=============== Change day 118 ===============\n",
      "data[0]:  120.34735870361328\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  36247.786169052124\n",
      "=============== Change day 119 ===============\n",
      "data[0]:  102.14969635009766\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  53686.53328418732\n",
      "=============== Change day 120 ===============\n",
      "data[0]:  107.3486099243164\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  40935.05072402954\n",
      "=============== Change day 121 ===============\n",
      "data[0]:  73.45536041259766\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  49677.64684391022\n",
      "=============== Change day 122 ===============\n",
      "data[0]:  74.96541595458984\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  45635.08618068695\n",
      "=============== Change day 123 ===============\n",
      "data[0]:  119.0071029663086\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  42555.87227630615\n",
      "=============== Change day 124 ===============\n",
      "data[0]:  97.68251037597656\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  48467.65041732788\n",
      "=============== Change day 125 ===============\n",
      "data[0]:  93.0757827758789\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  37492.12419128418\n",
      "=============== Change day 126 ===============\n",
      "data[0]:  80.6235580444336\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  42796.92467880249\n",
      "=============== Change day 127 ===============\n",
      "data[0]:  81.9566879272461\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  46793.43087768555\n",
      "=============== Change day 128 ===============\n",
      "data[0]:  120.72237396240234\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  58395.19430351257\n",
      "=============== Change day 129 ===============\n",
      "data[0]:  83.67916870117188\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  8\n",
      "pred_day_revenue:  30475.078855514526\n",
      "=============== Change day 130 ===============\n",
      "data[0]:  54.53262710571289\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  37397.65043067932\n",
      "=============== Change day 131 ===============\n",
      "data[0]:  21.65105628967285\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  8\n",
      "pred_day_revenue:  33796.85453796387\n",
      "=============== Change day 132 ===============\n",
      "data[0]:  93.23123931884766\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  30611.730995178223\n",
      "=============== Change day 133 ===============\n",
      "data[0]:  51.97099304199219\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  54497.4079284668\n",
      "=============== Change day 134 ===============\n",
      "data[0]:  132.8085479736328\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  47518.701152801514\n",
      "=============== Change day 135 ===============\n",
      "data[0]:  106.38813781738281\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  44642.02231025696\n",
      "=============== Change day 136 ===============\n",
      "data[0]:  95.97001647949219\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  36551.20090675354\n",
      "=============== Change day 137 ===============\n",
      "data[0]:  76.65874481201172\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  45157.147647857666\n",
      "=============== Change day 138 ===============\n",
      "data[0]:  108.61726379394531\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  42379.96754837036\n",
      "=============== Change day 139 ===============\n",
      "data[0]:  83.65599822998047\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  2\n",
      "pred_day_revenue:  41439.30498123169\n",
      "=============== Change day 140 ===============\n",
      "data[0]:  62.05605697631836\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  5\n",
      "pred_day_revenue:  45169.19527435303\n",
      "=============== Change day 141 ===============\n",
      "data[0]:  114.6640396118164\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  0\n",
      "pred_day_revenue:  48221.53921890259\n",
      "=============== Change day 142 ===============\n",
      "data[0]:  125.0027084350586\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  45040.56777572632\n",
      "=============== Change day 143 ===============\n",
      "data[0]:  82.18878936767578\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  25820.604210853577\n",
      "=============== Change day 144 ===============\n",
      "data[0]:  72.0450668334961\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  34700.435775756836\n",
      "=============== Change day 145 ===============\n",
      "data[0]:  26.827590942382812\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  33482.37776374817\n",
      "=============== Change day 146 ===============\n",
      "data[0]:  71.66998291015625\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  35942.24508666992\n",
      "=============== Change day 147 ===============\n",
      "data[0]:  37.84708023071289\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  1\n",
      "pred_day_revenue:  47292.84078216553\n",
      "=============== Change day 148 ===============\n",
      "data[0]:  120.16744232177734\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  2\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  39837.89872741699\n",
      "=============== Change day 149 ===============\n",
      "data[0]:  45.732295989990234\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  59110.57307243347\n",
      "=============== Change day 150 ===============\n",
      "data[0]:  73.06279754638672\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  29184.295882225037\n",
      "=============== Change day 151 ===============\n",
      "data[0]:  23.313796997070312\n",
      "(1-data[1]):  0.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  4\n",
      "pred_day_revenue:  23329.375995635986\n",
      "=============== Change day 152 ===============\n",
      "data[0]:  146.17481994628906\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  1\n",
      "df_weekdays[i]:  3\n",
      "pred_day_revenue:  35600.173604011536\n",
      "=============== Change day 153 ===============\n",
      "data[0]:  160.2654266357422\n",
      "(1-data[1]):  1.0\n",
      "df_weekend[i]:  0\n",
      "df_weekdays[i]:  1\n",
      "total_days:  153\n",
      "----- revenue_norm = 27145.130876541138\n",
      "----- revenue = 27145.130876541138\n",
      "----- revenue_norm.shape = (153,)\n",
      "----- revenue.shape = (153,)\n",
      "----- final_data.shape = (153, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\" 上述在讀回 np 資訊\"\n",
    "\n",
    "total_days = 0\n",
    "pred_day_revenue = 0\n",
    "total_weekdays = 0\n",
    "total_weekends = 0\n",
    "total_cancel = 0\n",
    "total_adr = 0\n",
    "\n",
    "\n",
    "bef_year = df_year[0]\n",
    "bef_month = df_month[0]\n",
    "bef_week = df_week[0]\n",
    "\n",
    "bef_date = df_date[0]\n",
    "pred_revenue_daylist = []\n",
    "#             adr_daylist = []\n",
    "#             cancel_daylist = []\n",
    "#             weekends_daylist = []\n",
    "#             weekdays_daylist = []\n",
    "\n",
    "for i, data in enumerate(npy_data):\n",
    "    \n",
    "    # 換天\n",
    "    if df_year[i] != bef_year or df_month[i] != bef_month or df_week[i] != bef_week or df_date[i] != bef_date or (i+1) ==len(npy_data):\n",
    "        bef_year = df_year[i]\n",
    "        bef_month = df_month[i]\n",
    "        bef_week = df_week[i]\n",
    "        bef_date = df_date[i]\n",
    "\n",
    "        pred_revenue_daylist.append(pred_day_revenue)\n",
    "        print(\"pred_day_revenue: \",pred_day_revenue)\n",
    "\n",
    "        total_days += 1\n",
    "        print(\"=============== Change day %d ===============\"%total_days)\n",
    "\n",
    "\n",
    "#                     cancel_daylist.append(total_cancel)\n",
    "#                     weekends_daylist.append(total_weekends)\n",
    "#                     weekdays_daylist.append(total_weekdays)\n",
    "#                     adr_daylist.append(total_adr)\n",
    "\n",
    "#                     total_weekdays = 0\n",
    "#                     total_weekends = 0\n",
    "#                     total_cancel = 0\n",
    "#                     total_adr = 0\n",
    "\n",
    "        pred_day_revenue = data[0] * (1-data[1]) * (df_weekend[i] + df_weekdays[i])\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"data[0]: \",data[0])\n",
    "        print(\"(1-data[1]): \",(1-data[1]))\n",
    "        print(\"df_weekend[i]: \",df_weekend[i])\n",
    "        print(\"df_weekdays[i]: \",df_weekdays[i])\n",
    "\n",
    "\n",
    "        \n",
    "    else :\n",
    "        pred_revenue = data[0] * (1-data[1]) * (df_weekend[i] + df_weekdays[i])\n",
    "\n",
    "#         print(\"data[0]: \",data[0])\n",
    "#         print(\"(1-data[1]): \",(1-data[1]))\n",
    "#         print(\"df_weekend[i]: \",df_weekend[i])\n",
    "#         print(\"df_weekdays[i]: \",df_weekdays[i])\n",
    "\n",
    "        pred_day_revenue += pred_revenue\n",
    "#         print(\"pred_day_revenue: \",pred_day_revenue)\n",
    "\n",
    "print(\"total_days: \",total_days)\n",
    "\n",
    "revenue = np.array(pred_revenue_daylist)\n",
    "revenue_norm = revenue\n",
    "# for i in range(len(revenue)):\n",
    "    \n",
    "#     revenue_norm[i] = (revenue[i]-min(revenue))/(max(revenue)-min(revenue))\n",
    "\n",
    "#             weekdays = np.array(weekdays_daylist)\n",
    "#             weekends = np.array(weekends_daylist)\n",
    "#             cancel = np.array(cancel_daylist)\n",
    "#             adr = np.array(adr_daylist)\n",
    "print(\"----- revenue_norm = {}\".format(revenue_norm[0]))\n",
    "print(\"----- revenue = {}\".format(revenue[0]))\n",
    "final_data = np.zeros(shape = (revenue.shape[0],1) )\n",
    "final_data[:,0] = revenue_norm[:]\n",
    "#             final_data[:,1] = adr[:]\n",
    "#             final_data[:,2] = cancel[:]\n",
    "#             final_data[:,3] = weekdays[:]\n",
    "#             final_data[:,4] = weekends[:]\n",
    "print(\"----- revenue_norm.shape = {}\".format(revenue_norm.shape))\n",
    "print(\"----- revenue.shape = {}\".format(revenue.shape))\n",
    "\n",
    "#             print(\"----- adr.shape = {}\".format(adr.shape))\n",
    "#             print(\"----- cancel.shape = {}\".format(cancel.shape))\n",
    "#             print(\"----- weekdays.shape = {}\".format(weekdays.shape))\n",
    "#             print(\"----- weekends.shape = {}\".format(weekends.shape))\n",
    "\n",
    "print(\"----- final_data.shape = {}\".format(final_data.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train + valid metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if train_valid_test == \"train\" :\n",
    "    df_label = pd.read_csv(os.path.join(\"./data/\", csv_label_name))  \n",
    "    df_label = df_label.fillna(0)\n",
    "    df_label = df_label.iloc[:512,1].to_numpy()\n",
    "    final_data = final_data[:512]\n",
    "    print(\"----- df_label.shape = {}\".format(df_label.shape))\n",
    "    print(\"final_data: \",final_data.shape)\n",
    "elif train_valid_test == \"valid\" :\n",
    "    df_label = pd.read_csv(os.path.join(\"./data/\", csv_label_name))  \n",
    "    df_label = df_label.fillna(0)\n",
    "    df_label = df_label.iloc[512:,1].to_numpy()\n",
    "    final_data = final_data[512:]\n",
    "    print(\"----- df_label.shape = {}\".format(df_label.shape))\n",
    "    print(\"final_data: \",final_data.shape)\n",
    "else:\n",
    "    print(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_data.shape (153, 1)\n",
      "i: 0, data: 2\n",
      "i: 1, data: 2\n",
      "i: 2, data: 3\n",
      "i: 3, data: 1\n",
      "i: 4, data: 3\n",
      "i: 5, data: 2\n",
      "i: 6, data: 2\n",
      "i: 7, data: 5\n",
      "i: 8, data: 5\n",
      "i: 9, data: 4\n",
      "i: 10, data: 3\n",
      "i: 11, data: 2\n",
      "i: 12, data: 5\n",
      "i: 13, data: 4\n",
      "i: 14, data: 3\n",
      "i: 15, data: 3\n",
      "i: 16, data: 4\n",
      "i: 17, data: 2\n",
      "i: 18, data: 2\n",
      "i: 19, data: 2\n",
      "i: 20, data: 3\n",
      "i: 21, data: 4\n",
      "i: 22, data: 3\n",
      "i: 23, data: 3\n",
      "i: 24, data: 1\n",
      "i: 25, data: 1\n",
      "i: 26, data: 2\n",
      "i: 27, data: 5\n",
      "i: 28, data: 5\n",
      "i: 29, data: 4\n",
      "i: 30, data: 4\n",
      "i: 31, data: 3\n",
      "i: 32, data: 2\n",
      "i: 33, data: 3\n",
      "i: 34, data: 4\n",
      "i: 35, data: 3\n",
      "i: 36, data: 3\n",
      "i: 37, data: 4\n",
      "i: 38, data: 3\n",
      "i: 39, data: 2\n",
      "i: 40, data: 3\n",
      "i: 41, data: 3\n",
      "i: 42, data: 3\n",
      "i: 43, data: 4\n",
      "i: 44, data: 4\n",
      "i: 45, data: 3\n",
      "i: 46, data: 2\n",
      "i: 47, data: 3\n",
      "i: 48, data: 3\n",
      "i: 49, data: 3\n",
      "i: 50, data: 4\n",
      "i: 51, data: 3\n",
      "i: 52, data: 2\n",
      "i: 53, data: 3\n",
      "i: 54, data: 4\n",
      "i: 55, data: 2\n",
      "i: 56, data: 2\n",
      "i: 57, data: 4\n",
      "i: 58, data: 3\n",
      "i: 59, data: 2\n",
      "i: 60, data: 2\n",
      "i: 61, data: 3\n",
      "i: 62, data: 4\n",
      "i: 63, data: 4\n",
      "i: 64, data: 3\n",
      "i: 65, data: 4\n",
      "i: 66, data: 3\n",
      "i: 67, data: 2\n",
      "i: 68, data: 4\n",
      "i: 69, data: 3\n",
      "i: 70, data: 3\n",
      "i: 71, data: 5\n",
      "i: 72, data: 4\n",
      "i: 73, data: 2\n",
      "i: 74, data: 2\n",
      "i: 75, data: 4\n",
      "i: 76, data: 3\n",
      "i: 77, data: 3\n",
      "i: 78, data: 3\n",
      "i: 79, data: 4\n",
      "i: 80, data: 3\n",
      "i: 81, data: 3\n",
      "i: 82, data: 3\n",
      "i: 83, data: 3\n",
      "i: 84, data: 3\n",
      "i: 85, data: 3\n",
      "i: 86, data: 3\n",
      "i: 87, data: 2\n",
      "i: 88, data: 2\n",
      "i: 89, data: 2\n",
      "i: 90, data: 2\n",
      "i: 91, data: 4\n",
      "i: 92, data: 3\n",
      "i: 93, data: 4\n",
      "i: 94, data: 4\n",
      "i: 95, data: 2\n",
      "i: 96, data: 3\n",
      "i: 97, data: 2\n",
      "i: 98, data: 3\n",
      "i: 99, data: 5\n",
      "i: 100, data: 4\n",
      "i: 101, data: 3\n",
      "i: 102, data: 3\n",
      "i: 103, data: 3\n",
      "i: 104, data: 3\n",
      "i: 105, data: 6\n",
      "i: 106, data: 4\n",
      "i: 107, data: 5\n",
      "i: 108, data: 3\n",
      "i: 109, data: 2\n",
      "i: 110, data: 3\n",
      "i: 111, data: 3\n",
      "i: 112, data: 4\n",
      "i: 113, data: 4\n",
      "i: 114, data: 6\n",
      "i: 115, data: 3\n",
      "i: 116, data: 2\n",
      "i: 117, data: 3\n",
      "i: 118, data: 3\n",
      "i: 119, data: 5\n",
      "i: 120, data: 4\n",
      "i: 121, data: 4\n",
      "i: 122, data: 4\n",
      "i: 123, data: 4\n",
      "i: 124, data: 4\n",
      "i: 125, data: 3\n",
      "i: 126, data: 4\n",
      "i: 127, data: 4\n",
      "i: 128, data: 5\n",
      "i: 129, data: 3\n",
      "i: 130, data: 3\n",
      "i: 131, data: 3\n",
      "i: 132, data: 3\n",
      "i: 133, data: 5\n",
      "i: 134, data: 4\n",
      "i: 135, data: 4\n",
      "i: 136, data: 3\n",
      "i: 137, data: 4\n",
      "i: 138, data: 4\n",
      "i: 139, data: 4\n",
      "i: 140, data: 4\n",
      "i: 141, data: 4\n",
      "i: 142, data: 4\n",
      "i: 143, data: 2\n",
      "i: 144, data: 3\n",
      "i: 145, data: 3\n",
      "i: 146, data: 3\n",
      "i: 147, data: 4\n",
      "i: 148, data: 3\n",
      "i: 149, data: 5\n",
      "i: 150, data: 2\n",
      "i: 151, data: 2\n",
      "i: 152, data: 3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "metric_list = []\n",
    "metric_data = []\n",
    "\n",
    "\n",
    "\n",
    "if train_valid_test == \"train\" or train_valid_test == \"valid\":\n",
    "    print(\"final_data.shape\",final_data.shape)\n",
    "    print(\"df_label.shape\",df_label.shape)\n",
    "    for i, data in enumerate (final_data) :\n",
    "        metric_data.append(np.abs(math.floor(data/10000)-df_label[i]))\n",
    "        print(\"i: \"+str(i)+\", data: \"+str(math.floor(data/10000))+\", label: \"+str(df_label[i]))\n",
    "    print(\"========================================================+\")\n",
    "    print(str(train_valid_test)+\" MAE: \"+str(np.mean(metric_data)))\n",
    "    print(\"========================================================+\")\n",
    "\n",
    "else:\n",
    "    print(\"final_data.shape\",final_data.shape)\n",
    "    for i, data in enumerate (final_data) :\n",
    "        metric_data.append(np.abs(math.floor(data/10000)))\n",
    "        print(\"i: \"+str(i)+\", data: \"+str(math.floor(data/10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_data.shape (153, 1)\n",
      "i: 0, data: 2\n",
      "i: 1, data: 2\n",
      "i: 2, data: 3\n",
      "i: 3, data: 1\n",
      "i: 4, data: 3\n",
      "i: 5, data: 2\n",
      "i: 6, data: 2\n",
      "i: 7, data: 5\n",
      "i: 8, data: 5\n",
      "i: 9, data: 4\n",
      "i: 10, data: 3\n",
      "i: 11, data: 2\n",
      "i: 12, data: 5\n",
      "i: 13, data: 4\n",
      "i: 14, data: 3\n",
      "i: 15, data: 3\n",
      "i: 16, data: 4\n",
      "i: 17, data: 2\n",
      "i: 18, data: 2\n",
      "i: 19, data: 2\n",
      "i: 20, data: 3\n",
      "i: 21, data: 4\n",
      "i: 22, data: 3\n",
      "i: 23, data: 3\n",
      "i: 24, data: 1\n",
      "i: 25, data: 1\n",
      "i: 26, data: 2\n",
      "i: 27, data: 5\n",
      "i: 28, data: 5\n",
      "i: 29, data: 4\n",
      "i: 30, data: 4\n",
      "i: 31, data: 3\n",
      "i: 32, data: 2\n",
      "i: 33, data: 3\n",
      "i: 34, data: 4\n",
      "i: 35, data: 3\n",
      "i: 36, data: 3\n",
      "i: 37, data: 4\n",
      "i: 38, data: 3\n",
      "i: 39, data: 2\n",
      "i: 40, data: 3\n",
      "i: 41, data: 3\n",
      "i: 42, data: 3\n",
      "i: 43, data: 4\n",
      "i: 44, data: 4\n",
      "i: 45, data: 3\n",
      "i: 46, data: 2\n",
      "i: 47, data: 3\n",
      "i: 48, data: 3\n",
      "i: 49, data: 3\n",
      "i: 50, data: 4\n",
      "i: 51, data: 3\n",
      "i: 52, data: 2\n",
      "i: 53, data: 3\n",
      "i: 54, data: 4\n",
      "i: 55, data: 2\n",
      "i: 56, data: 2\n",
      "i: 57, data: 4\n",
      "i: 58, data: 3\n",
      "i: 59, data: 2\n",
      "i: 60, data: 2\n",
      "i: 61, data: 3\n",
      "i: 62, data: 4\n",
      "i: 63, data: 4\n",
      "i: 64, data: 3\n",
      "i: 65, data: 4\n",
      "i: 66, data: 3\n",
      "i: 67, data: 2\n",
      "i: 68, data: 4\n",
      "i: 69, data: 3\n",
      "i: 70, data: 3\n",
      "i: 71, data: 5\n",
      "i: 72, data: 4\n",
      "i: 73, data: 2\n",
      "i: 74, data: 2\n",
      "i: 75, data: 4\n",
      "i: 76, data: 3\n",
      "i: 77, data: 3\n",
      "i: 78, data: 3\n",
      "i: 79, data: 4\n",
      "i: 80, data: 3\n",
      "i: 81, data: 3\n",
      "i: 82, data: 3\n",
      "i: 83, data: 3\n",
      "i: 84, data: 3\n",
      "i: 85, data: 3\n",
      "i: 86, data: 3\n",
      "i: 87, data: 2\n",
      "i: 88, data: 2\n",
      "i: 89, data: 2\n",
      "i: 90, data: 2\n",
      "i: 91, data: 4\n",
      "i: 92, data: 3\n",
      "i: 93, data: 4\n",
      "i: 94, data: 4\n",
      "i: 95, data: 2\n",
      "i: 96, data: 3\n",
      "i: 97, data: 2\n",
      "i: 98, data: 3\n",
      "i: 99, data: 5\n",
      "i: 100, data: 4\n",
      "i: 101, data: 3\n",
      "i: 102, data: 3\n",
      "i: 103, data: 3\n",
      "i: 104, data: 3\n",
      "i: 105, data: 6\n",
      "i: 106, data: 4\n",
      "i: 107, data: 5\n",
      "i: 108, data: 3\n",
      "i: 109, data: 2\n",
      "i: 110, data: 3\n",
      "i: 111, data: 3\n",
      "i: 112, data: 4\n",
      "i: 113, data: 4\n",
      "i: 114, data: 6\n",
      "i: 115, data: 3\n",
      "i: 116, data: 2\n",
      "i: 117, data: 3\n",
      "i: 118, data: 3\n",
      "i: 119, data: 5\n",
      "i: 120, data: 4\n",
      "i: 121, data: 4\n",
      "i: 122, data: 4\n",
      "i: 123, data: 4\n",
      "i: 124, data: 4\n",
      "i: 125, data: 3\n",
      "i: 126, data: 4\n",
      "i: 127, data: 4\n",
      "i: 128, data: 5\n",
      "i: 129, data: 3\n",
      "i: 130, data: 3\n",
      "i: 131, data: 3\n",
      "i: 132, data: 3\n",
      "i: 133, data: 5\n",
      "i: 134, data: 4\n",
      "i: 135, data: 4\n",
      "i: 136, data: 3\n",
      "i: 137, data: 4\n",
      "i: 138, data: 4\n",
      "i: 139, data: 4\n",
      "i: 140, data: 4\n",
      "i: 141, data: 4\n",
      "i: 142, data: 4\n",
      "i: 143, data: 2\n",
      "i: 144, data: 3\n",
      "i: 145, data: 3\n",
      "i: 146, data: 3\n",
      "i: 147, data: 4\n",
      "i: 148, data: 3\n",
      "i: 149, data: 5\n",
      "i: 150, data: 2\n",
      "i: 151, data: 2\n",
      "i: 152, data: 3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "result = []\n",
    "print(\"final_data.shape\",final_data.shape)\n",
    "# print(\"df_label.shape\",df_label.shape)\n",
    "\n",
    "for i, data in enumerate (final_data) :\n",
    "    print(\"i: \"+str(i)+\", data: \"+str(math.floor(data/10000)))\n",
    "    result.append(math.floor(data/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of prediction\n"
     ]
    }
   ],
   "source": [
    "df_test_nolabel = pd.read_csv(\"./data/test_nolabel.csv\")  \n",
    "df_test_nolabel = df_test_nolabel.iloc[:,:].to_numpy()\n",
    "\n",
    "with open(os.path.join(\"./output_csv/test_pred_2021_5_5_test.csv\"), 'w') as f:\n",
    "    f.write('arrival_date,label\\n')\n",
    "    for i, y in  enumerate(result):\n",
    "        f.write('{},{}\\n'.format(df_test_nolabel[i][0], y))\n",
    "print(\"End of prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
